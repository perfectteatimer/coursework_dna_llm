{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f238530",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d148130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'girls_code'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/yalibina/ZDNA_LLM.git girls_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f026591a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: optuna in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.3.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna) (1.15.2)\n",
      "Requirement already satisfied: colorlog in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna) (24.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna) (2.0.40)\n",
      "Requirement already satisfied: tqdm in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna) (4.67.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: C:\\Users\\1\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "546709ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Sparse_vector' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Nazar1997/Sparse_vector.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "049547e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: `git lfs clone` is deprecated and will not be updated\n",
      "          with new flags from `git clone`\n",
      "\n",
      "`git clone` has been updated in upstream Git to have comparable\n",
      "speeds to `git lfs clone`.\n",
      "fatal: destination path 'z_dna' already exists and is not an empty directory.\n",
      "Error(s) during clone:\n",
      "`git clone` failed: exit status 128\n"
     ]
    }
   ],
   "source": [
    "!git lfs clone https://github.com/vladislareon/z_dna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a76ebb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.14.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.5.2)\n",
      "Requirement already satisfied: torch in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.5.1+cu118)\n",
      "Requirement already satisfied: tqdm in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.67.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.4.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.9.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\1\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: C:\\Users\\1\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas scipy scikit-learn torch tqdm joblib seaborn matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9380959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import cuda\n",
    "from torch.utils import data\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "import os\n",
    "from joblib import load\n",
    "\n",
    "mcc = evaluate.load(\"matthews_correlation\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, matthews_corrcoef\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "from modeling_hyena import HyenaDNAForTokenClassification\n",
    "\n",
    "from collections import defaultdict\n",
    "from captum.attr import IntegratedGradients, Saliency, NoiseTunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "333f1d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSEMBLY_d = {}\n",
    "chroms_d = {}\n",
    "all_features_d = {}\n",
    "groups_d = {}\n",
    "feature_names_d = {}\n",
    "ZDNA_d = {}\n",
    "black_list_d = {}\n",
    "DNA_d = {}\n",
    "DNA_features_d = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd57249",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3c44c7",
   "metadata": {},
   "source": [
    "## metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSEMBLY = \"ZDNA_2016\"\n",
    "chroms = [f\"chr{i}\" for i in list(range(1, 23)) + [\"X\", \"Y\"]]\n",
    "\n",
    "ZDNA = load(\"z_dna/hg38_zdna/sparse/ZDNA_cousine.pkl\")\n",
    "\n",
    "black_list = load(\"../data_zdna/blacklist_hg38_v2.pkl\")\n",
    "\n",
    "all_features = sorted(\n",
    "    [i[:-4] for i in os.listdir(\"z_dna/hg38_features/sparse\") if i.endswith(\".pkl\")]\n",
    ")\n",
    "groups = [\"DNase-seq\", \"Histone\", \"RNA polymerase\", \"TFs and others\"]\n",
    "feature_names = [i for i in all_features if (i.split(\"_\")[0] in groups)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a533295",
   "metadata": {},
   "source": [
    "## DNA sequence assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcce566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:05<00:00,  4.64it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from joblib import load\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def load_chrom_sequence(chrom: str) -> str:\n",
    "    base_dir = os.path.join(\"z_dna\", \"hg38_dna\")\n",
    "    files = sorted(f for f in os.listdir(base_dir) if f.startswith(f\"{chrom}_\"))\n",
    "    return \"\".join(load(os.path.join(base_dir, f)) for f in files)\n",
    "\n",
    "\n",
    "DNA = {chrom: load_chrom_sequence(chrom) for chrom in tqdm(chroms)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ccc03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'hg38'\n",
    "ASSEMBLY_d[mode] = ASSEMBLY\n",
    "chroms_d[mode] = chroms\n",
    "all_features_d[mode] = all_features\n",
    "groups_d[mode] = groups \n",
    "feature_names_d[mode] = feature_names\n",
    "ZDNA_d[mode] = ZDNA\n",
    "black_list_d[mode] = black_list\n",
    "DNA_d[mode] = DNA\n",
    "# DNA_features_d[mode] = DNA_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a257fa9",
   "metadata": {},
   "source": [
    "## Creating and labeling windows \n",
    "We traverse each chromosome with fixed-size windows (width = 256 e.g.), filter by 'N' count and blacklist, and note whether the window is included in Z-DNA regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b1c8dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ff4170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2489564/2489564 [01:04<00:00, 38881.32it/s]\n",
      "100%|██████████| 2421935/2421935 [01:02<00:00, 38518.44it/s]\n",
      "100%|██████████| 1982955/1982955 [00:49<00:00, 39847.19it/s]\n",
      "100%|██████████| 1902145/1902145 [00:48<00:00, 39207.83it/s]\n",
      "100%|██████████| 1815382/1815382 [00:45<00:00, 40276.79it/s]\n",
      "100%|██████████| 1708059/1708059 [00:43<00:00, 39663.14it/s]\n",
      "100%|██████████| 1593459/1593459 [00:40<00:00, 39467.53it/s]\n",
      "100%|██████████| 1451386/1451386 [00:35<00:00, 40401.24it/s]\n",
      "100%|██████████| 1383947/1383947 [00:31<00:00, 43583.83it/s]\n",
      "100%|██████████| 1337974/1337974 [00:34<00:00, 38379.38it/s]\n",
      "100%|██████████| 1350866/1350866 [00:33<00:00, 40278.09it/s]\n",
      "100%|██████████| 1332753/1332753 [00:33<00:00, 40309.49it/s]\n",
      "100%|██████████| 1143643/1143643 [00:26<00:00, 42616.28it/s]\n",
      "100%|██████████| 1070437/1070437 [00:26<00:00, 40928.17it/s]\n",
      "100%|██████████| 1019911/1019911 [00:22<00:00, 44461.56it/s]\n",
      "100%|██████████| 903383/903383 [00:21<00:00, 41891.53it/s]\n",
      "100%|██████████| 832574/832574 [00:20<00:00, 40373.75it/s]\n",
      "100%|██████████| 803732/803732 [00:19<00:00, 40991.07it/s]\n",
      "100%|██████████| 586176/586176 [00:14<00:00, 40865.87it/s]\n",
      "100%|██████████| 644441/644441 [00:15<00:00, 40845.04it/s]\n",
      "100%|██████████| 467099/467099 [00:12<00:00, 38156.89it/s]\n",
      "100%|██████████| 508184/508184 [00:11<00:00, 46085.41it/s]\n",
      "100%|██████████| 1560408/1560408 [00:38<00:00, 40184.47it/s]\n",
      "100%|██████████| 572274/572274 [00:10<00:00, 53087.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44775\n",
      "28267509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "ints_in = []\n",
    "ints_out = []\n",
    "\n",
    "for chrm in chroms:\n",
    "    for st in trange(0, ZDNA[chrm].shape - width, width):\n",
    "        interval = [st, min(st + width, ZDNA[chrm].shape)]\n",
    "        N_count = sum([bp == \"N\" for bp in DNA[chrm][interval[0] : interval[1]]])\n",
    "        bl_count = black_list[chrm][interval[0] : interval[1]].sum()\n",
    "        if N_count > width / 2 or bl_count > 0:\n",
    "            continue\n",
    "        else:\n",
    "            if ZDNA[chrm][interval[0] : interval[1]].any():\n",
    "                ints_in.append([chrm, int(interval[0]), int(interval[1]), 1])\n",
    "            else:\n",
    "                ints_out.append([chrm, int(interval[0]), int(interval[1]), 0])\n",
    "print(len(ints_in))\n",
    "print(len(ints_out))\n",
    "\n",
    "ints_in_full = ints_in\n",
    "ints_out_full = ints_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e68197",
   "metadata": {},
   "source": [
    "attempt to augment the data \n",
    "* **Window width:** `width = 100` nucleotides\n",
    "* **Overlap:** `stride = width // 2 = 50` → each next window is shifted by 50 nucleotides, i.e. 50% overlap\n",
    "* **Length jitter:** `delta = 20` → for each window, the length becomes random in the range `[80, 120]` so that the model sees fragments of different lengths.\n",
    "* **Generation of windows:**\n",
    "\n",
    "  1. For each chromosome, traverse the positions `st` from `0` to the end in `stride` steps.\n",
    "  2. Count `cur_w = width + randint(-delta, delta)`, set `end = min(st + cur_w, seq_len)`.\n",
    "  3. discard the window if it is too \"dirty\" (more than half of `N` characters or overlaps with black\\_list).\n",
    "  4. If the `[st:end]` zone has at least one \"1\" in the ZDNA mask - add to `ints_in`, otherwise to `ints_out`.\n",
    "The windows will go: [0-100], [50-150], [100-200], ...\n",
    "\n",
    "The model will see each position in different contexts\n",
    "\n",
    "\n",
    "* `ints_in_full` - Z-DNA fragments.\n",
    "* `ints_out_full` - fragments without Z-DNA\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b34d28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89477\n",
      "56534788\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(10)\n",
    "\n",
    "# import random\n",
    "# width  = 100           # базовая длина окна\n",
    "# delta  = 20            # максимально ± джиттер по длине\n",
    "# stride = width // 2    # 50% перекрытие\n",
    "\n",
    "# ints_in, ints_out = [], []\n",
    "\n",
    "# for chrm in chroms:\n",
    "#     seq_len = len(DNA[chrm])\n",
    "#     # скользим с шагом stride\n",
    "#     for st in range(0, seq_len - width + 1, stride):\n",
    "#         cur_w = width + random.randint(-delta, delta)\n",
    "#         end   = min(st + cur_w, seq_len)\n",
    "\n",
    "#         subseq = DNA[chrm][st:end]\n",
    "#         N_count  = sum(bp == \"N\" for bp in subseq)\n",
    "#         bl_count = black_list[chrm][st:end].sum()\n",
    "\n",
    "#         if N_count > cur_w / 2 or bl_count > 0:\n",
    "#             continue\n",
    "\n",
    "#         if ZDNA[chrm][st:end].any():\n",
    "#             ints_in .append([chrm, st, end, 1])\n",
    "#         else:\n",
    "#             ints_out.append([chrm, st, end, 0])\n",
    "\n",
    "\n",
    "# print(len(ints_in))\n",
    "# print(len(ints_out))\n",
    "\n",
    "# ints_in_full = ints_in # есть то что ищем (только оно и есть)\n",
    "# ints_out_full = ints_out # тут нет зднк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a494871e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44775\n",
      "28267509\n"
     ]
    }
   ],
   "source": [
    "print(len(ints_in_full))\n",
    "print(len(ints_out_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544e6cab",
   "metadata": {},
   "source": [
    "## Loading the dataset created above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46b431b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ints_in and ints_out have been saved as separate files.\n"
     ]
    }
   ],
   "source": [
    "# from joblib import dump\n",
    "\n",
    "# dump(ints_in_full, 'ints_in_aug.pkl')\n",
    "# dump(ints_out_full, 'ints_out_aug.pkl')\n",
    "\n",
    "# print(\"ints_in and ints_out have been saved as separate files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b27aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ints_in and ints_out have been loaded from files.\n"
     ]
    }
   ],
   "source": [
    "# ints_in_full = load('ints_in100.pkl')\n",
    "# ints_out_full = load('ints_out100.pkl')\n",
    "# print(\"ints_in and ints_out have been loaded from files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe0e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44775\n",
      "28267509\n"
     ]
    }
   ],
   "source": [
    "# print(len(ints_in_full))\n",
    "# print(len(ints_out_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34615a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(ints_in_full2))\n",
    "# print(len(ints_out_full2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6e7efa",
   "metadata": {},
   "source": [
    "## Balance of classes\n",
    "To keep the model from \"bogging down\" on classes, we balance negative examples to positive examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44dbdb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44775\n",
      "134325\n"
     ]
    }
   ],
   "source": [
    "ints_in = ints_in_full\n",
    "ints_out = [ints_out_full[i] for i in np.random.choice(range(len(ints_out_full)),\n",
    "                                                    size=len(ints_in) * 3, replace=False)] # 3:1 ratio\n",
    "print(len(ints_in)) \n",
    "print(len(ints_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "338966bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chr1', 905400, 905500, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ints_in[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803260e6",
   "metadata": {},
   "source": [
    "# Dateset class and division of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790aab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class DNATokenClassificationDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        chroms: List[str],\n",
    "        dna_source: Dict[str, str],\n",
    "        labels_source: Dict[str, torch.Tensor],\n",
    "        intervals: List[Tuple[str, int, int]],\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        max_length: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        chroms - list of available chromosomes\n",
    "        dna_source - {chrom: dna_string}\n",
    "        labels_source - {chrom: Tensor[length_of_chrom]} with 0/1 labels by nucleotide\n",
    "        intervals - [(chrom, start, end), ...]\n",
    "        tokeniser - HyenaDNATokenizer\n",
    "        max_length - Lmax for padding/truncation\n",
    "        \"\"\"\n",
    "        self.intervals = intervals\n",
    "        self.dna_source = dna_source\n",
    "        self.labels_source = labels_source\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.intervals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        interval = self.intervals[idx]\n",
    "        chrom = interval[0]\n",
    "        start = interval[1]\n",
    "        end = interval[2]\n",
    "        seq = self.dna_source[chrom][start:end].upper()\n",
    "        char_labels = self.labels_source[chrom][start:end]  # Tensor of shape (L,)\n",
    "\n",
    "        # # ----- RC-aug -----\n",
    "        # if random.random() < 0.5:\n",
    "        #     comp = seq.translate(str.maketrans(\"ACGT\", \"TGCA\"))\n",
    "        #     seq = comp[::-1]\n",
    "        #     char_labels = char_labels[::-1]\n",
    "        # # --------------------------\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            seq,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length + 1,\n",
    "            return_special_tokens_mask=True,\n",
    "            return_attention_mask=True,\n",
    "            # add_special_tokens=False,\n",
    "        )\n",
    "        input_ids = torch.tensor(enc[\"input_ids\"], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(enc[\"attention_mask\"], dtype=torch.long)\n",
    "        special_tokens_mask = torch.tensor(enc[\"special_tokens_mask\"], dtype=torch.long)\n",
    "\n",
    "        # Align labels: one character per token\n",
    "        labels_by_tok = []\n",
    "        char_ptr = 0\n",
    "        for is_special in special_tokens_mask.tolist():\n",
    "            if is_special:\n",
    "                # CLS, SEP, PAD → ignore\n",
    "                labels_by_tok.append(-100)\n",
    "            else:\n",
    "                # If there are any more character labels, take the next one\n",
    "                if char_ptr < len(char_labels):\n",
    "                    labels_by_tok.append(int(char_labels[char_ptr]))\n",
    "                    char_ptr += 1\n",
    "                else:\n",
    "                    # The original string has been truncated → put -100\n",
    "                    labels_by_tok.append(-100)\n",
    "\n",
    "        labels_by_tok = torch.tensor(labels_by_tok, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels_by_tok,\n",
    "            \"seq\": seq,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f2430",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c6ddd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f9f316",
   "metadata": {},
   "source": [
    "I was training on the HyenaDNA with context window length 1k and 32k, there's no change in quality, 32k just has params to work with larger context which are unused in my case (since length of my seq <1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e21ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of HyenaDNAForSequenceClassification were not initialized from the model checkpoint at LongSafari/hyenadna-tiny-1k-seqlen-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['hyena.backbone.embeddings.word_embeddings.weight', 'hyena.backbone.layers.0.mixer.in_proj.weight', 'hyena.backbone.layers.0.mixer.in_proj.bias', 'hyena.backbone.layers.0.mixer.out_proj.weight', 'hyena.backbone.layers.0.mixer.out_proj.bias', 'hyena.backbone.layers.0.mixer.short_filter.weight', 'hyena.backbone.layers.0.mixer.short_filter.bias', 'hyena.backbone.layers.0.mixer.filter_fn.bias', 'hyena.backbone.layers.0.mixer.filter_fn.pos_emb.z', 'hyena.backbone.layers.0.mixer.filter_fn.pos_emb.t', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.0.weight', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.0.bias', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.1.freq', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.2.weight', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.2.bias', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.3.freq', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.4.weight', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.4.bias', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.5.freq', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.6.weight', 'hyena.backbone.layers.0.mixer.filter_fn.modulation.deltas', 'hyena.backbone.layers.0.norm1.weight', 'hyena.backbone.layers.0.norm1.bias', 'hyena.backbone.layers.0.mlp.fc1.weight', 'hyena.backbone.layers.0.mlp.fc1.bias', 'hyena.backbone.layers.0.mlp.fc2.weight', 'hyena.backbone.layers.0.mlp.fc2.bias', 'hyena.backbone.layers.0.norm2.weight', 'hyena.backbone.layers.0.norm2.bias', 'hyena.backbone.layers.1.mixer.in_proj.weight', 'hyena.backbone.layers.1.mixer.in_proj.bias', 'hyena.backbone.layers.1.mixer.out_proj.weight', 'hyena.backbone.layers.1.mixer.out_proj.bias', 'hyena.backbone.layers.1.mixer.short_filter.weight', 'hyena.backbone.layers.1.mixer.short_filter.bias', 'hyena.backbone.layers.1.mixer.filter_fn.bias', 'hyena.backbone.layers.1.mixer.filter_fn.pos_emb.z', 'hyena.backbone.layers.1.mixer.filter_fn.pos_emb.t', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.0.weight', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.0.bias', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.1.freq', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.2.weight', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.2.bias', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.3.freq', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.4.weight', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.4.bias', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.5.freq', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.6.weight', 'hyena.backbone.layers.1.mixer.filter_fn.modulation.deltas', 'hyena.backbone.layers.1.norm1.weight', 'hyena.backbone.layers.1.norm1.bias', 'hyena.backbone.layers.1.mlp.fc1.weight', 'hyena.backbone.layers.1.mlp.fc1.bias', 'hyena.backbone.layers.1.mlp.fc2.weight', 'hyena.backbone.layers.1.mlp.fc2.bias', 'hyena.backbone.layers.1.norm2.weight', 'hyena.backbone.layers.1.norm2.bias', 'hyena.backbone.ln_f.weight', 'hyena.backbone.ln_f.bias', 'score.weight'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HyenaDNAForTokenClassification(\n",
       "  (hyena): HyenaDNAModel(\n",
       "    (backbone): HyenaLMBackbone(\n",
       "      (embeddings): HyenaEmbeddings(\n",
       "        (word_embeddings): Embedding(16, 128)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x HyenaBlock(\n",
       "          (mixer): HyenaOperator(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (in_proj): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (short_filter): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(2,), groups=384)\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (pos_emb): HyenaPositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "                (1): HyenaSin()\n",
       "                (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (3): HyenaSin()\n",
       "                (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (5): HyenaSin()\n",
       "                (6): Linear(in_features=64, out_features=128, bias=False)\n",
       "              )\n",
       "              (modulation): HyenaExponentialModulation()\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): HyenaMlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=False)\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"LongSafari/hyenadna-tiny-1k-seqlen-hf\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# SequenceClassification model to get the \"hyena\" + head score\n",
    "config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "config.num_labels = 2\n",
    "seq_model, seq_loading_info = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    trust_remote_code=True,\n",
    "    output_loading_info=True,\n",
    ")\n",
    "# print(config)\n",
    "# TokenClassificationPerToken without loading weights\n",
    "token_model = HyenaDNAForTokenClassification(config)\n",
    "\n",
    "# weights from seq_model to token_model\n",
    "seq_sd = seq_model.state_dict()\n",
    "token_sd = token_model.state_dict()\n",
    "\n",
    "# Backbone: all parameters \"hyena.\"\n",
    "for k, v in seq_sd.items():\n",
    "    if k.startswith(\"hyena.\"):\n",
    "        token_sd[k] = v.clone()\n",
    "print(seq_sd.keys())\n",
    "# rename score → classifier\n",
    "token_sd[\"classifier.weight\"] = seq_sd[\"score.weight\"].clone()\n",
    "\n",
    "missing, unexpected = token_model.load_state_dict(token_sd, strict=False)\n",
    "\n",
    "token_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224437a3",
   "metadata": {},
   "source": [
    "Check if models have some weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b100f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model.to(device)\n",
    "for p1, p2 in zip(seq_model.parameters(), token_model.parameters()):\n",
    "    if p1.data.ne(p2.data).sum() > 0:\n",
    "        print(\"Parameters are not equal\")\n",
    "        print(\"parameter name:\", p1.name)\n",
    "        print(\"parameter name:\", p2.name)\n",
    "for p1, p2 in zip(seq_sd.keys(), token_sd.keys()):\n",
    "    if p1 != p2 and p2 != \"classifier.weight\" and p1 != \"score.weight\":\n",
    "        print(\"Parameters are not equal\")\n",
    "        print(\"parameter name:\", p1)\n",
    "        print(\"parameter name:\", p2)\n",
    "for (name1, param1), (name2, param2) in zip(seq_sd.items(), token_sd.items()):\n",
    "    if name2 != \"classifier.weight\" and name1 != \"score.weight\":\n",
    "        if not torch.equal(param1, param2):\n",
    "            print(\"Parameters are not equal\")\n",
    "            print(\"parameter name in seq_sd:\", name1)\n",
    "            print(\"parameter name in token_sd:\", name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd94bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['hyena.backbone.embeddings.word_embeddings.weight', 'hyena.backbone.layers.0.mixer.in_proj.weight', 'hyena.backbone.layers.0.mixer.in_proj.bias', 'hyena.backbone.layers.0.mixer.out_proj.weight', 'hyena.backbone.layers.0.mixer.out_proj.bias', 'hyena.backbone.layers.0.mixer.short_filter.weight', 'hyena.backbone.layers.0.mixer.short_filter.bias', 'hyena.backbone.layers.0.mixer.filter_fn.bias', 'hyena.backbone.layers.0.mixer.filter_fn.pos_emb.z', 'hyena.backbone.layers.0.mixer.filter_fn.pos_emb.t', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.0.weight', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.0.bias', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.1.freq', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.2.weight', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.2.bias', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.3.freq', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.4.weight', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.4.bias', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.5.freq', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.6.weight', 'hyena.backbone.layers.0.mixer.filter_fn.modulation.deltas', 'hyena.backbone.layers.0.norm1.weight', 'hyena.backbone.layers.0.norm1.bias', 'hyena.backbone.layers.0.mlp.fc1.weight', 'hyena.backbone.layers.0.mlp.fc1.bias', 'hyena.backbone.layers.0.mlp.fc2.weight', 'hyena.backbone.layers.0.mlp.fc2.bias', 'hyena.backbone.layers.0.norm2.weight', 'hyena.backbone.layers.0.norm2.bias', 'hyena.backbone.layers.1.mixer.in_proj.weight', 'hyena.backbone.layers.1.mixer.in_proj.bias', 'hyena.backbone.layers.1.mixer.out_proj.weight', 'hyena.backbone.layers.1.mixer.out_proj.bias', 'hyena.backbone.layers.1.mixer.short_filter.weight', 'hyena.backbone.layers.1.mixer.short_filter.bias', 'hyena.backbone.layers.1.mixer.filter_fn.bias', 'hyena.backbone.layers.1.mixer.filter_fn.pos_emb.z', 'hyena.backbone.layers.1.mixer.filter_fn.pos_emb.t', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.0.weight', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.0.bias', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.1.freq', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.2.weight', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.2.bias', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.3.freq', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.4.weight', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.4.bias', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.5.freq', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.6.weight', 'hyena.backbone.layers.1.mixer.filter_fn.modulation.deltas', 'hyena.backbone.layers.1.norm1.weight', 'hyena.backbone.layers.1.norm1.bias', 'hyena.backbone.layers.1.mlp.fc1.weight', 'hyena.backbone.layers.1.mlp.fc1.bias', 'hyena.backbone.layers.1.mlp.fc2.weight', 'hyena.backbone.layers.1.mlp.fc2.bias', 'hyena.backbone.layers.1.norm2.weight', 'hyena.backbone.layers.1.norm2.bias', 'hyena.backbone.ln_f.weight', 'hyena.backbone.ln_f.bias', 'score.weight'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_sd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eff790",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_intervals = ints_in + ints_out\n",
    "\n",
    "# 20 % for test\n",
    "train_val, test_intervals = train_test_split(\n",
    "    all_intervals,\n",
    "    test_size=0.2,\n",
    "    stratify=[f\"{label}_{chrom}\" for chrom, _, _, label in all_intervals],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# 80 % for train + val\n",
    "train_intervals, val_intervals = train_test_split(\n",
    "    train_val,\n",
    "    test_size=0.25,  # 0.25 от 80% = 20% от всех\n",
    "    stratify=[f\"{label}_{chrom}\" for chrom, _, _, label in train_val],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "train_ds = DNATokenClassificationDataset(\n",
    "    chroms=chroms,\n",
    "    dna_source=DNA,\n",
    "    labels_source=ZDNA,\n",
    "    intervals=[(c, s, e) for c, s, e, _ in train_intervals],\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=width,\n",
    ")\n",
    "val_ds = DNATokenClassificationDataset(\n",
    "    chroms=chroms,\n",
    "    dna_source=DNA,\n",
    "    labels_source=ZDNA,\n",
    "    intervals=[(c, s, e) for c, s, e, _ in val_intervals],\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=width,\n",
    ")\n",
    "test_ds = DNATokenClassificationDataset(\n",
    "    chroms=chroms,\n",
    "    dna_source=DNA,\n",
    "    labels_source=ZDNA,\n",
    "    intervals=[(c, s, e) for c, s, e, _ in test_intervals],\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=width,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "27ac0c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([ 9,  9,  7,  8,  7,  7, 10,  9,  9, 10,  9, 10,  9, 10,  9, 10,  9,  8,\n",
      "         9, 10,  8, 10,  9, 10, 10,  9,  9,  9,  7,  7,  9,  9,  8,  8, 10, 10,\n",
      "         8,  8, 10,  9,  8,  7,  9, 10,  9,  9, 10,  9,  8, 10,  9,  7, 10,  9,\n",
      "         8, 10,  9,  8,  8,  9,  8,  8,  8,  9,  7, 10,  7,  8,  8,  8,  7,  8,\n",
      "         7,  9,  9,  7, 10, 10,  7,  9,  9,  8, 10,  9,  7,  9,  7,  8, 10,  9,\n",
      "         8,  7,  8, 10,  9,  8,  8,  8,  8, 10,  1])\n",
      "tensor([   0,    0,    0,    0,    0,    0,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0, -100])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_ds)):\n",
    "    if sum(train_ds[i]['labels'].numpy()) != 0:\n",
    "        print(i, train_ds[i]['input_ids'])\n",
    "        print(train_ds[i]['labels'])\n",
    "        print(train_ds[i]['attention_mask'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb318586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GGACAATGGTGTGTGTGCGTCTGTTGGGAAGGCCTTCCTGCAGTGGTGCTGATGCTGCCGCCCGATACCCACAGGATTAGGCTGAGACTGCACTGCCCCT'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]['seq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9745eccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input_ids: 101\n",
      "Length of labels: 101\n",
      "Length of attention_mask: 101\n"
     ]
    }
   ],
   "source": [
    "example = train_ds[1]\n",
    "print(\"Length of input_ids:\", len(example[\"input_ids\"]))\n",
    "print(\"Length of labels:\", len(example[\"labels\"]))\n",
    "print(\"Length of attention_mask:\", len(example[\"attention_mask\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eb865c",
   "metadata": {},
   "source": [
    "# Defining function for metrics calculations\n",
    "Evaluated using accuracy, F1 (binary, macro, micro, weighted), precision, recall, ROC AUC, and Matthews correlation.  \n",
    "In some cases did not calculate all metrics, but the most important for every case of experiments are F1 binary and MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3303b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics2(p):\n",
    "    preds = p.predictions.argmax(-1).flatten()\n",
    "    labs = p.label_ids.flatten()\n",
    "    mask = labs != -100\n",
    "    preds = preds[mask]\n",
    "    labs = labs[mask]\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labs, preds),\n",
    "        \"f1\": f1_score(labs, preds, average=\"binary\"),\n",
    "        \"roc_auc\": roc_auc_score(labs, preds),\n",
    "        \"matthews\": matthews_corrcoef(labs, preds),\n",
    "        \"precision\": precision_metric.compute(predictions=preds, references=labs)[\n",
    "            \"precision\"\n",
    "        ],\n",
    "        \"recall\": recall_metric.compute(predictions=preds, references=labs)[\"recall\"],\n",
    "        \"f1_macro\": f1_metric.compute(\n",
    "            predictions=preds, references=labs, average=\"macro\"\n",
    "        )[\"f1\"],\n",
    "        \"f1_micro\": f1_metric.compute(\n",
    "            predictions=preds, references=labs, average=\"micro\"\n",
    "        )[\"f1\"],\n",
    "        \"f1_weighted\": f1_metric.compute(\n",
    "            predictions=preds, references=labs, average=\"weighted\"\n",
    "        )[\"f1\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a7ebed",
   "metadata": {},
   "source": [
    "# Experiments for finding best params for Z-DNA task without PEFT methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ab37b7",
   "metadata": {},
   "source": [
    "## Important note before checking code below\n",
    "- Since the dataset is quite large, the time required for training is also not small, so some training was stopped by me when I realised that it would not bring any improvement in the last \n",
    "epochs.\n",
    "- It may look dirty, but I specifically titled experiments everywhere for better understanding, in particular, I highlighted the settings from the best training.\n",
    "- TLDR of exps below.\n",
    "\n",
    "### Overview of Conducted Experiments\n",
    "\n",
    "Several experiments were carried out to evaluate the effect of different configurations on model performance, including:\n",
    "* **Sequence length variations** — from short (100) to long (1024) inputs.\n",
    "* **Batch sizes** — tested values included 8, 16, 32, 64, 128 for performance vs. generalization.\n",
    "* **Loss functions** — standard CrossEntropy with class weights and Focal Loss.\n",
    "* **Data augmentation** — including random cropping (±20), overlaps.\n",
    "* **Reverse-complement augmentation** — attempted to randomly reverse-complement input sequences on-the-fly (i.e. half the mini-batches were fed their reverse-complement to enforce strand-invariance).\n",
    "* **Freezing strategies** — two-stage training with frozen backbone followed by fine-tuning.\n",
    "* **Model head variants** — replaced the classification head with Electra-small (for model 32k) in some runs.\n",
    "* **Learning rate schedules** — linear warmup, cosine decay, constant with warmup and dynamic `ReduceLROnPlateau`.\n",
    "* **Tokenization setups** — tested with and without special tokens.\n",
    "* **Data division** — compared classic stratified split vs. stratified K-Fold cross-validation.\n",
    "Each configuration was chosen to test a specific hypothesis about performance scaling, optimization stability, or imbalance handling.\n",
    "\n",
    "#### Result\n",
    "Best F1 and MCC were achieved in the following configuration:  \n",
    "Loss weighting: CE = 0.7 (negative class) and 2 (positive class); len 100, batch 64, no augmentation, classic HyenaDNA tokenization, classic stratfied split train/test/val 60/20/20 model trained in two stages:\n",
    "\n",
    "- **Stage 1 (Frozen Backbone):**  \n",
    "  Trained only the classification head for 3 epochs using LR = 1e-3.  \n",
    "  All Hyena backbone layers were frozen to allow the classifier to quickly specialize.\n",
    "\n",
    "- **Stage 2 (Full Fine-Tuning):**  \n",
    "  Unfroze the entire model and fine-tuned for 6 more epochs.  \n",
    "  Used discriminative learning rates: 1e-5 for the backbone and 5e-4 for the head.  \n",
    "  Achieved F1 score of **0.64** on the evaluation set.\n",
    "\n",
    "**Why this worked best:**  \n",
    "We should not to forget that HyenaDNA has no own head to token-classification task, so I coded it by my own -> no pretrained wheights for this classification task. This setup effectively combined a good initialization (thanks to the frozen backbone) with targeted fine-tuning using class-aware loss. The discriminative learning rates prevented catastrophic forgetting in the pretrained backbone while letting the head adapt aggressively. Balanced class weights helped counteract the heavy class imbalance, while two-stage training ensured both stability and flexibility in learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98147786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of HyenaDNAForSequenceClassification were not initialized from the model checkpoint at ./models/hyenadna-small-32k_len100_bs64_freeze_3_epochs_unfreeze_6_epochs_bestmodel and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration:\n",
      "HyenaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"./models/hyenadna-small-32k_len100_bs64_freeze_3_epochs_unfreeze_6_epochs_bestmodel\",\n",
      "  \"activation_freq\": 10,\n",
      "  \"architectures\": [\n",
      "    \"HyenaDNAForTokenClassificationPerToken\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"LongSafari/hyenadna-small-32k-seqlen-hf--configuration_hyena.HyenaConfig\",\n",
      "    \"AutoModel\": \"LongSafari/hyenadna-small-32k-seqlen-hf--modeling_hyena.HyenaDNAModel\",\n",
      "    \"AutoModelForCausalLM\": \"LongSafari/hyenadna-small-32k-seqlen-hf--modeling_hyena.HyenaDNAForCausalLM\",\n",
      "    \"AutoModelForSequenceClassification\": \"LongSafari/hyenadna-small-32k-seqlen-hf--modeling_hyena.HyenaDNAForSequenceClassification\"\n",
      "  },\n",
      "  \"d_inner\": 1024,\n",
      "  \"d_model\": 256,\n",
      "  \"emb_dim\": 5,\n",
      "  \"embed_dropout\": 0.1,\n",
      "  \"filter_order\": 64,\n",
      "  \"hyena_dropout\": 0.0,\n",
      "  \"hyena_filter_dropout\": 0.0,\n",
      "  \"hyena_order\": 2,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_seq_len\": 32770,\n",
      "  \"model_type\": \"hyenadna\",\n",
      "  \"n_layer\": 4,\n",
      "  \"num_inner_mlps\": 2,\n",
      "  \"pad_token_id\": 4,\n",
      "  \"pad_vocab_size_multiple\": 8,\n",
      "  \"short_filter_order\": 3,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"train_freq\": true,\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_bias\": true,\n",
      "  \"vocab_size\": 12\n",
      "}\n",
      "\n",
      "\n",
      "Model state_dict keys (weights components):\n",
      "['hyena.backbone.embeddings.word_embeddings.weight', 'hyena.backbone.layers.0.mixer.in_proj.weight', 'hyena.backbone.layers.0.mixer.in_proj.bias', 'hyena.backbone.layers.0.mixer.out_proj.weight', 'hyena.backbone.layers.0.mixer.out_proj.bias', 'hyena.backbone.layers.0.mixer.short_filter.weight', 'hyena.backbone.layers.0.mixer.short_filter.bias', 'hyena.backbone.layers.0.mixer.filter_fn.bias', 'hyena.backbone.layers.0.mixer.filter_fn.pos_emb.z', 'hyena.backbone.layers.0.mixer.filter_fn.pos_emb.t', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.0.weight', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.0.bias', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.1.freq', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.2.weight', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.2.bias', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.3.freq', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.4.weight', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.4.bias', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.5.freq', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.6.weight', 'hyena.backbone.layers.0.mixer.filter_fn.modulation.deltas', 'hyena.backbone.layers.0.norm1.weight', 'hyena.backbone.layers.0.norm1.bias', 'hyena.backbone.layers.0.mlp.fc1.weight', 'hyena.backbone.layers.0.mlp.fc1.bias', 'hyena.backbone.layers.0.mlp.fc2.weight', 'hyena.backbone.layers.0.mlp.fc2.bias', 'hyena.backbone.layers.0.norm2.weight', 'hyena.backbone.layers.0.norm2.bias', 'hyena.backbone.layers.1.mixer.in_proj.weight', 'hyena.backbone.layers.1.mixer.in_proj.bias', 'hyena.backbone.layers.1.mixer.out_proj.weight', 'hyena.backbone.layers.1.mixer.out_proj.bias', 'hyena.backbone.layers.1.mixer.short_filter.weight', 'hyena.backbone.layers.1.mixer.short_filter.bias', 'hyena.backbone.layers.1.mixer.filter_fn.bias', 'hyena.backbone.layers.1.mixer.filter_fn.pos_emb.z', 'hyena.backbone.layers.1.mixer.filter_fn.pos_emb.t', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.0.weight', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.0.bias', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.1.freq', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.2.weight', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.2.bias', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.3.freq', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.4.weight', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.4.bias', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.5.freq', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.6.weight', 'hyena.backbone.layers.1.mixer.filter_fn.modulation.deltas', 'hyena.backbone.layers.1.norm1.weight', 'hyena.backbone.layers.1.norm1.bias', 'hyena.backbone.layers.1.mlp.fc1.weight', 'hyena.backbone.layers.1.mlp.fc1.bias', 'hyena.backbone.layers.1.mlp.fc2.weight', 'hyena.backbone.layers.1.mlp.fc2.bias', 'hyena.backbone.layers.1.norm2.weight', 'hyena.backbone.layers.1.norm2.bias', 'hyena.backbone.layers.2.mixer.in_proj.weight', 'hyena.backbone.layers.2.mixer.in_proj.bias', 'hyena.backbone.layers.2.mixer.out_proj.weight', 'hyena.backbone.layers.2.mixer.out_proj.bias', 'hyena.backbone.layers.2.mixer.short_filter.weight', 'hyena.backbone.layers.2.mixer.short_filter.bias', 'hyena.backbone.layers.2.mixer.filter_fn.bias', 'hyena.backbone.layers.2.mixer.filter_fn.pos_emb.z', 'hyena.backbone.layers.2.mixer.filter_fn.pos_emb.t', 'hyena.backbone.layers.2.mixer.filter_fn.implicit_filter.0.weight', 'hyena.backbone.layers.2.mixer.filter_fn.implicit_filter.0.bias', 'hyena.backbone.layers.2.mixer.filter_fn.implicit_filter.1.freq', 'hyena.backbone.layers.2.mixer.filter_fn.implicit_filter.2.weight', 'hyena.backbone.layers.2.mixer.filter_fn.implicit_filter.2.bias', 'hyena.backbone.layers.2.mixer.filter_fn.implicit_filter.3.freq', 'hyena.backbone.layers.2.mixer.filter_fn.implicit_filter.4.weight', 'hyena.backbone.layers.2.mixer.filter_fn.implicit_filter.4.bias', 'hyena.backbone.layers.2.mixer.filter_fn.implicit_filter.5.freq', 'hyena.backbone.layers.2.mixer.filter_fn.implicit_filter.6.weight', 'hyena.backbone.layers.2.mixer.filter_fn.modulation.deltas', 'hyena.backbone.layers.2.norm1.weight', 'hyena.backbone.layers.2.norm1.bias', 'hyena.backbone.layers.2.mlp.fc1.weight', 'hyena.backbone.layers.2.mlp.fc1.bias', 'hyena.backbone.layers.2.mlp.fc2.weight', 'hyena.backbone.layers.2.mlp.fc2.bias', 'hyena.backbone.layers.2.norm2.weight', 'hyena.backbone.layers.2.norm2.bias', 'hyena.backbone.layers.3.mixer.in_proj.weight', 'hyena.backbone.layers.3.mixer.in_proj.bias', 'hyena.backbone.layers.3.mixer.out_proj.weight', 'hyena.backbone.layers.3.mixer.out_proj.bias', 'hyena.backbone.layers.3.mixer.short_filter.weight', 'hyena.backbone.layers.3.mixer.short_filter.bias', 'hyena.backbone.layers.3.mixer.filter_fn.bias', 'hyena.backbone.layers.3.mixer.filter_fn.pos_emb.z', 'hyena.backbone.layers.3.mixer.filter_fn.pos_emb.t', 'hyena.backbone.layers.3.mixer.filter_fn.implicit_filter.0.weight', 'hyena.backbone.layers.3.mixer.filter_fn.implicit_filter.0.bias', 'hyena.backbone.layers.3.mixer.filter_fn.implicit_filter.1.freq', 'hyena.backbone.layers.3.mixer.filter_fn.implicit_filter.2.weight', 'hyena.backbone.layers.3.mixer.filter_fn.implicit_filter.2.bias', 'hyena.backbone.layers.3.mixer.filter_fn.implicit_filter.3.freq', 'hyena.backbone.layers.3.mixer.filter_fn.implicit_filter.4.weight', 'hyena.backbone.layers.3.mixer.filter_fn.implicit_filter.4.bias', 'hyena.backbone.layers.3.mixer.filter_fn.implicit_filter.5.freq', 'hyena.backbone.layers.3.mixer.filter_fn.implicit_filter.6.weight', 'hyena.backbone.layers.3.mixer.filter_fn.modulation.deltas', 'hyena.backbone.layers.3.norm1.weight', 'hyena.backbone.layers.3.norm1.bias', 'hyena.backbone.layers.3.mlp.fc1.weight', 'hyena.backbone.layers.3.mlp.fc1.bias', 'hyena.backbone.layers.3.mlp.fc2.weight', 'hyena.backbone.layers.3.mlp.fc2.bias', 'hyena.backbone.layers.3.norm2.weight', 'hyena.backbone.layers.3.norm2.bias', 'hyena.backbone.ln_f.weight', 'hyena.backbone.ln_f.bias', 'score.weight']\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"./models/hyenadna-small-32k_len100_bs64_freeze_3_epochs_unfreeze_6_epochs_bestmodel\"\n",
    "config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_dir, config=config, trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "print(\"Model configuration:\")\n",
    "print(model.config)\n",
    "\n",
    "print(\"\\nModel state_dict keys (weights components):\")\n",
    "print(list(model.state_dict().keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509eacf",
   "metadata": {},
   "source": [
    "## Experiment: Epochs 35, Seq Length 100, Batch 64, CE=0.7 and 2, F1 = 0.59 \n",
    "Performed standard training with batch size = 64, sequence length = 100, cosine lr 2e-5, reverse complement augmentation and 10% warmup.  \n",
    "Used class-weighted cross-entropy loss (CE=0.7 and 2) with two-class weights.  \n",
    "Trained for 100 epochs; achieved F1 score of **0.59**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237bd654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_36532\\303236470.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='59197' max='168000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 59197/168000 1:25:14 < 2:36:40, 11.57 it/s, Epoch 35.24/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.408000</td>\n",
       "      <td>0.244385</td>\n",
       "      <td>0.951724</td>\n",
       "      <td>0.507290</td>\n",
       "      <td>0.747614</td>\n",
       "      <td>0.482142</td>\n",
       "      <td>0.493363</td>\n",
       "      <td>0.522025</td>\n",
       "      <td>0.740954</td>\n",
       "      <td>0.951724</td>\n",
       "      <td>0.952370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.221400</td>\n",
       "      <td>0.207409</td>\n",
       "      <td>0.956845</td>\n",
       "      <td>0.541600</td>\n",
       "      <td>0.756706</td>\n",
       "      <td>0.518997</td>\n",
       "      <td>0.547833</td>\n",
       "      <td>0.535507</td>\n",
       "      <td>0.759478</td>\n",
       "      <td>0.956845</td>\n",
       "      <td>0.956611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.198500</td>\n",
       "      <td>0.191669</td>\n",
       "      <td>0.958343</td>\n",
       "      <td>0.559971</td>\n",
       "      <td>0.767590</td>\n",
       "      <td>0.538118</td>\n",
       "      <td>0.563214</td>\n",
       "      <td>0.556764</td>\n",
       "      <td>0.769054</td>\n",
       "      <td>0.958343</td>\n",
       "      <td>0.958229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.186600</td>\n",
       "      <td>0.183140</td>\n",
       "      <td>0.959272</td>\n",
       "      <td>0.569090</td>\n",
       "      <td>0.771950</td>\n",
       "      <td>0.547734</td>\n",
       "      <td>0.573327</td>\n",
       "      <td>0.564915</td>\n",
       "      <td>0.773858</td>\n",
       "      <td>0.959272</td>\n",
       "      <td>0.959129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.178264</td>\n",
       "      <td>0.961519</td>\n",
       "      <td>0.579742</td>\n",
       "      <td>0.769615</td>\n",
       "      <td>0.560101</td>\n",
       "      <td>0.603814</td>\n",
       "      <td>0.557515</td>\n",
       "      <td>0.779789</td>\n",
       "      <td>0.961519</td>\n",
       "      <td>0.960789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.176200</td>\n",
       "      <td>0.176453</td>\n",
       "      <td>0.962042</td>\n",
       "      <td>0.583386</td>\n",
       "      <td>0.770232</td>\n",
       "      <td>0.564172</td>\n",
       "      <td>0.610908</td>\n",
       "      <td>0.558236</td>\n",
       "      <td>0.781750</td>\n",
       "      <td>0.962042</td>\n",
       "      <td>0.961228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.174000</td>\n",
       "      <td>0.174430</td>\n",
       "      <td>0.962199</td>\n",
       "      <td>0.585364</td>\n",
       "      <td>0.771378</td>\n",
       "      <td>0.566215</td>\n",
       "      <td>0.612566</td>\n",
       "      <td>0.560476</td>\n",
       "      <td>0.782781</td>\n",
       "      <td>0.962199</td>\n",
       "      <td>0.961400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.172700</td>\n",
       "      <td>0.174377</td>\n",
       "      <td>0.959412</td>\n",
       "      <td>0.581771</td>\n",
       "      <td>0.785349</td>\n",
       "      <td>0.560560</td>\n",
       "      <td>0.570987</td>\n",
       "      <td>0.592969</td>\n",
       "      <td>0.780221</td>\n",
       "      <td>0.959412</td>\n",
       "      <td>0.959775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.171900</td>\n",
       "      <td>0.172400</td>\n",
       "      <td>0.961752</td>\n",
       "      <td>0.588381</td>\n",
       "      <td>0.777662</td>\n",
       "      <td>0.568529</td>\n",
       "      <td>0.603281</td>\n",
       "      <td>0.574198</td>\n",
       "      <td>0.784162</td>\n",
       "      <td>0.961752</td>\n",
       "      <td>0.961303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.171200</td>\n",
       "      <td>0.170973</td>\n",
       "      <td>0.961259</td>\n",
       "      <td>0.588701</td>\n",
       "      <td>0.781288</td>\n",
       "      <td>0.568413</td>\n",
       "      <td>0.595163</td>\n",
       "      <td>0.582378</td>\n",
       "      <td>0.784187</td>\n",
       "      <td>0.961259</td>\n",
       "      <td>0.961059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.170100</td>\n",
       "      <td>0.171330</td>\n",
       "      <td>0.962601</td>\n",
       "      <td>0.590928</td>\n",
       "      <td>0.774881</td>\n",
       "      <td>0.571907</td>\n",
       "      <td>0.616482</td>\n",
       "      <td>0.567407</td>\n",
       "      <td>0.785666</td>\n",
       "      <td>0.962601</td>\n",
       "      <td>0.961862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.170614</td>\n",
       "      <td>0.960977</td>\n",
       "      <td>0.589661</td>\n",
       "      <td>0.784257</td>\n",
       "      <td>0.569176</td>\n",
       "      <td>0.590384</td>\n",
       "      <td>0.588940</td>\n",
       "      <td>0.784588</td>\n",
       "      <td>0.960977</td>\n",
       "      <td>0.960955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.169000</td>\n",
       "      <td>0.171023</td>\n",
       "      <td>0.962649</td>\n",
       "      <td>0.592062</td>\n",
       "      <td>0.775823</td>\n",
       "      <td>0.573025</td>\n",
       "      <td>0.616677</td>\n",
       "      <td>0.569337</td>\n",
       "      <td>0.786245</td>\n",
       "      <td>0.962649</td>\n",
       "      <td>0.961939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.168500</td>\n",
       "      <td>0.170820</td>\n",
       "      <td>0.961572</td>\n",
       "      <td>0.590771</td>\n",
       "      <td>0.781575</td>\n",
       "      <td>0.570675</td>\n",
       "      <td>0.599135</td>\n",
       "      <td>0.582636</td>\n",
       "      <td>0.785305</td>\n",
       "      <td>0.961572</td>\n",
       "      <td>0.961317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.167900</td>\n",
       "      <td>0.170513</td>\n",
       "      <td>0.961937</td>\n",
       "      <td>0.591411</td>\n",
       "      <td>0.779864</td>\n",
       "      <td>0.571613</td>\n",
       "      <td>0.604768</td>\n",
       "      <td>0.578631</td>\n",
       "      <td>0.785725</td>\n",
       "      <td>0.961937</td>\n",
       "      <td>0.961537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.167600</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0.962193</td>\n",
       "      <td>0.591663</td>\n",
       "      <td>0.778433</td>\n",
       "      <td>0.572113</td>\n",
       "      <td>0.608945</td>\n",
       "      <td>0.575336</td>\n",
       "      <td>0.785921</td>\n",
       "      <td>0.962193</td>\n",
       "      <td>0.961683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.167300</td>\n",
       "      <td>0.169746</td>\n",
       "      <td>0.961886</td>\n",
       "      <td>0.592423</td>\n",
       "      <td>0.781364</td>\n",
       "      <td>0.572542</td>\n",
       "      <td>0.603393</td>\n",
       "      <td>0.581845</td>\n",
       "      <td>0.786216</td>\n",
       "      <td>0.961886</td>\n",
       "      <td>0.961556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.166400</td>\n",
       "      <td>0.169246</td>\n",
       "      <td>0.961384</td>\n",
       "      <td>0.593159</td>\n",
       "      <td>0.785591</td>\n",
       "      <td>0.572892</td>\n",
       "      <td>0.595031</td>\n",
       "      <td>0.591298</td>\n",
       "      <td>0.786444</td>\n",
       "      <td>0.961384</td>\n",
       "      <td>0.961326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.166400</td>\n",
       "      <td>0.169208</td>\n",
       "      <td>0.962635</td>\n",
       "      <td>0.594796</td>\n",
       "      <td>0.779002</td>\n",
       "      <td>0.575568</td>\n",
       "      <td>0.614809</td>\n",
       "      <td>0.576045</td>\n",
       "      <td>0.787605</td>\n",
       "      <td>0.962635</td>\n",
       "      <td>0.962056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.165800</td>\n",
       "      <td>0.169973</td>\n",
       "      <td>0.959965</td>\n",
       "      <td>0.587975</td>\n",
       "      <td>0.788993</td>\n",
       "      <td>0.567071</td>\n",
       "      <td>0.576396</td>\n",
       "      <td>0.600029</td>\n",
       "      <td>0.783468</td>\n",
       "      <td>0.959965</td>\n",
       "      <td>0.960346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.165700</td>\n",
       "      <td>0.170074</td>\n",
       "      <td>0.962155</td>\n",
       "      <td>0.592942</td>\n",
       "      <td>0.780138</td>\n",
       "      <td>0.573293</td>\n",
       "      <td>0.607609</td>\n",
       "      <td>0.578966</td>\n",
       "      <td>0.786548</td>\n",
       "      <td>0.962155</td>\n",
       "      <td>0.961721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.170659</td>\n",
       "      <td>0.962125</td>\n",
       "      <td>0.593654</td>\n",
       "      <td>0.781161</td>\n",
       "      <td>0.573946</td>\n",
       "      <td>0.606705</td>\n",
       "      <td>0.581153</td>\n",
       "      <td>0.786895</td>\n",
       "      <td>0.962125</td>\n",
       "      <td>0.961737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.165300</td>\n",
       "      <td>0.169869</td>\n",
       "      <td>0.960538</td>\n",
       "      <td>0.590140</td>\n",
       "      <td>0.787737</td>\n",
       "      <td>0.569452</td>\n",
       "      <td>0.583673</td>\n",
       "      <td>0.596751</td>\n",
       "      <td>0.784705</td>\n",
       "      <td>0.960538</td>\n",
       "      <td>0.960746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.164700</td>\n",
       "      <td>0.170471</td>\n",
       "      <td>0.961973</td>\n",
       "      <td>0.592857</td>\n",
       "      <td>0.781271</td>\n",
       "      <td>0.573040</td>\n",
       "      <td>0.604611</td>\n",
       "      <td>0.581552</td>\n",
       "      <td>0.786456</td>\n",
       "      <td>0.961973</td>\n",
       "      <td>0.961622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.164400</td>\n",
       "      <td>0.169951</td>\n",
       "      <td>0.962178</td>\n",
       "      <td>0.593708</td>\n",
       "      <td>0.780857</td>\n",
       "      <td>0.574049</td>\n",
       "      <td>0.607579</td>\n",
       "      <td>0.580455</td>\n",
       "      <td>0.786937</td>\n",
       "      <td>0.962178</td>\n",
       "      <td>0.961768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.164300</td>\n",
       "      <td>0.171613</td>\n",
       "      <td>0.963238</td>\n",
       "      <td>0.594511</td>\n",
       "      <td>0.774581</td>\n",
       "      <td>0.576107</td>\n",
       "      <td>0.625961</td>\n",
       "      <td>0.566070</td>\n",
       "      <td>0.787629</td>\n",
       "      <td>0.963238</td>\n",
       "      <td>0.962359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.163500</td>\n",
       "      <td>0.170253</td>\n",
       "      <td>0.962285</td>\n",
       "      <td>0.594889</td>\n",
       "      <td>0.781485</td>\n",
       "      <td>0.575285</td>\n",
       "      <td>0.608737</td>\n",
       "      <td>0.581657</td>\n",
       "      <td>0.787555</td>\n",
       "      <td>0.962285</td>\n",
       "      <td>0.961877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.170360</td>\n",
       "      <td>0.962111</td>\n",
       "      <td>0.593744</td>\n",
       "      <td>0.781357</td>\n",
       "      <td>0.574021</td>\n",
       "      <td>0.606428</td>\n",
       "      <td>0.581581</td>\n",
       "      <td>0.786937</td>\n",
       "      <td>0.962111</td>\n",
       "      <td>0.961734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.162800</td>\n",
       "      <td>0.170045</td>\n",
       "      <td>0.962493</td>\n",
       "      <td>0.594476</td>\n",
       "      <td>0.779602</td>\n",
       "      <td>0.575106</td>\n",
       "      <td>0.612521</td>\n",
       "      <td>0.577464</td>\n",
       "      <td>0.787407</td>\n",
       "      <td>0.962493</td>\n",
       "      <td>0.961967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.163100</td>\n",
       "      <td>0.170861</td>\n",
       "      <td>0.962147</td>\n",
       "      <td>0.594212</td>\n",
       "      <td>0.781649</td>\n",
       "      <td>0.574504</td>\n",
       "      <td>0.606778</td>\n",
       "      <td>0.582156</td>\n",
       "      <td>0.787180</td>\n",
       "      <td>0.962147</td>\n",
       "      <td>0.961774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.162700</td>\n",
       "      <td>0.170614</td>\n",
       "      <td>0.962214</td>\n",
       "      <td>0.593342</td>\n",
       "      <td>0.780197</td>\n",
       "      <td>0.573734</td>\n",
       "      <td>0.608386</td>\n",
       "      <td>0.579024</td>\n",
       "      <td>0.786764</td>\n",
       "      <td>0.962214</td>\n",
       "      <td>0.961770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.162600</td>\n",
       "      <td>0.171932</td>\n",
       "      <td>0.963095</td>\n",
       "      <td>0.593850</td>\n",
       "      <td>0.774812</td>\n",
       "      <td>0.575290</td>\n",
       "      <td>0.623714</td>\n",
       "      <td>0.566716</td>\n",
       "      <td>0.787260</td>\n",
       "      <td>0.963095</td>\n",
       "      <td>0.962254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.162100</td>\n",
       "      <td>0.170802</td>\n",
       "      <td>0.962085</td>\n",
       "      <td>0.594636</td>\n",
       "      <td>0.782555</td>\n",
       "      <td>0.574858</td>\n",
       "      <td>0.605526</td>\n",
       "      <td>0.584132</td>\n",
       "      <td>0.787374</td>\n",
       "      <td>0.962085</td>\n",
       "      <td>0.961761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.161300</td>\n",
       "      <td>0.170814</td>\n",
       "      <td>0.963314</td>\n",
       "      <td>0.596690</td>\n",
       "      <td>0.776504</td>\n",
       "      <td>0.578212</td>\n",
       "      <td>0.625961</td>\n",
       "      <td>0.570035</td>\n",
       "      <td>0.788737</td>\n",
       "      <td>0.963314</td>\n",
       "      <td>0.962497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.161100</td>\n",
       "      <td>0.171870</td>\n",
       "      <td>0.959385</td>\n",
       "      <td>0.587616</td>\n",
       "      <td>0.792388</td>\n",
       "      <td>0.566627</td>\n",
       "      <td>0.568714</td>\n",
       "      <td>0.607817</td>\n",
       "      <td>0.783128</td>\n",
       "      <td>0.959385</td>\n",
       "      <td>0.960025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36532\\303236470.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcan_return_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2240\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2241\u001b[1;33m             return inner_training_loop(\n\u001b[0m\u001b[0;32m   2242\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2243\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2498\u001b[0m                 \u001b[0mupdate_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2499\u001b[0m                 \u001b[0mnum_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mupdate_step\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtotal_updates\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mremainder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2500\u001b[1;33m                 \u001b[0mbatch_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_batch_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2501\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2502\u001b[0m                     \u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mget_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[0;32m   5178\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5179\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5180\u001b[1;33m                 \u001b[0mbatch_samples\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5181\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5182\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\accelerate\\data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    573\u001b[0m                     \u001b[0mcurrent_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msend_to_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_non_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m                 \u001b[0mnext_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mskip_batches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mcurrent_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    699\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             if (\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/hyenadna1k_finetune_len100_batch64_onlinedatagen\",\n",
    "    label_names=[\"labels\"],\n",
    "    num_train_epochs=100,\n",
    "    learning_rate=2e-5,\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_safetensors=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=token_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "\n",
    "trainer.can_return_loss = True\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e858a064",
   "metadata": {},
   "source": [
    "## Experiment: Seq Length 100, Batch 64, CE=0.7 and 2, lr_scheduler_type constant with warmup F1 = 0.6319\n",
    "Performed standard training with batch size = 64, sequence length = 100, and 10% warmup.  \n",
    "Used class-weighted cross-entropy loss (CE=0.7 and 2) with two-class weights.  \n",
    "Trained for 12 epochs; achieved F1 score of **0.6319**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b673f7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_27580\\1091277043.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer64const = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20160' max='20160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20160/20160 37:12, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.191500</td>\n",
       "      <td>0.232097</td>\n",
       "      <td>0.962903</td>\n",
       "      <td>0.419611</td>\n",
       "      <td>0.639318</td>\n",
       "      <td>0.468421</td>\n",
       "      <td>0.822236</td>\n",
       "      <td>0.281681</td>\n",
       "      <td>0.700225</td>\n",
       "      <td>0.962903</td>\n",
       "      <td>0.954121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.171700</td>\n",
       "      <td>0.163139</td>\n",
       "      <td>0.963946</td>\n",
       "      <td>0.617269</td>\n",
       "      <td>0.796155</td>\n",
       "      <td>0.598391</td>\n",
       "      <td>0.623972</td>\n",
       "      <td>0.610708</td>\n",
       "      <td>0.799175</td>\n",
       "      <td>0.963946</td>\n",
       "      <td>0.963761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.163700</td>\n",
       "      <td>0.161823</td>\n",
       "      <td>0.964870</td>\n",
       "      <td>0.620233</td>\n",
       "      <td>0.792780</td>\n",
       "      <td>0.602116</td>\n",
       "      <td>0.638951</td>\n",
       "      <td>0.602580</td>\n",
       "      <td>0.800908</td>\n",
       "      <td>0.964870</td>\n",
       "      <td>0.964380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.161500</td>\n",
       "      <td>0.159412</td>\n",
       "      <td>0.964509</td>\n",
       "      <td>0.626634</td>\n",
       "      <td>0.803518</td>\n",
       "      <td>0.608004</td>\n",
       "      <td>0.627687</td>\n",
       "      <td>0.625585</td>\n",
       "      <td>0.804002</td>\n",
       "      <td>0.964509</td>\n",
       "      <td>0.964481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.158700</td>\n",
       "      <td>0.159709</td>\n",
       "      <td>0.960497</td>\n",
       "      <td>0.614930</td>\n",
       "      <td>0.818960</td>\n",
       "      <td>0.595934</td>\n",
       "      <td>0.573711</td>\n",
       "      <td>0.662529</td>\n",
       "      <td>0.797055</td>\n",
       "      <td>0.960497</td>\n",
       "      <td>0.961840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.159537</td>\n",
       "      <td>0.966509</td>\n",
       "      <td>0.629865</td>\n",
       "      <td>0.791735</td>\n",
       "      <td>0.613296</td>\n",
       "      <td>0.664614</td>\n",
       "      <td>0.598569</td>\n",
       "      <td>0.806163</td>\n",
       "      <td>0.966509</td>\n",
       "      <td>0.965675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.155700</td>\n",
       "      <td>0.157755</td>\n",
       "      <td>0.964034</td>\n",
       "      <td>0.627552</td>\n",
       "      <td>0.808435</td>\n",
       "      <td>0.608726</td>\n",
       "      <td>0.618887</td>\n",
       "      <td>0.636463</td>\n",
       "      <td>0.804328</td>\n",
       "      <td>0.964034</td>\n",
       "      <td>0.964273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.154600</td>\n",
       "      <td>0.157223</td>\n",
       "      <td>0.963387</td>\n",
       "      <td>0.628612</td>\n",
       "      <td>0.814932</td>\n",
       "      <td>0.609774</td>\n",
       "      <td>0.607841</td>\n",
       "      <td>0.650853</td>\n",
       "      <td>0.804678</td>\n",
       "      <td>0.963387</td>\n",
       "      <td>0.963980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.152900</td>\n",
       "      <td>0.156365</td>\n",
       "      <td>0.965274</td>\n",
       "      <td>0.633095</td>\n",
       "      <td>0.805688</td>\n",
       "      <td>0.614883</td>\n",
       "      <td>0.636928</td>\n",
       "      <td>0.629309</td>\n",
       "      <td>0.807435</td>\n",
       "      <td>0.965274</td>\n",
       "      <td>0.965175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.151600</td>\n",
       "      <td>0.157845</td>\n",
       "      <td>0.961469</td>\n",
       "      <td>0.619946</td>\n",
       "      <td>0.818320</td>\n",
       "      <td>0.600966</td>\n",
       "      <td>0.584392</td>\n",
       "      <td>0.660107</td>\n",
       "      <td>0.799826</td>\n",
       "      <td>0.961469</td>\n",
       "      <td>0.962579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.158403</td>\n",
       "      <td>0.964307</td>\n",
       "      <td>0.630036</td>\n",
       "      <td>0.809493</td>\n",
       "      <td>0.611347</td>\n",
       "      <td>0.621902</td>\n",
       "      <td>0.638386</td>\n",
       "      <td>0.805643</td>\n",
       "      <td>0.964307</td>\n",
       "      <td>0.964529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.148100</td>\n",
       "      <td>0.158044</td>\n",
       "      <td>0.963915</td>\n",
       "      <td>0.626776</td>\n",
       "      <td>0.808370</td>\n",
       "      <td>0.607899</td>\n",
       "      <td>0.617385</td>\n",
       "      <td>0.636457</td>\n",
       "      <td>0.803908</td>\n",
       "      <td>0.963915</td>\n",
       "      <td>0.964175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20160, training_loss=0.15978329825022863, metrics={'train_runtime': 2232.2643, 'train_samples_per_second': 577.674, 'train_steps_per_second': 9.031, 'total_flos': 2558451915694080.0, 'train_loss': 0.15978329825022863, 'epoch': 12.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args64const = TrainingArguments(\n",
    "    output_dir=\"./results/hyenadna32_finetune_len 100_batch64_onlinedatagen\",            \n",
    "    label_names=[\"labels\"],\n",
    "    num_train_epochs=12,                                 \n",
    "    learning_rate=6e-4,                                  \n",
    "    per_device_train_batch_size=64,                     \n",
    "    per_device_eval_batch_size=64,                     \n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,                                   \n",
    "    lr_scheduler_type=\"constant_with_warmup\",                          \n",
    "    warmup_ratio=0.10,                                    \n",
    "    fp16=True,                                           \n",
    "\n",
    "                                   \n",
    "    save_total_limit=3,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "\n",
    "trainer64const = Trainer(\n",
    "    model = token_model, \n",
    "    args=training_args64const,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,   \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "\n",
    "trainer64const.can_return_loss = True\n",
    "trainer64const.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1bcebb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='560' max='560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [560/560 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test metrics ===\n",
      "eval_loss: 0.1566\n",
      "eval_accuracy: 0.9650\n",
      "eval_f1: 0.6319\n",
      "eval_roc_auc: 0.8055\n",
      "eval_matthews: 0.6135\n",
      "eval_precision: 0.6347\n",
      "eval_recall: 0.6291\n",
      "eval_f1_macro: 0.8068\n",
      "eval_f1_micro: 0.9650\n",
      "eval_f1_weighted: 0.9649\n",
      "eval_runtime: 84.5764\n",
      "eval_samples_per_second: 423.5220\n",
      "eval_steps_per_second: 6.6210\n",
      "epoch: 12.0000\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer64const.evaluate(eval_dataset=test_ds)\n",
    "print(\"=== Test metrics ===\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e393821",
   "metadata": {},
   "source": [
    "## Experiment: Seq Length 100, Batch 8, CE=0.7 and 2, F1 =0.6385\n",
    "Performed standard training with batch size = 8, sequence length = 100, and 10% warmup.  \n",
    "Used class-weighted cross-entropy loss (CE=0.7 and 2) with two-class weights.  \n",
    "Trained for 12 epochs; achieved F1 score of **0.6385**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ade48df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_27580\\732634328.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer8 = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: romanbokhyan (romanbokhyan-hse-university). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\1\\vscode-projects\\coursework\\HyenaDNA config\\wandb\\run-20250504_185649-s9al0xrx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/s9al0xrx' target=\"_blank\">./results/hyenadna32_finetune_len256_batch32_onlinedatagen</a></strong> to <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface' target=\"_blank\">https://wandb.ai/romanbokhyan-hse-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/s9al0xrx' target=\"_blank\">https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/s9al0xrx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='161196' max='161196' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [161196/161196 2:09:43, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.186600</td>\n",
       "      <td>0.171516</td>\n",
       "      <td>0.964056</td>\n",
       "      <td>0.592970</td>\n",
       "      <td>0.767353</td>\n",
       "      <td>0.576220</td>\n",
       "      <td>0.643292</td>\n",
       "      <td>0.549950</td>\n",
       "      <td>0.787084</td>\n",
       "      <td>0.964056</td>\n",
       "      <td>0.962715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.167800</td>\n",
       "      <td>0.162957</td>\n",
       "      <td>0.965423</td>\n",
       "      <td>0.615833</td>\n",
       "      <td>0.783358</td>\n",
       "      <td>0.598894</td>\n",
       "      <td>0.653675</td>\n",
       "      <td>0.582132</td>\n",
       "      <td>0.798865</td>\n",
       "      <td>0.965423</td>\n",
       "      <td>0.964470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.161100</td>\n",
       "      <td>0.158945</td>\n",
       "      <td>0.963472</td>\n",
       "      <td>0.619719</td>\n",
       "      <td>0.802787</td>\n",
       "      <td>0.600561</td>\n",
       "      <td>0.614341</td>\n",
       "      <td>0.625192</td>\n",
       "      <td>0.800267</td>\n",
       "      <td>0.963472</td>\n",
       "      <td>0.963624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.159315</td>\n",
       "      <td>0.965307</td>\n",
       "      <td>0.627676</td>\n",
       "      <td>0.798561</td>\n",
       "      <td>0.609651</td>\n",
       "      <td>0.641684</td>\n",
       "      <td>0.614267</td>\n",
       "      <td>0.804741</td>\n",
       "      <td>0.965307</td>\n",
       "      <td>0.964946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.155500</td>\n",
       "      <td>0.153674</td>\n",
       "      <td>0.965055</td>\n",
       "      <td>0.632367</td>\n",
       "      <td>0.806520</td>\n",
       "      <td>0.614023</td>\n",
       "      <td>0.633435</td>\n",
       "      <td>0.631302</td>\n",
       "      <td>0.807011</td>\n",
       "      <td>0.965055</td>\n",
       "      <td>0.965027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.154428</td>\n",
       "      <td>0.966733</td>\n",
       "      <td>0.635842</td>\n",
       "      <td>0.797307</td>\n",
       "      <td>0.619053</td>\n",
       "      <td>0.663910</td>\n",
       "      <td>0.610051</td>\n",
       "      <td>0.809206</td>\n",
       "      <td>0.966733</td>\n",
       "      <td>0.966064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.150900</td>\n",
       "      <td>0.155070</td>\n",
       "      <td>0.966145</td>\n",
       "      <td>0.634478</td>\n",
       "      <td>0.800393</td>\n",
       "      <td>0.617009</td>\n",
       "      <td>0.652752</td>\n",
       "      <td>0.617199</td>\n",
       "      <td>0.808364</td>\n",
       "      <td>0.966145</td>\n",
       "      <td>0.965694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.151955</td>\n",
       "      <td>0.965208</td>\n",
       "      <td>0.636294</td>\n",
       "      <td>0.810386</td>\n",
       "      <td>0.618032</td>\n",
       "      <td>0.633344</td>\n",
       "      <td>0.639272</td>\n",
       "      <td>0.809012</td>\n",
       "      <td>0.965208</td>\n",
       "      <td>0.965285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.146700</td>\n",
       "      <td>0.153037</td>\n",
       "      <td>0.966403</td>\n",
       "      <td>0.639508</td>\n",
       "      <td>0.804690</td>\n",
       "      <td>0.622057</td>\n",
       "      <td>0.653654</td>\n",
       "      <td>0.625960</td>\n",
       "      <td>0.810944</td>\n",
       "      <td>0.966403</td>\n",
       "      <td>0.966057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.144200</td>\n",
       "      <td>0.153800</td>\n",
       "      <td>0.963999</td>\n",
       "      <td>0.630719</td>\n",
       "      <td>0.812846</td>\n",
       "      <td>0.611991</td>\n",
       "      <td>0.616338</td>\n",
       "      <td>0.645787</td>\n",
       "      <td>0.805898</td>\n",
       "      <td>0.963999</td>\n",
       "      <td>0.964397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.141300</td>\n",
       "      <td>0.157328</td>\n",
       "      <td>0.965678</td>\n",
       "      <td>0.633052</td>\n",
       "      <td>0.802368</td>\n",
       "      <td>0.615165</td>\n",
       "      <td>0.644641</td>\n",
       "      <td>0.621873</td>\n",
       "      <td>0.807525</td>\n",
       "      <td>0.965678</td>\n",
       "      <td>0.965385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.138000</td>\n",
       "      <td>0.160962</td>\n",
       "      <td>0.965392</td>\n",
       "      <td>0.629921</td>\n",
       "      <td>0.800698</td>\n",
       "      <td>0.611886</td>\n",
       "      <td>0.641587</td>\n",
       "      <td>0.618671</td>\n",
       "      <td>0.805884</td>\n",
       "      <td>0.965392</td>\n",
       "      <td>0.965093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=161196, training_loss=0.15436139439598184, metrics={'train_runtime': 7788.0604, 'train_samples_per_second': 165.577, 'train_steps_per_second': 20.698, 'total_flos': 2558451915694080.0, 'train_loss': 0.15436139439598184, 'epoch': 12.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args8 = TrainingArguments(\n",
    "    output_dir=\"./results/hyenadna32_finetune_len256_batch32_onlinedatagen\",            \n",
    "    label_names=[\"labels\"],\n",
    "    num_train_epochs=12,                                 \n",
    "    learning_rate=5e-4,                                  \n",
    "    per_device_train_batch_size=8,                     \n",
    "    per_device_eval_batch_size=8,                     \n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,                                   \n",
    "    lr_scheduler_type=\"linear\",                          \n",
    "    warmup_ratio=0.10,                                    \n",
    "    fp16=True,                                           \n",
    "\n",
    "                                   \n",
    "    save_total_limit=3,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "\n",
    "trainer8 = Trainer(\n",
    "    model = token_model, \n",
    "    args=training_args8,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,   \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "\n",
    "trainer8.can_return_loss = True\n",
    "trainer8.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "598fb950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4478' max='4478' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4478/4478 01:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test metrics ===\n",
      "eval_loss: 0.1531\n",
      "eval_accuracy: 0.9662\n",
      "eval_f1: 0.6385\n",
      "eval_roc_auc: 0.8046\n",
      "eval_matthews: 0.6209\n",
      "eval_precision: 0.6516\n",
      "eval_recall: 0.6259\n",
      "eval_f1_macro: 0.8104\n",
      "eval_f1_micro: 0.9662\n",
      "eval_f1_weighted: 0.9658\n",
      "eval_runtime: 137.5481\n",
      "eval_samples_per_second: 260.4180\n",
      "eval_steps_per_second: 32.5560\n",
      "epoch: 12.0000\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer8.evaluate(eval_dataset=test_ds)\n",
    "print(\"=== Test metrics ===\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545fc223",
   "metadata": {},
   "source": [
    "## Experiment: Sequence Length 100, Batch Size 64, 12 Epochs, 10% Warmup, 5-Fold Stratified CV  \n",
    "Used sequence length = 100, batch size = 64, trained for 12 epochs with 10% warmup.  \n",
    "Performed 5-fold stratified cross-validation using chromosome-labeled intervals.  \n",
    "Best checkpoints saved per fold and evaluated on a fixed held-out test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48139db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_6128\\2406559715.py:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21492' max='21492' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21492/21492 36:29, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.189100</td>\n",
       "      <td>0.168801</td>\n",
       "      <td>0.957485</td>\n",
       "      <td>0.590667</td>\n",
       "      <td>0.811080</td>\n",
       "      <td>0.571041</td>\n",
       "      <td>0.541676</td>\n",
       "      <td>0.649401</td>\n",
       "      <td>0.784123</td>\n",
       "      <td>0.957485</td>\n",
       "      <td>0.959302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.169500</td>\n",
       "      <td>0.166243</td>\n",
       "      <td>0.961470</td>\n",
       "      <td>0.602325</td>\n",
       "      <td>0.798127</td>\n",
       "      <td>0.582293</td>\n",
       "      <td>0.587657</td>\n",
       "      <td>0.617744</td>\n",
       "      <td>0.791040</td>\n",
       "      <td>0.961470</td>\n",
       "      <td>0.961926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.162100</td>\n",
       "      <td>0.161663</td>\n",
       "      <td>0.967109</td>\n",
       "      <td>0.624467</td>\n",
       "      <td>0.782652</td>\n",
       "      <td>0.609442</td>\n",
       "      <td>0.677751</td>\n",
       "      <td>0.578950</td>\n",
       "      <td>0.803634</td>\n",
       "      <td>0.967109</td>\n",
       "      <td>0.965875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.158600</td>\n",
       "      <td>0.159789</td>\n",
       "      <td>0.967493</td>\n",
       "      <td>0.630209</td>\n",
       "      <td>0.786406</td>\n",
       "      <td>0.615185</td>\n",
       "      <td>0.681055</td>\n",
       "      <td>0.586427</td>\n",
       "      <td>0.806604</td>\n",
       "      <td>0.967493</td>\n",
       "      <td>0.966335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.156100</td>\n",
       "      <td>0.156834</td>\n",
       "      <td>0.966002</td>\n",
       "      <td>0.631099</td>\n",
       "      <td>0.799523</td>\n",
       "      <td>0.613502</td>\n",
       "      <td>0.647315</td>\n",
       "      <td>0.615676</td>\n",
       "      <td>0.806639</td>\n",
       "      <td>0.966002</td>\n",
       "      <td>0.965596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.153400</td>\n",
       "      <td>0.157069</td>\n",
       "      <td>0.962396</td>\n",
       "      <td>0.622371</td>\n",
       "      <td>0.816810</td>\n",
       "      <td>0.603519</td>\n",
       "      <td>0.591993</td>\n",
       "      <td>0.656036</td>\n",
       "      <td>0.801292</td>\n",
       "      <td>0.962396</td>\n",
       "      <td>0.963310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.150900</td>\n",
       "      <td>0.155473</td>\n",
       "      <td>0.964648</td>\n",
       "      <td>0.632285</td>\n",
       "      <td>0.812013</td>\n",
       "      <td>0.613825</td>\n",
       "      <td>0.621497</td>\n",
       "      <td>0.643454</td>\n",
       "      <td>0.806858</td>\n",
       "      <td>0.964648</td>\n",
       "      <td>0.964940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.147700</td>\n",
       "      <td>0.158532</td>\n",
       "      <td>0.967239</td>\n",
       "      <td>0.635122</td>\n",
       "      <td>0.794446</td>\n",
       "      <td>0.618945</td>\n",
       "      <td>0.670087</td>\n",
       "      <td>0.603626</td>\n",
       "      <td>0.808986</td>\n",
       "      <td>0.967239</td>\n",
       "      <td>0.966425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.143300</td>\n",
       "      <td>0.159195</td>\n",
       "      <td>0.963808</td>\n",
       "      <td>0.627843</td>\n",
       "      <td>0.812931</td>\n",
       "      <td>0.609113</td>\n",
       "      <td>0.610399</td>\n",
       "      <td>0.646313</td>\n",
       "      <td>0.804411</td>\n",
       "      <td>0.963808</td>\n",
       "      <td>0.964299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.137200</td>\n",
       "      <td>0.167014</td>\n",
       "      <td>0.963834</td>\n",
       "      <td>0.621560</td>\n",
       "      <td>0.804606</td>\n",
       "      <td>0.602616</td>\n",
       "      <td>0.614516</td>\n",
       "      <td>0.628767</td>\n",
       "      <td>0.801285</td>\n",
       "      <td>0.963834</td>\n",
       "      <td>0.964031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>0.177082</td>\n",
       "      <td>0.962541</td>\n",
       "      <td>0.609456</td>\n",
       "      <td>0.799181</td>\n",
       "      <td>0.589861</td>\n",
       "      <td>0.600410</td>\n",
       "      <td>0.618778</td>\n",
       "      <td>0.794891</td>\n",
       "      <td>0.962541</td>\n",
       "      <td>0.962809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.120700</td>\n",
       "      <td>0.189715</td>\n",
       "      <td>0.960700</td>\n",
       "      <td>0.594672</td>\n",
       "      <td>0.794202</td>\n",
       "      <td>0.574244</td>\n",
       "      <td>0.579793</td>\n",
       "      <td>0.610334</td>\n",
       "      <td>0.787010</td>\n",
       "      <td>0.960700</td>\n",
       "      <td>0.961179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='560' max='560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [560/560 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_6128\\2406559715.py:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21492' max='21492' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21492/21492 36:02, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.189200</td>\n",
       "      <td>0.176018</td>\n",
       "      <td>0.953620</td>\n",
       "      <td>0.578240</td>\n",
       "      <td>0.815988</td>\n",
       "      <td>0.559346</td>\n",
       "      <td>0.512234</td>\n",
       "      <td>0.663774</td>\n",
       "      <td>0.776851</td>\n",
       "      <td>0.953620</td>\n",
       "      <td>0.956434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.167600</td>\n",
       "      <td>0.163783</td>\n",
       "      <td>0.964936</td>\n",
       "      <td>0.618486</td>\n",
       "      <td>0.788498</td>\n",
       "      <td>0.600736</td>\n",
       "      <td>0.645825</td>\n",
       "      <td>0.593367</td>\n",
       "      <td>0.800055</td>\n",
       "      <td>0.964936</td>\n",
       "      <td>0.964230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.161300</td>\n",
       "      <td>0.160794</td>\n",
       "      <td>0.966751</td>\n",
       "      <td>0.628630</td>\n",
       "      <td>0.786670</td>\n",
       "      <td>0.612963</td>\n",
       "      <td>0.675940</td>\n",
       "      <td>0.587510</td>\n",
       "      <td>0.805613</td>\n",
       "      <td>0.966751</td>\n",
       "      <td>0.965642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.157600</td>\n",
       "      <td>0.157003</td>\n",
       "      <td>0.964665</td>\n",
       "      <td>0.632074</td>\n",
       "      <td>0.807487</td>\n",
       "      <td>0.613518</td>\n",
       "      <td>0.630500</td>\n",
       "      <td>0.633656</td>\n",
       "      <td>0.806758</td>\n",
       "      <td>0.964665</td>\n",
       "      <td>0.964707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.154900</td>\n",
       "      <td>0.155985</td>\n",
       "      <td>0.966116</td>\n",
       "      <td>0.638885</td>\n",
       "      <td>0.804502</td>\n",
       "      <td>0.621268</td>\n",
       "      <td>0.652566</td>\n",
       "      <td>0.625766</td>\n",
       "      <td>0.810555</td>\n",
       "      <td>0.966116</td>\n",
       "      <td>0.965779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.158035</td>\n",
       "      <td>0.965691</td>\n",
       "      <td>0.631160</td>\n",
       "      <td>0.798149</td>\n",
       "      <td>0.613486</td>\n",
       "      <td>0.650590</td>\n",
       "      <td>0.612856</td>\n",
       "      <td>0.806584</td>\n",
       "      <td>0.965691</td>\n",
       "      <td>0.965203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.149600</td>\n",
       "      <td>0.157451</td>\n",
       "      <td>0.966159</td>\n",
       "      <td>0.636744</td>\n",
       "      <td>0.801411</td>\n",
       "      <td>0.619285</td>\n",
       "      <td>0.655302</td>\n",
       "      <td>0.619209</td>\n",
       "      <td>0.809499</td>\n",
       "      <td>0.966159</td>\n",
       "      <td>0.965704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.156956</td>\n",
       "      <td>0.963451</td>\n",
       "      <td>0.630109</td>\n",
       "      <td>0.814574</td>\n",
       "      <td>0.611218</td>\n",
       "      <td>0.611466</td>\n",
       "      <td>0.649925</td>\n",
       "      <td>0.805442</td>\n",
       "      <td>0.963451</td>\n",
       "      <td>0.963979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.140300</td>\n",
       "      <td>0.160297</td>\n",
       "      <td>0.964002</td>\n",
       "      <td>0.630728</td>\n",
       "      <td>0.811020</td>\n",
       "      <td>0.611914</td>\n",
       "      <td>0.620004</td>\n",
       "      <td>0.641830</td>\n",
       "      <td>0.805904</td>\n",
       "      <td>0.964002</td>\n",
       "      <td>0.964297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.132800</td>\n",
       "      <td>0.169597</td>\n",
       "      <td>0.962763</td>\n",
       "      <td>0.620288</td>\n",
       "      <td>0.807114</td>\n",
       "      <td>0.600898</td>\n",
       "      <td>0.606265</td>\n",
       "      <td>0.634975</td>\n",
       "      <td>0.800355</td>\n",
       "      <td>0.962763</td>\n",
       "      <td>0.963172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.123300</td>\n",
       "      <td>0.185381</td>\n",
       "      <td>0.961371</td>\n",
       "      <td>0.605443</td>\n",
       "      <td>0.798686</td>\n",
       "      <td>0.585295</td>\n",
       "      <td>0.592684</td>\n",
       "      <td>0.618765</td>\n",
       "      <td>0.792567</td>\n",
       "      <td>0.961371</td>\n",
       "      <td>0.961765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.113900</td>\n",
       "      <td>0.203094</td>\n",
       "      <td>0.958653</td>\n",
       "      <td>0.586238</td>\n",
       "      <td>0.793820</td>\n",
       "      <td>0.565052</td>\n",
       "      <td>0.562962</td>\n",
       "      <td>0.611523</td>\n",
       "      <td>0.782239</td>\n",
       "      <td>0.958653</td>\n",
       "      <td>0.959463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='560' max='560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [560/560 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_6128\\2406559715.py:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21492' max='21492' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21492/21492 36:43, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.189500</td>\n",
       "      <td>0.168126</td>\n",
       "      <td>0.963402</td>\n",
       "      <td>0.608513</td>\n",
       "      <td>0.789470</td>\n",
       "      <td>0.589440</td>\n",
       "      <td>0.620235</td>\n",
       "      <td>0.597226</td>\n",
       "      <td>0.794658</td>\n",
       "      <td>0.963402</td>\n",
       "      <td>0.963073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.169200</td>\n",
       "      <td>0.166726</td>\n",
       "      <td>0.959652</td>\n",
       "      <td>0.601006</td>\n",
       "      <td>0.806901</td>\n",
       "      <td>0.580921</td>\n",
       "      <td>0.568013</td>\n",
       "      <td>0.638068</td>\n",
       "      <td>0.789879</td>\n",
       "      <td>0.959652</td>\n",
       "      <td>0.960761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.160918</td>\n",
       "      <td>0.962222</td>\n",
       "      <td>0.618126</td>\n",
       "      <td>0.810112</td>\n",
       "      <td>0.598741</td>\n",
       "      <td>0.595975</td>\n",
       "      <td>0.641988</td>\n",
       "      <td>0.799127</td>\n",
       "      <td>0.962222</td>\n",
       "      <td>0.962887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.158700</td>\n",
       "      <td>0.160551</td>\n",
       "      <td>0.961216</td>\n",
       "      <td>0.617308</td>\n",
       "      <td>0.816621</td>\n",
       "      <td>0.598159</td>\n",
       "      <td>0.582292</td>\n",
       "      <td>0.656804</td>\n",
       "      <td>0.798440</td>\n",
       "      <td>0.961216</td>\n",
       "      <td>0.962320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.155800</td>\n",
       "      <td>0.156911</td>\n",
       "      <td>0.965600</td>\n",
       "      <td>0.632561</td>\n",
       "      <td>0.802266</td>\n",
       "      <td>0.614624</td>\n",
       "      <td>0.643769</td>\n",
       "      <td>0.621736</td>\n",
       "      <td>0.807258</td>\n",
       "      <td>0.965600</td>\n",
       "      <td>0.965315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.153100</td>\n",
       "      <td>0.158839</td>\n",
       "      <td>0.964708</td>\n",
       "      <td>0.626260</td>\n",
       "      <td>0.801380</td>\n",
       "      <td>0.607766</td>\n",
       "      <td>0.631758</td>\n",
       "      <td>0.620856</td>\n",
       "      <td>0.803870</td>\n",
       "      <td>0.964708</td>\n",
       "      <td>0.964562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.150100</td>\n",
       "      <td>0.155909</td>\n",
       "      <td>0.965380</td>\n",
       "      <td>0.634461</td>\n",
       "      <td>0.806480</td>\n",
       "      <td>0.616302</td>\n",
       "      <td>0.638112</td>\n",
       "      <td>0.630851</td>\n",
       "      <td>0.808145</td>\n",
       "      <td>0.965380</td>\n",
       "      <td>0.965286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.146300</td>\n",
       "      <td>0.158110</td>\n",
       "      <td>0.964040</td>\n",
       "      <td>0.628596</td>\n",
       "      <td>0.809626</td>\n",
       "      <td>0.609795</td>\n",
       "      <td>0.618568</td>\n",
       "      <td>0.638955</td>\n",
       "      <td>0.804851</td>\n",
       "      <td>0.964040</td>\n",
       "      <td>0.964317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.141200</td>\n",
       "      <td>0.159029</td>\n",
       "      <td>0.965392</td>\n",
       "      <td>0.631753</td>\n",
       "      <td>0.802912</td>\n",
       "      <td>0.613661</td>\n",
       "      <td>0.640411</td>\n",
       "      <td>0.623326</td>\n",
       "      <td>0.806798</td>\n",
       "      <td>0.965392</td>\n",
       "      <td>0.965169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.133800</td>\n",
       "      <td>0.168542</td>\n",
       "      <td>0.963083</td>\n",
       "      <td>0.615314</td>\n",
       "      <td>0.800088</td>\n",
       "      <td>0.595945</td>\n",
       "      <td>0.610763</td>\n",
       "      <td>0.619933</td>\n",
       "      <td>0.797963</td>\n",
       "      <td>0.963083</td>\n",
       "      <td>0.963214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.124600</td>\n",
       "      <td>0.179550</td>\n",
       "      <td>0.960934</td>\n",
       "      <td>0.602998</td>\n",
       "      <td>0.800390</td>\n",
       "      <td>0.582807</td>\n",
       "      <td>0.584289</td>\n",
       "      <td>0.622945</td>\n",
       "      <td>0.791227</td>\n",
       "      <td>0.960934</td>\n",
       "      <td>0.961527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>0.199146</td>\n",
       "      <td>0.958501</td>\n",
       "      <td>0.582009</td>\n",
       "      <td>0.791369</td>\n",
       "      <td>0.560726</td>\n",
       "      <td>0.559300</td>\n",
       "      <td>0.606641</td>\n",
       "      <td>0.780088</td>\n",
       "      <td>0.958501</td>\n",
       "      <td>0.959299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='560' max='560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [560/560 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_6128\\2406559715.py:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21492' max='21492' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21492/21492 35:43, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.190800</td>\n",
       "      <td>0.167786</td>\n",
       "      <td>0.962601</td>\n",
       "      <td>0.602007</td>\n",
       "      <td>0.788971</td>\n",
       "      <td>0.582408</td>\n",
       "      <td>0.606920</td>\n",
       "      <td>0.597172</td>\n",
       "      <td>0.791193</td>\n",
       "      <td>0.962601</td>\n",
       "      <td>0.962457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.168800</td>\n",
       "      <td>0.167244</td>\n",
       "      <td>0.962789</td>\n",
       "      <td>0.608253</td>\n",
       "      <td>0.795122</td>\n",
       "      <td>0.588722</td>\n",
       "      <td>0.606604</td>\n",
       "      <td>0.609911</td>\n",
       "      <td>0.794360</td>\n",
       "      <td>0.962789</td>\n",
       "      <td>0.962837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.162100</td>\n",
       "      <td>0.159797</td>\n",
       "      <td>0.960192</td>\n",
       "      <td>0.612492</td>\n",
       "      <td>0.819563</td>\n",
       "      <td>0.593647</td>\n",
       "      <td>0.568240</td>\n",
       "      <td>0.664218</td>\n",
       "      <td>0.795755</td>\n",
       "      <td>0.960192</td>\n",
       "      <td>0.961658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.158300</td>\n",
       "      <td>0.156836</td>\n",
       "      <td>0.964552</td>\n",
       "      <td>0.627645</td>\n",
       "      <td>0.805958</td>\n",
       "      <td>0.609044</td>\n",
       "      <td>0.624552</td>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.804518</td>\n",
       "      <td>0.964552</td>\n",
       "      <td>0.964635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>0.156299</td>\n",
       "      <td>0.967341</td>\n",
       "      <td>0.634565</td>\n",
       "      <td>0.792170</td>\n",
       "      <td>0.618755</td>\n",
       "      <td>0.675041</td>\n",
       "      <td>0.598668</td>\n",
       "      <td>0.808736</td>\n",
       "      <td>0.967341</td>\n",
       "      <td>0.966408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.153600</td>\n",
       "      <td>0.154818</td>\n",
       "      <td>0.966010</td>\n",
       "      <td>0.635188</td>\n",
       "      <td>0.803860</td>\n",
       "      <td>0.617462</td>\n",
       "      <td>0.645988</td>\n",
       "      <td>0.624742</td>\n",
       "      <td>0.808681</td>\n",
       "      <td>0.966010</td>\n",
       "      <td>0.965740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.150700</td>\n",
       "      <td>0.155290</td>\n",
       "      <td>0.962479</td>\n",
       "      <td>0.624992</td>\n",
       "      <td>0.818817</td>\n",
       "      <td>0.606254</td>\n",
       "      <td>0.593413</td>\n",
       "      <td>0.660122</td>\n",
       "      <td>0.802622</td>\n",
       "      <td>0.962479</td>\n",
       "      <td>0.963425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>0.154822</td>\n",
       "      <td>0.966305</td>\n",
       "      <td>0.636713</td>\n",
       "      <td>0.803385</td>\n",
       "      <td>0.619209</td>\n",
       "      <td>0.650589</td>\n",
       "      <td>0.623416</td>\n",
       "      <td>0.809523</td>\n",
       "      <td>0.966305</td>\n",
       "      <td>0.965963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.142200</td>\n",
       "      <td>0.158154</td>\n",
       "      <td>0.963647</td>\n",
       "      <td>0.627302</td>\n",
       "      <td>0.812677</td>\n",
       "      <td>0.608490</td>\n",
       "      <td>0.609737</td>\n",
       "      <td>0.645909</td>\n",
       "      <td>0.804097</td>\n",
       "      <td>0.963647</td>\n",
       "      <td>0.964144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.135100</td>\n",
       "      <td>0.166574</td>\n",
       "      <td>0.965500</td>\n",
       "      <td>0.626621</td>\n",
       "      <td>0.797165</td>\n",
       "      <td>0.608760</td>\n",
       "      <td>0.642824</td>\n",
       "      <td>0.611215</td>\n",
       "      <td>0.804268</td>\n",
       "      <td>0.965500</td>\n",
       "      <td>0.965086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>0.179304</td>\n",
       "      <td>0.961518</td>\n",
       "      <td>0.606319</td>\n",
       "      <td>0.801933</td>\n",
       "      <td>0.586419</td>\n",
       "      <td>0.588148</td>\n",
       "      <td>0.625648</td>\n",
       "      <td>0.793045</td>\n",
       "      <td>0.961518</td>\n",
       "      <td>0.962082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.116200</td>\n",
       "      <td>0.195314</td>\n",
       "      <td>0.960159</td>\n",
       "      <td>0.593066</td>\n",
       "      <td>0.795188</td>\n",
       "      <td>0.572476</td>\n",
       "      <td>0.574428</td>\n",
       "      <td>0.612954</td>\n",
       "      <td>0.786060</td>\n",
       "      <td>0.960159</td>\n",
       "      <td>0.960772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='560' max='560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [560/560 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_6128\\2406559715.py:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21492' max='21492' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21492/21492 35:36, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.180620</td>\n",
       "      <td>0.965177</td>\n",
       "      <td>0.583302</td>\n",
       "      <td>0.752562</td>\n",
       "      <td>0.570410</td>\n",
       "      <td>0.667686</td>\n",
       "      <td>0.517854</td>\n",
       "      <td>0.782566</td>\n",
       "      <td>0.965177</td>\n",
       "      <td>0.963072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.167423</td>\n",
       "      <td>0.962174</td>\n",
       "      <td>0.599356</td>\n",
       "      <td>0.790573</td>\n",
       "      <td>0.579509</td>\n",
       "      <td>0.597582</td>\n",
       "      <td>0.601140</td>\n",
       "      <td>0.789753</td>\n",
       "      <td>0.962174</td>\n",
       "      <td>0.962228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.161100</td>\n",
       "      <td>0.160506</td>\n",
       "      <td>0.965712</td>\n",
       "      <td>0.622977</td>\n",
       "      <td>0.792782</td>\n",
       "      <td>0.605447</td>\n",
       "      <td>0.645604</td>\n",
       "      <td>0.601882</td>\n",
       "      <td>0.802508</td>\n",
       "      <td>0.965712</td>\n",
       "      <td>0.965140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.158400</td>\n",
       "      <td>0.158303</td>\n",
       "      <td>0.963094</td>\n",
       "      <td>0.619545</td>\n",
       "      <td>0.808789</td>\n",
       "      <td>0.600460</td>\n",
       "      <td>0.601727</td>\n",
       "      <td>0.638450</td>\n",
       "      <td>0.800076</td>\n",
       "      <td>0.963094</td>\n",
       "      <td>0.963613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>0.155493</td>\n",
       "      <td>0.967228</td>\n",
       "      <td>0.634515</td>\n",
       "      <td>0.794783</td>\n",
       "      <td>0.618245</td>\n",
       "      <td>0.667767</td>\n",
       "      <td>0.604418</td>\n",
       "      <td>0.808680</td>\n",
       "      <td>0.967228</td>\n",
       "      <td>0.966451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.153300</td>\n",
       "      <td>0.154667</td>\n",
       "      <td>0.966078</td>\n",
       "      <td>0.633882</td>\n",
       "      <td>0.803454</td>\n",
       "      <td>0.616188</td>\n",
       "      <td>0.644154</td>\n",
       "      <td>0.623932</td>\n",
       "      <td>0.808048</td>\n",
       "      <td>0.966078</td>\n",
       "      <td>0.965820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.150600</td>\n",
       "      <td>0.155610</td>\n",
       "      <td>0.963496</td>\n",
       "      <td>0.626490</td>\n",
       "      <td>0.814709</td>\n",
       "      <td>0.607784</td>\n",
       "      <td>0.604223</td>\n",
       "      <td>0.650461</td>\n",
       "      <td>0.803650</td>\n",
       "      <td>0.963496</td>\n",
       "      <td>0.964134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.147000</td>\n",
       "      <td>0.155861</td>\n",
       "      <td>0.965104</td>\n",
       "      <td>0.630639</td>\n",
       "      <td>0.807232</td>\n",
       "      <td>0.612330</td>\n",
       "      <td>0.628339</td>\n",
       "      <td>0.632956</td>\n",
       "      <td>0.806163</td>\n",
       "      <td>0.965104</td>\n",
       "      <td>0.965164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.142000</td>\n",
       "      <td>0.158487</td>\n",
       "      <td>0.964531</td>\n",
       "      <td>0.627348</td>\n",
       "      <td>0.807587</td>\n",
       "      <td>0.608771</td>\n",
       "      <td>0.620514</td>\n",
       "      <td>0.634335</td>\n",
       "      <td>0.804364</td>\n",
       "      <td>0.964531</td>\n",
       "      <td>0.964717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.134900</td>\n",
       "      <td>0.166151</td>\n",
       "      <td>0.960186</td>\n",
       "      <td>0.606374</td>\n",
       "      <td>0.813494</td>\n",
       "      <td>0.587081</td>\n",
       "      <td>0.567051</td>\n",
       "      <td>0.651559</td>\n",
       "      <td>0.792704</td>\n",
       "      <td>0.960186</td>\n",
       "      <td>0.961493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.125500</td>\n",
       "      <td>0.180295</td>\n",
       "      <td>0.962714</td>\n",
       "      <td>0.604570</td>\n",
       "      <td>0.792974</td>\n",
       "      <td>0.585006</td>\n",
       "      <td>0.603547</td>\n",
       "      <td>0.605596</td>\n",
       "      <td>0.792502</td>\n",
       "      <td>0.962714</td>\n",
       "      <td>0.962744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.115800</td>\n",
       "      <td>0.196068</td>\n",
       "      <td>0.958366</td>\n",
       "      <td>0.580435</td>\n",
       "      <td>0.793681</td>\n",
       "      <td>0.559411</td>\n",
       "      <td>0.552061</td>\n",
       "      <td>0.611884</td>\n",
       "      <td>0.779266</td>\n",
       "      <td>0.958366</td>\n",
       "      <td>0.959380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='560' max='560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [560/560 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test metrics per fold ===\n",
      "      eval_loss  eval_accuracy   eval_f1  eval_roc_auc  eval_matthews  \\\n",
      "fold                                                                    \n",
      "1      0.157914       0.967087  0.638562      0.797023       0.622167   \n",
      "2      0.155208       0.966142  0.637336      0.803256       0.619763   \n",
      "3      0.155311       0.965642  0.636919      0.806819       0.618916   \n",
      "4      0.156389       0.966195  0.637802      0.803420       0.620263   \n",
      "5      0.155974       0.967216  0.638301      0.795630       0.622157   \n",
      "\n",
      "      eval_precision  eval_recall  eval_f1_macro  eval_f1_micro  \\\n",
      "fold                                                              \n",
      "1           0.671130     0.609008       0.810660       0.967087   \n",
      "2           0.652153     0.623177       0.809789       0.966142   \n",
      "3           0.642711     0.631229       0.809443       0.965642   \n",
      "4           0.652816     0.623464       0.810036       0.966195   \n",
      "5           0.674324     0.605932       0.810566       0.967216   \n",
      "\n",
      "      eval_f1_weighted  eval_runtime  eval_samples_per_second  \\\n",
      "fold                                                            \n",
      "1             0.966326       86.2991                  415.068   \n",
      "2             0.965776       87.8558                  407.713   \n",
      "3             0.965495       88.8115                  403.326   \n",
      "4             0.965825       85.3088                  419.886   \n",
      "5             0.966382       85.2587                  420.133   \n",
      "\n",
      "      eval_steps_per_second  epoch  \n",
      "fold                                \n",
      "1                     6.489   12.0  \n",
      "2                     6.374   12.0  \n",
      "3                     6.305   12.0  \n",
      "4                     6.564   12.0  \n",
      "5                     6.568   12.0  \n",
      "\n",
      "=== Mean test metrics across folds ===\n",
      "eval_loss                    0.156159\n",
      "eval_accuracy                0.966456\n",
      "eval_f1                      0.637784\n",
      "eval_roc_auc                 0.801230\n",
      "eval_matthews                0.620653\n",
      "eval_precision               0.658627\n",
      "eval_recall                  0.618562\n",
      "eval_f1_macro                0.810099\n",
      "eval_f1_micro                0.966456\n",
      "eval_f1_weighted             0.965961\n",
      "eval_runtime                86.706780\n",
      "eval_samples_per_second    413.225200\n",
      "eval_steps_per_second        6.460000\n",
      "epoch                       12.000000\n",
      "dtype: float64\n",
      "\n",
      "=== Mean test metrics across folds (rounded) ===\n",
      "eval_loss                    0.1562\n",
      "eval_accuracy                0.9665\n",
      "eval_f1                      0.6378\n",
      "eval_roc_auc                 0.8012\n",
      "eval_matthews                0.6207\n",
      "eval_precision               0.6586\n",
      "eval_recall                  0.6186\n",
      "eval_f1_macro                0.8101\n",
      "eval_f1_micro                0.9665\n",
      "eval_f1_weighted             0.9660\n",
      "eval_runtime                86.7068\n",
      "eval_samples_per_second    413.2252\n",
      "eval_steps_per_second        6.4600\n",
      "epoch                       12.0000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "all_intervals = ints_in + ints_out\n",
    "strata        = [f\"{lbl}_{chrom}\" for chrom,_,_,lbl in all_intervals]\n",
    "train_val, test_intervals = train_test_split(\n",
    "    all_intervals,\n",
    "    test_size=0.2,\n",
    "    stratify=strata,\n",
    "    random_state=42,\n",
    ")\n",
    "test_ds = DNATokenClassificationDataset(\n",
    "    chroms, DNA, ZDNA,\n",
    "    intervals=[(c,s,e) for c,s,e,_ in test_intervals],\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=width,\n",
    ")\n",
    "\n",
    "X   = np.arange(len(train_val))\n",
    "y   = [lbl for _,_,_,lbl in train_val]\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "device     = torch.device(\"cuda\" if cuda.is_available() else \"cpu\")\n",
    "base_model = copy.deepcopy(token_model).to(device)\n",
    "\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf.split(X, y), start=1):\n",
    "    print(f\"\\n=== Fold {fold} ===\")\n",
    "\n",
    "    train_intervals = [train_val[i] for i in tr_idx]\n",
    "    val_intervals   = [train_val[i] for i in val_idx]\n",
    "\n",
    "    train_ds = DNATokenClassificationDataset(\n",
    "        chroms, DNA, ZDNA,\n",
    "        intervals=[(c,s,e) for c,s,e,_ in train_intervals],\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=width,\n",
    "    )\n",
    "    val_ds = DNATokenClassificationDataset(\n",
    "        chroms, DNA, ZDNA,\n",
    "        intervals=[(c,s,e) for c,s,e,_ in val_intervals],\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=width,\n",
    "    )\n",
    "\n",
    "    model = copy.deepcopy(base_model).to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/fold{fold}\",\n",
    "        num_train_epochs=12,\n",
    "        learning_rate=5e-4,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=64,\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_ratio=0.10,\n",
    "        fp16=True,\n",
    "\n",
    "        logging_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        save_safetensors=False\n",
    "\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics2,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    save_dir = f\"./models/fold{fold}\"\n",
    "    trainer.save_model(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "    metrics = trainer.evaluate(eval_dataset=test_ds)\n",
    "    fold_metrics.append({\"fold\": fold, **metrics})\n",
    "\n",
    "df = pd.DataFrame(fold_metrics).set_index(\"fold\")\n",
    "print(\"\\n=== Test metrics per fold ===\")\n",
    "print(df)\n",
    "print(\"\\n=== Mean test metrics across folds ===\")\n",
    "print(df.mean())\n",
    "print(\"\\n=== Mean test metrics across folds (rounded) ===\")\n",
    "print(df.mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2133176e",
   "metadata": {},
   "source": [
    "## Experiment: Sequence Length 1024, Batch Size 64, 12 Epochs, 10% Warmup  \n",
    "Used sequence length = 1024 and batch size = 64 for 12 epochs with 10% warmup.  \n",
    "No cross-validation; single run on one train/validation split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c661ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_19876\\447254621.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_1024len = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8731' max='11628' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8731/11628 2:12:55 < 44:07, 1.09 it/s, Epoch 9.01/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.066300</td>\n",
       "      <td>0.047774</td>\n",
       "      <td>0.992669</td>\n",
       "      <td>0.482754</td>\n",
       "      <td>0.710210</td>\n",
       "      <td>0.484039</td>\n",
       "      <td>0.561976</td>\n",
       "      <td>0.423109</td>\n",
       "      <td>0.739531</td>\n",
       "      <td>0.992669</td>\n",
       "      <td>0.992156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.046810</td>\n",
       "      <td>0.992722</td>\n",
       "      <td>0.496083</td>\n",
       "      <td>0.720147</td>\n",
       "      <td>0.496082</td>\n",
       "      <td>0.563469</td>\n",
       "      <td>0.443093</td>\n",
       "      <td>0.746209</td>\n",
       "      <td>0.992722</td>\n",
       "      <td>0.992290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.046130</td>\n",
       "      <td>0.991442</td>\n",
       "      <td>0.506377</td>\n",
       "      <td>0.768994</td>\n",
       "      <td>0.503240</td>\n",
       "      <td>0.474466</td>\n",
       "      <td>0.542889</td>\n",
       "      <td>0.751030</td>\n",
       "      <td>0.991442</td>\n",
       "      <td>0.991728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.045198</td>\n",
       "      <td>0.992372</td>\n",
       "      <td>0.516856</td>\n",
       "      <td>0.750514</td>\n",
       "      <td>0.513165</td>\n",
       "      <td>0.529634</td>\n",
       "      <td>0.504680</td>\n",
       "      <td>0.756506</td>\n",
       "      <td>0.992372</td>\n",
       "      <td>0.992280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.044100</td>\n",
       "      <td>0.044378</td>\n",
       "      <td>0.992625</td>\n",
       "      <td>0.528175</td>\n",
       "      <td>0.753545</td>\n",
       "      <td>0.524781</td>\n",
       "      <td>0.547077</td>\n",
       "      <td>0.510535</td>\n",
       "      <td>0.762229</td>\n",
       "      <td>0.992625</td>\n",
       "      <td>0.992499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.044633</td>\n",
       "      <td>0.992223</td>\n",
       "      <td>0.524333</td>\n",
       "      <td>0.763067</td>\n",
       "      <td>0.520445</td>\n",
       "      <td>0.518647</td>\n",
       "      <td>0.530145</td>\n",
       "      <td>0.760206</td>\n",
       "      <td>0.992223</td>\n",
       "      <td>0.992265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.044739</td>\n",
       "      <td>0.993231</td>\n",
       "      <td>0.522712</td>\n",
       "      <td>0.728014</td>\n",
       "      <td>0.524627</td>\n",
       "      <td>0.607950</td>\n",
       "      <td>0.458437</td>\n",
       "      <td>0.759652</td>\n",
       "      <td>0.993231</td>\n",
       "      <td>0.992760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.041400</td>\n",
       "      <td>0.045112</td>\n",
       "      <td>0.992972</td>\n",
       "      <td>0.525574</td>\n",
       "      <td>0.739298</td>\n",
       "      <td>0.524303</td>\n",
       "      <td>0.578596</td>\n",
       "      <td>0.481454</td>\n",
       "      <td>0.761017</td>\n",
       "      <td>0.992972</td>\n",
       "      <td>0.992653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.039700</td>\n",
       "      <td>0.046331</td>\n",
       "      <td>0.993155</td>\n",
       "      <td>0.523074</td>\n",
       "      <td>0.730873</td>\n",
       "      <td>0.523961</td>\n",
       "      <td>0.598918</td>\n",
       "      <td>0.464280</td>\n",
       "      <td>0.759813</td>\n",
       "      <td>0.993155</td>\n",
       "      <td>0.992725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19876\\447254621.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[0mtrainer_1024len\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcan_return_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m \u001b[0mtrainer_1024len\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2240\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2241\u001b[1;33m             return inner_training_loop(\n\u001b[0m\u001b[0;32m   2242\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2243\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2546\u001b[0m                     \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2547\u001b[0m                     \u001b[1;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2548\u001b[1;33m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2550\u001b[0m                     if (\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3738\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"scale_wrt_gas\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\accelerate\\accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2324\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2325\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2326\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2327\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m     _engine_run_backward(\n\u001b[0m\u001b[0;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m         \u001b[1;33m)\u001b[0m  \u001b[1;31m# Calls into the C++ engine to run the backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/hyenadna32_finetune_lenrandom_batch64_onlinedatagenwithaug\",            \n",
    "    label_names=[\"labels\"],\n",
    "    num_train_epochs=12,                                 \n",
    "    learning_rate=5e-4,                                  \n",
    "    per_device_train_batch_size=64,                      \n",
    "    per_device_eval_batch_size=64,                      \n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,                                  \n",
    "    lr_scheduler_type=\"linear\",                           \n",
    "    warmup_ratio=0.10,                           \n",
    "    fp16=True,                                 \n",
    "\n",
    "\n",
    "    logging_steps=100,                                   \n",
    "    eval_steps=200,                                  \n",
    "    save_total_limit=3,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "\n",
    "trainer_1024len = Trainer(\n",
    "    model = token_model, \n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,     \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "\n",
    "trainer_1024len.can_return_loss = True\n",
    "trainer_1024len.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = trainer_1024len.evaluate(eval_dataset=test_ds)\n",
    "print(\"=== Тестовые метрики ===\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f73002",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./models/hyenadna-small-32k_len1024\"\n",
    "trainer_1024len.save_model(save_dir)       \n",
    "tokenizer.save_pretrained(save_dir)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323c4c3e",
   "metadata": {},
   "source": [
    "## Experiment: Augmented Data, Sequence Length ~100±20, Batch Size 64, 10% Warmup, No Freezing  \n",
    "Used online data augmentation with variable sequence lengths (80–120) and overlapping.  \n",
    "Trained for 12 epochs, batch size = 64, learning rate = 5e-4, no layer freezing.  \n",
    "Warmup ratio set to 10%; evaluated with full classification metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1936a446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_19876\\4001457223.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_aug_classic = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40272' max='40272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40272/40272 1:13:30, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.170500</td>\n",
       "      <td>0.168118</td>\n",
       "      <td>0.964879</td>\n",
       "      <td>0.608082</td>\n",
       "      <td>0.778251</td>\n",
       "      <td>0.591067</td>\n",
       "      <td>0.649060</td>\n",
       "      <td>0.571971</td>\n",
       "      <td>0.794849</td>\n",
       "      <td>0.964879</td>\n",
       "      <td>0.963822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>0.160852</td>\n",
       "      <td>0.965046</td>\n",
       "      <td>0.626230</td>\n",
       "      <td>0.798638</td>\n",
       "      <td>0.608019</td>\n",
       "      <td>0.638192</td>\n",
       "      <td>0.614707</td>\n",
       "      <td>0.803948</td>\n",
       "      <td>0.965046</td>\n",
       "      <td>0.964734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.160600</td>\n",
       "      <td>0.160793</td>\n",
       "      <td>0.966693</td>\n",
       "      <td>0.632074</td>\n",
       "      <td>0.792801</td>\n",
       "      <td>0.615609</td>\n",
       "      <td>0.667032</td>\n",
       "      <td>0.600598</td>\n",
       "      <td>0.807316</td>\n",
       "      <td>0.966693</td>\n",
       "      <td>0.965861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.157300</td>\n",
       "      <td>0.157704</td>\n",
       "      <td>0.967992</td>\n",
       "      <td>0.638011</td>\n",
       "      <td>0.789469</td>\n",
       "      <td>0.623420</td>\n",
       "      <td>0.691575</td>\n",
       "      <td>0.592147</td>\n",
       "      <td>0.810633</td>\n",
       "      <td>0.967992</td>\n",
       "      <td>0.966810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.154700</td>\n",
       "      <td>0.154918</td>\n",
       "      <td>0.965181</td>\n",
       "      <td>0.635372</td>\n",
       "      <td>0.809224</td>\n",
       "      <td>0.617091</td>\n",
       "      <td>0.633906</td>\n",
       "      <td>0.636844</td>\n",
       "      <td>0.808545</td>\n",
       "      <td>0.965181</td>\n",
       "      <td>0.965219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.152200</td>\n",
       "      <td>0.156297</td>\n",
       "      <td>0.965428</td>\n",
       "      <td>0.634528</td>\n",
       "      <td>0.806117</td>\n",
       "      <td>0.616402</td>\n",
       "      <td>0.639091</td>\n",
       "      <td>0.630030</td>\n",
       "      <td>0.808192</td>\n",
       "      <td>0.965428</td>\n",
       "      <td>0.965311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.149400</td>\n",
       "      <td>0.155987</td>\n",
       "      <td>0.961900</td>\n",
       "      <td>0.626698</td>\n",
       "      <td>0.823900</td>\n",
       "      <td>0.608213</td>\n",
       "      <td>0.587602</td>\n",
       "      <td>0.671368</td>\n",
       "      <td>0.803312</td>\n",
       "      <td>0.961900</td>\n",
       "      <td>0.963099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.145400</td>\n",
       "      <td>0.157199</td>\n",
       "      <td>0.967816</td>\n",
       "      <td>0.642603</td>\n",
       "      <td>0.796620</td>\n",
       "      <td>0.626967</td>\n",
       "      <td>0.682143</td>\n",
       "      <td>0.607396</td>\n",
       "      <td>0.812876</td>\n",
       "      <td>0.967816</td>\n",
       "      <td>0.966927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.140200</td>\n",
       "      <td>0.161880</td>\n",
       "      <td>0.962217</td>\n",
       "      <td>0.624138</td>\n",
       "      <td>0.817980</td>\n",
       "      <td>0.605221</td>\n",
       "      <td>0.593140</td>\n",
       "      <td>0.658555</td>\n",
       "      <td>0.802123</td>\n",
       "      <td>0.962217</td>\n",
       "      <td>0.963152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>0.175291</td>\n",
       "      <td>0.963476</td>\n",
       "      <td>0.619908</td>\n",
       "      <td>0.802821</td>\n",
       "      <td>0.600750</td>\n",
       "      <td>0.614659</td>\n",
       "      <td>0.625248</td>\n",
       "      <td>0.800362</td>\n",
       "      <td>0.963476</td>\n",
       "      <td>0.963624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>0.189460</td>\n",
       "      <td>0.962688</td>\n",
       "      <td>0.609973</td>\n",
       "      <td>0.796353</td>\n",
       "      <td>0.590385</td>\n",
       "      <td>0.607463</td>\n",
       "      <td>0.612503</td>\n",
       "      <td>0.795190</td>\n",
       "      <td>0.962688</td>\n",
       "      <td>0.962761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.115500</td>\n",
       "      <td>0.212175</td>\n",
       "      <td>0.960180</td>\n",
       "      <td>0.591980</td>\n",
       "      <td>0.792142</td>\n",
       "      <td>0.571240</td>\n",
       "      <td>0.578223</td>\n",
       "      <td>0.606408</td>\n",
       "      <td>0.785525</td>\n",
       "      <td>0.960180</td>\n",
       "      <td>0.960629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40272, training_loss=0.14751272841867683, metrics={'train_runtime': 4410.688, 'train_samples_per_second': 584.246, 'train_steps_per_second': 9.131, 'total_flos': 5112713550938112.0, 'train_loss': 0.14751272841867683, 'epoch': 12.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/hyenadna32_finetune_lenrandom_batch64_onlinedatagenwithaug\",            \n",
    "    label_names=[\"labels\"],\n",
    "    num_train_epochs=12,                                 \n",
    "    learning_rate=5e-4,                                  \n",
    "    per_device_train_batch_size=64,                    \n",
    "    per_device_eval_batch_size=64,                      \n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,                                   \n",
    "    lr_scheduler_type=\"linear\",                          \n",
    "    warmup_ratio=0.10,                                    \n",
    "    fp16=True,                                           \n",
    "\n",
    "    logging_steps=100,                                  \n",
    "    eval_steps=200,                                     \n",
    "    save_total_limit=3,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "\n",
    "trainer_aug_classic = Trainer(\n",
    "    model = token_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,    \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "\n",
    "trainer_aug_classic.can_return_loss = True\n",
    "trainer_aug_classic.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b94523d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Тестовые метрики ===\n",
      "eval_loss: 0.1555\n",
      "eval_accuracy: 0.9682\n",
      "eval_f1: 0.6437\n",
      "eval_roc_auc: 0.7973\n",
      "eval_matthews: 0.6282\n",
      "eval_precision: 0.6829\n",
      "eval_recall: 0.6086\n",
      "eval_f1_macro: 0.8135\n",
      "eval_f1_micro: 0.9682\n",
      "eval_f1_weighted: 0.9673\n",
      "eval_runtime: 162.3159\n",
      "eval_samples_per_second: 441.0040\n",
      "eval_steps_per_second: 6.8940\n",
      "epoch: 12.0000\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer_aug_classic.evaluate(eval_dataset=test_ds)\n",
    "print(\"=== Тестовые метрики ===\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b4937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/hyenadna-small-32k_lenrandom_data_augementation\\\\tokenizer_config.json',\n",
       " './models/hyenadna-small-32k_lenrandom_data_augementation\\\\special_tokens_map.json',\n",
       " './models/hyenadna-small-32k_lenrandom_data_augementation\\\\added_tokens.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = \"./models/hyenadna-small-32k_lenrandom_data_augementation\"\n",
    "trainer_aug_classic.save_model(save_dir)      \n",
    "tokenizer.save_pretrained(save_dir)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde838b",
   "metadata": {},
   "source": [
    "## Experiment: CE weights 0.7 and 2, Overlapping Chunks [0,50], [25,75], Length ~100±20, 2-Stage Training (Freeze + Unfreeze)  \n",
    "Applied 50% overlapping sliding windows with random sequence length (80–120).  \n",
    "Cross-entropy loss weighted as 0.7 and 2; training performed in two stages:  \n",
    "Stage 1: Freeze backbone, train classifier head (3 epochs, LR=1e-3).  \n",
    "Stage 2: Unfreeze backbone, train full model (6 epochs, backbone LR=1e-5, head LR=5e-4).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273fd15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_19876\\1723855482.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_stage1_augdata = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: romanbokhyan (romanbokhyan-hse-university). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\1\\vscode-projects\\coursework\\HyenaDNA config\\wandb\\run-20250503_160218-wjzkdw4g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/wjzkdw4g' target=\"_blank\">./stage1</a></strong> to <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface' target=\"_blank\">https://wandb.ai/romanbokhyan-hse-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/wjzkdw4g' target=\"_blank\">https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/wjzkdw4g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10068' max='10068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10068/10068 17:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.233400</td>\n",
       "      <td>0.225265</td>\n",
       "      <td>0.947842</td>\n",
       "      <td>0.420770</td>\n",
       "      <td>0.686530</td>\n",
       "      <td>0.394285</td>\n",
       "      <td>0.446680</td>\n",
       "      <td>0.397701</td>\n",
       "      <td>0.696731</td>\n",
       "      <td>0.947842</td>\n",
       "      <td>0.946400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.228400</td>\n",
       "      <td>0.224603</td>\n",
       "      <td>0.943737</td>\n",
       "      <td>0.441970</td>\n",
       "      <td>0.717638</td>\n",
       "      <td>0.413134</td>\n",
       "      <td>0.418898</td>\n",
       "      <td>0.467730</td>\n",
       "      <td>0.706172</td>\n",
       "      <td>0.943737</td>\n",
       "      <td>0.945204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.229000</td>\n",
       "      <td>0.222447</td>\n",
       "      <td>0.947737</td>\n",
       "      <td>0.436568</td>\n",
       "      <td>0.699467</td>\n",
       "      <td>0.409352</td>\n",
       "      <td>0.448725</td>\n",
       "      <td>0.425053</td>\n",
       "      <td>0.704583</td>\n",
       "      <td>0.947737</td>\n",
       "      <td>0.947064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_19876\\1723855482.py:99: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_stage2_augdata = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20136' max='20136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20136/20136 47:19, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.165900</td>\n",
       "      <td>0.161199</td>\n",
       "      <td>0.966503</td>\n",
       "      <td>0.617779</td>\n",
       "      <td>0.777349</td>\n",
       "      <td>0.602907</td>\n",
       "      <td>0.676730</td>\n",
       "      <td>0.568275</td>\n",
       "      <td>0.800131</td>\n",
       "      <td>0.966503</td>\n",
       "      <td>0.965111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.159500</td>\n",
       "      <td>0.156132</td>\n",
       "      <td>0.965085</td>\n",
       "      <td>0.625538</td>\n",
       "      <td>0.797471</td>\n",
       "      <td>0.607395</td>\n",
       "      <td>0.639464</td>\n",
       "      <td>0.612206</td>\n",
       "      <td>0.803614</td>\n",
       "      <td>0.965085</td>\n",
       "      <td>0.964723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.157600</td>\n",
       "      <td>0.154608</td>\n",
       "      <td>0.965986</td>\n",
       "      <td>0.631613</td>\n",
       "      <td>0.797903</td>\n",
       "      <td>0.614144</td>\n",
       "      <td>0.652388</td>\n",
       "      <td>0.612120</td>\n",
       "      <td>0.806891</td>\n",
       "      <td>0.965986</td>\n",
       "      <td>0.965471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.153600</td>\n",
       "      <td>0.154774</td>\n",
       "      <td>0.965305</td>\n",
       "      <td>0.631061</td>\n",
       "      <td>0.802665</td>\n",
       "      <td>0.612919</td>\n",
       "      <td>0.639440</td>\n",
       "      <td>0.622898</td>\n",
       "      <td>0.806429</td>\n",
       "      <td>0.965305</td>\n",
       "      <td>0.965089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.149500</td>\n",
       "      <td>0.155015</td>\n",
       "      <td>0.965066</td>\n",
       "      <td>0.630430</td>\n",
       "      <td>0.803776</td>\n",
       "      <td>0.612119</td>\n",
       "      <td>0.635438</td>\n",
       "      <td>0.625501</td>\n",
       "      <td>0.806048</td>\n",
       "      <td>0.965066</td>\n",
       "      <td>0.964935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.149200</td>\n",
       "      <td>0.157110</td>\n",
       "      <td>0.964749</td>\n",
       "      <td>0.628787</td>\n",
       "      <td>0.804202</td>\n",
       "      <td>0.610286</td>\n",
       "      <td>0.630838</td>\n",
       "      <td>0.626748</td>\n",
       "      <td>0.805141</td>\n",
       "      <td>0.964749</td>\n",
       "      <td>0.964694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20136, training_loss=0.15770960990805735, metrics={'train_runtime': 2839.7961, 'train_samples_per_second': 453.717, 'train_steps_per_second': 7.091, 'total_flos': 2556356775469056.0, 'train_loss': 0.15770960990805735, 'epoch': 6.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- STEP 1: Freeze-backbone, train only the head ---\n",
    "# Freeze all backbone parameters\n",
    "for param in token_model.hyena.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Check that the classifier still requires_grad=True\n",
    "for name, param in token_model.named_parameters():\n",
    "    if \"classifier\" in name:\n",
    "        assert param.requires_grad\n",
    "\n",
    "# Re-create Trainer with a large LR for the head\n",
    "training_args_stage1_augdata = TrainingArguments(\n",
    "    output_dir=\"./stage1\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-3, \n",
    "    per_device_train_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "\n",
    "trainer_stage1_augdata = Trainer(\n",
    "    model=token_model,\n",
    "    args=training_args_stage1_augdata,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "trainer_stage1_augdata.train()\n",
    "\n",
    "# After that, trainer_stage1.model stores the weights of the backbone (frozen)\n",
    "# and trained head (F1 best).\n",
    "\n",
    "# --- STEP 2: unfreeze-backbone and retrain the whole model ---\n",
    "# Remove freeze\n",
    "for param in token_model.hyena.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(\n",
    "    [\n",
    "      { \"params\": token_model.hyena.parameters(), \"lr\": 1e-5},\n",
    "      { \"params\": token_model.classifier.parameters(), \"lr\": 5e-4},\n",
    "    ],\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# New TrainingArguments with a lower base LR\n",
    "training_args_stage2_augdata = TrainingArguments(\n",
    "    output_dir=\"./stage2\",\n",
    "    num_train_epochs=6,\n",
    "    learning_rate=5e-5, # since the backbone is now also learning\n",
    "    per_device_train_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer with custom optimiser\n",
    "trainer_stage2_augdata = Trainer(\n",
    "    model=token_model,\n",
    "    args=training_args_stage2_augdata,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None), \n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "\n",
    "trainer_stage2_augdata.can_return_loss = True\n",
    "trainer_stage2_augdata.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8edc17",
   "metadata": {},
   "source": [
    "## Experiment: Best Model from all: CE 07 and 2 Two-Stage Fine-Tuning (F1 = 0.64)  \n",
    "Loss weighting: CE = 0.7 and 2; model trained in two stages:\n",
    "\n",
    "- **Stage 1 (Frozen Backbone):**  \n",
    "  Trained only the classification head for 3 epochs using LR = 1e-3.  \n",
    "  All Hyena backbone layers frozen.\n",
    "\n",
    "- **Stage 2 (Full Fine-Tuning):**  \n",
    "  Unfroze the entire model and fine-tuned for 6 more epochs.  \n",
    "  Used discriminative learning rates: 1e-5 (backbone), 5e-4 (head).  \n",
    "  Achieved F1 score of **0.64** on the evaluation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e85bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_15016\\1303135873.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_stage1 = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5040' max='5040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5040/5040 09:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.151846</td>\n",
       "      <td>0.965746</td>\n",
       "      <td>0.638059</td>\n",
       "      <td>0.808259</td>\n",
       "      <td>0.620095</td>\n",
       "      <td>0.641965</td>\n",
       "      <td>0.634199</td>\n",
       "      <td>0.810041</td>\n",
       "      <td>0.965746</td>\n",
       "      <td>0.965647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.146700</td>\n",
       "      <td>0.152068</td>\n",
       "      <td>0.966010</td>\n",
       "      <td>0.638772</td>\n",
       "      <td>0.807002</td>\n",
       "      <td>0.620989</td>\n",
       "      <td>0.646463</td>\n",
       "      <td>0.631261</td>\n",
       "      <td>0.810469</td>\n",
       "      <td>0.966010</td>\n",
       "      <td>0.965818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.151845</td>\n",
       "      <td>0.965612</td>\n",
       "      <td>0.637697</td>\n",
       "      <td>0.808899</td>\n",
       "      <td>0.619650</td>\n",
       "      <td>0.639712</td>\n",
       "      <td>0.635695</td>\n",
       "      <td>0.809823</td>\n",
       "      <td>0.965612</td>\n",
       "      <td>0.965560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_15016\\1303135873.py:99: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_stage2 = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10080' max='10080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10080/10080 22:51, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.145300</td>\n",
       "      <td>0.150837</td>\n",
       "      <td>0.965975</td>\n",
       "      <td>0.640418</td>\n",
       "      <td>0.809446</td>\n",
       "      <td>0.622575</td>\n",
       "      <td>0.644442</td>\n",
       "      <td>0.636445</td>\n",
       "      <td>0.811281</td>\n",
       "      <td>0.965975</td>\n",
       "      <td>0.965874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.143500</td>\n",
       "      <td>0.151908</td>\n",
       "      <td>0.966681</td>\n",
       "      <td>0.641283</td>\n",
       "      <td>0.804653</td>\n",
       "      <td>0.624040</td>\n",
       "      <td>0.657802</td>\n",
       "      <td>0.625573</td>\n",
       "      <td>0.811906</td>\n",
       "      <td>0.966681</td>\n",
       "      <td>0.966283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.143200</td>\n",
       "      <td>0.152103</td>\n",
       "      <td>0.965162</td>\n",
       "      <td>0.637054</td>\n",
       "      <td>0.811758</td>\n",
       "      <td>0.618781</td>\n",
       "      <td>0.631981</td>\n",
       "      <td>0.642210</td>\n",
       "      <td>0.809379</td>\n",
       "      <td>0.965162</td>\n",
       "      <td>0.965295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.142600</td>\n",
       "      <td>0.152221</td>\n",
       "      <td>0.965520</td>\n",
       "      <td>0.637934</td>\n",
       "      <td>0.809965</td>\n",
       "      <td>0.619832</td>\n",
       "      <td>0.637827</td>\n",
       "      <td>0.638040</td>\n",
       "      <td>0.809916</td>\n",
       "      <td>0.965520</td>\n",
       "      <td>0.965523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.144900</td>\n",
       "      <td>0.152867</td>\n",
       "      <td>0.965963</td>\n",
       "      <td>0.638730</td>\n",
       "      <td>0.807340</td>\n",
       "      <td>0.620911</td>\n",
       "      <td>0.645580</td>\n",
       "      <td>0.632024</td>\n",
       "      <td>0.810435</td>\n",
       "      <td>0.965963</td>\n",
       "      <td>0.965791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.142300</td>\n",
       "      <td>0.152668</td>\n",
       "      <td>0.965797</td>\n",
       "      <td>0.638344</td>\n",
       "      <td>0.808211</td>\n",
       "      <td>0.620410</td>\n",
       "      <td>0.642706</td>\n",
       "      <td>0.634041</td>\n",
       "      <td>0.810197</td>\n",
       "      <td>0.965797</td>\n",
       "      <td>0.965687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10080, training_loss=0.14389189491196283, metrics={'train_runtime': 1371.3274, 'train_samples_per_second': 470.172, 'train_steps_per_second': 7.351, 'total_flos': 1279225957847040.0, 'train_loss': 0.14389189491196283, 'epoch': 6.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- STEP 1: Freeze-backbone, train only the head ---\n",
    "# Freeze all backbone parameters\n",
    "for param in token_model.hyena.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Check that the classifier still requires_grad=True\n",
    "for name, param in token_model.named_parameters():\n",
    "    if \"classifier\" in name:\n",
    "        assert param.requires_grad\n",
    "\n",
    "# Re-create Trainer with a large LR for the head\n",
    "training_args_stage1_augdata = TrainingArguments(\n",
    "    output_dir=\"./stage1\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-3, # train the head quickly\n",
    "    per_device_train_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "\n",
    "trainer_stage1_augdata = Trainer(\n",
    "    model=token_model,\n",
    "    args=training_args_stage1_augdata,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "trainer_stage1_augdata.train()\n",
    "\n",
    "# After that, trainer_stage1.model stores the weights of the backbone (frozen)\n",
    "# and trained head (F1 best).\n",
    "\n",
    "# --- STEP 2: unfreeze-backbone and retrain the whole model ---\n",
    "# Remove freeze\n",
    "for param in token_model.hyena.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(\n",
    "    [\n",
    "      { \"params\": token_model.hyena.parameters(), \"lr\": 1e-5},\n",
    "      { \"params\": token_model.classifier.parameters(), \"lr\": 5e-4},\n",
    "    ],\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# New TrainingArguments with a lower base LR\n",
    "training_args_stage2_augdata = TrainingArguments(\n",
    "    output_dir=\"./stage2\",\n",
    "    num_train_epochs=6,\n",
    "    learning_rate=5e-5, # since the backbone is now also learning\n",
    "    per_device_train_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer with custom optimiser\n",
    "trainer_stage2_augdata = Trainer(\n",
    "    model=token_model,\n",
    "    args=training_args_stage2_augdata,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None), \n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "\n",
    "trainer_stage2_augdata.can_return_loss = True\n",
    "trainer_stage2_augdata.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97047aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Тестовые метрики ===\n",
      "eval_loss: 0.1525\n",
      "eval_accuracy: 0.9664\n",
      "eval_f1: 0.6401\n",
      "eval_roc_auc: 0.8046\n",
      "eval_matthews: 0.6226\n",
      "eval_precision: 0.6550\n",
      "eval_recall: 0.6257\n",
      "eval_f1_macro: 0.8112\n",
      "eval_f1_micro: 0.9664\n",
      "eval_f1_weighted: 0.9660\n",
      "eval_runtime: 124.5654\n",
      "eval_samples_per_second: 287.5600\n",
      "eval_steps_per_second: 35.9490\n",
      "epoch: 6.0000\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer_stage2_augdata.evaluate(eval_dataset=test_ds)\n",
    "print(\"=== Тестовые метрики ===\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cb9ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/hyenadna-small-32k_len100_bs64_freeze_3_epochs_unfreeze_6_epochs_bestmodel\\\\tokenizer_config.json',\n",
       " './models/hyenadna-small-32k_len100_bs64_freeze_3_epochs_unfreeze_6_epochs_bestmodel\\\\special_tokens_map.json',\n",
       " './models/hyenadna-small-32k_len100_bs64_freeze_3_epochs_unfreeze_6_epochs_bestmodel\\\\added_tokens.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = \"./models/hyenadna-small-32k_len100_bs64_freeze_3_epochs_unfreeze_6_epochs_bestmodel\"\n",
    "trainer_stage2_augdata.save_model(save_dir)     \n",
    "tokenizer.save_pretrained(save_dir)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b4e84f",
   "metadata": {},
   "source": [
    "## Experiment: Seq Length 100, Batch 64, CE=0.7 and 2, F1 = 0.639  \n",
    "Performed standard training with batch size = 64, sequence length = 100, and 10% warmup.  \n",
    "Used class-weighted cross-entropy loss (CE=0.7 and 2) with two-class weights.  \n",
    "Trained for 12 epochs; achieved F1 score of **0.639**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfb6fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/hyenadna32_finetune_len256_batch32_onlinedatagen\",            \n",
    "    label_names=[\"labels\"],\n",
    "    num_train_epochs=100,                                 \n",
    "    learning_rate=5e-4,                                  \n",
    "    per_device_train_batch_size=64,                     \n",
    "    per_device_eval_batch_size=64,                     \n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,                                   \n",
    "    lr_scheduler_type=\"linear\",                          \n",
    "    warmup_ratio=0.10,                                    \n",
    "    fp16=True,                                           \n",
    "\n",
    "    save_total_limit=3,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = token_model, \n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,   \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "\n",
    "trainer.can_return_loss = True\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1f1450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Тестовые метрики ===\n",
      "eval_loss: 0.1564\n",
      "eval_accuracy: 0.9663\n",
      "eval_f1: 0.6374\n",
      "eval_roc_auc: 0.8021\n",
      "eval_matthews: 0.6200\n",
      "eval_precision: 0.6552\n",
      "eval_recall: 0.6207\n",
      "eval_f1_macro: 0.8099\n",
      "eval_f1_micro: 0.9663\n",
      "eval_f1_weighted: 0.9659\n",
      "eval_runtime: 86.1764\n",
      "eval_samples_per_second: 415.6590\n",
      "eval_steps_per_second: 6.4980\n",
      "epoch: 12.0000\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer.evaluate(eval_dataset=test_ds)\n",
    "print(\"=== Тестовые метрики ===\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9c7228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/hyenadna-small-32k_len100_bs64_lr5e-4_12epochs_linear_adamw_07_2weights_best2\\\\tokenizer_config.json',\n",
       " './models/hyenadna-small-32k_len100_bs64_lr5e-4_12epochs_linear_adamw_07_2weights_best2\\\\special_tokens_map.json',\n",
       " './models/hyenadna-small-32k_len100_bs64_lr5e-4_12epochs_linear_adamw_07_2weights_best2\\\\added_tokens.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = \"./models/hyenadna-small-32k_len100_bs64_lr5e-4_12epochs_linear_adamw_07_2weights_best2\"\n",
    "\n",
    "trainer.save_model(save_dir)      \n",
    "tokenizer.save_pretrained(save_dir)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db604659",
   "metadata": {},
   "source": [
    "## Experiment: Seq Length 100, Batch 64, CE=1.0 and 8.0 \n",
    "Trained with sequence length = 100, batch size = 64, and 10% warmup.  \n",
    "Ran for 12 epochs with standard linear scheduler and no layer freezing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5283ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_5952\\3354301602.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_ira = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: romanbokhyan (romanbokhyan-hse-university). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\1\\vscode-projects\\coursework\\HyenaDNA config\\wandb\\run-20250503_115030-bwac98cv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/bwac98cv' target=\"_blank\">./results/hyenadna32_finetune_len256_batch32_onlinedatagen</a></strong> to <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface' target=\"_blank\">https://wandb.ai/romanbokhyan-hse-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/bwac98cv' target=\"_blank\">https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/bwac98cv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20160' max='20160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20160/20160 37:32, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.274600</td>\n",
       "      <td>0.250347</td>\n",
       "      <td>0.941472</td>\n",
       "      <td>0.541474</td>\n",
       "      <td>0.839072</td>\n",
       "      <td>0.532154</td>\n",
       "      <td>0.431777</td>\n",
       "      <td>0.725896</td>\n",
       "      <td>0.755108</td>\n",
       "      <td>0.941472</td>\n",
       "      <td>0.948400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.252200</td>\n",
       "      <td>0.241920</td>\n",
       "      <td>0.940461</td>\n",
       "      <td>0.544157</td>\n",
       "      <td>0.848307</td>\n",
       "      <td>0.537653</td>\n",
       "      <td>0.428129</td>\n",
       "      <td>0.746455</td>\n",
       "      <td>0.756154</td>\n",
       "      <td>0.940461</td>\n",
       "      <td>0.947965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.239000</td>\n",
       "      <td>0.236699</td>\n",
       "      <td>0.933841</td>\n",
       "      <td>0.527059</td>\n",
       "      <td>0.858079</td>\n",
       "      <td>0.526895</td>\n",
       "      <td>0.399484</td>\n",
       "      <td>0.774345</td>\n",
       "      <td>0.745746</td>\n",
       "      <td>0.933841</td>\n",
       "      <td>0.943611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.234200</td>\n",
       "      <td>0.233389</td>\n",
       "      <td>0.938674</td>\n",
       "      <td>0.542698</td>\n",
       "      <td>0.855870</td>\n",
       "      <td>0.539155</td>\n",
       "      <td>0.420700</td>\n",
       "      <td>0.764352</td>\n",
       "      <td>0.754916</td>\n",
       "      <td>0.938674</td>\n",
       "      <td>0.946927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.233294</td>\n",
       "      <td>0.931491</td>\n",
       "      <td>0.522632</td>\n",
       "      <td>0.863213</td>\n",
       "      <td>0.525296</td>\n",
       "      <td>0.391030</td>\n",
       "      <td>0.787750</td>\n",
       "      <td>0.742865</td>\n",
       "      <td>0.931491</td>\n",
       "      <td>0.942128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.226500</td>\n",
       "      <td>0.231300</td>\n",
       "      <td>0.940791</td>\n",
       "      <td>0.551195</td>\n",
       "      <td>0.856680</td>\n",
       "      <td>0.546662</td>\n",
       "      <td>0.431203</td>\n",
       "      <td>0.763719</td>\n",
       "      <td>0.759750</td>\n",
       "      <td>0.940791</td>\n",
       "      <td>0.948447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.222400</td>\n",
       "      <td>0.231728</td>\n",
       "      <td>0.931587</td>\n",
       "      <td>0.523483</td>\n",
       "      <td>0.864013</td>\n",
       "      <td>0.526321</td>\n",
       "      <td>0.391595</td>\n",
       "      <td>0.789327</td>\n",
       "      <td>0.743316</td>\n",
       "      <td>0.931587</td>\n",
       "      <td>0.942217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.217300</td>\n",
       "      <td>0.231497</td>\n",
       "      <td>0.933972</td>\n",
       "      <td>0.530623</td>\n",
       "      <td>0.862708</td>\n",
       "      <td>0.531686</td>\n",
       "      <td>0.401034</td>\n",
       "      <td>0.783944</td>\n",
       "      <td>0.747556</td>\n",
       "      <td>0.933972</td>\n",
       "      <td>0.943833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>0.236653</td>\n",
       "      <td>0.938382</td>\n",
       "      <td>0.541576</td>\n",
       "      <td>0.855800</td>\n",
       "      <td>0.538181</td>\n",
       "      <td>0.419300</td>\n",
       "      <td>0.764528</td>\n",
       "      <td>0.754274</td>\n",
       "      <td>0.938382</td>\n",
       "      <td>0.946719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.199100</td>\n",
       "      <td>0.249864</td>\n",
       "      <td>0.929546</td>\n",
       "      <td>0.513043</td>\n",
       "      <td>0.858314</td>\n",
       "      <td>0.515419</td>\n",
       "      <td>0.382324</td>\n",
       "      <td>0.779587</td>\n",
       "      <td>0.737534</td>\n",
       "      <td>0.929546</td>\n",
       "      <td>0.940651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.185300</td>\n",
       "      <td>0.274304</td>\n",
       "      <td>0.934137</td>\n",
       "      <td>0.520861</td>\n",
       "      <td>0.847605</td>\n",
       "      <td>0.517650</td>\n",
       "      <td>0.398414</td>\n",
       "      <td>0.751967</td>\n",
       "      <td>0.742749</td>\n",
       "      <td>0.934137</td>\n",
       "      <td>0.943511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.170400</td>\n",
       "      <td>0.308840</td>\n",
       "      <td>0.932298</td>\n",
       "      <td>0.508554</td>\n",
       "      <td>0.838960</td>\n",
       "      <td>0.504020</td>\n",
       "      <td>0.388552</td>\n",
       "      <td>0.735800</td>\n",
       "      <td>0.736099</td>\n",
       "      <td>0.932298</td>\n",
       "      <td>0.941979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20160, training_loss=0.2217384898473346, metrics={'train_runtime': 2255.992, 'train_samples_per_second': 571.598, 'train_steps_per_second': 8.936, 'total_flos': 2558451915694080.0, 'train_loss': 0.2217384898473346, 'epoch': 12.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_args = TrainingArguments(\n",
    "    output_dir=\"./results/hyenadna32_finetune_len256_batch32_onlinedatagen\",          \n",
    "    label_names=[\"labels\"],\n",
    "    num_train_epochs=12,                               \n",
    "    learning_rate=5e-4,                             \n",
    "    per_device_train_batch_size=64,                      \n",
    "    per_device_eval_batch_size=64,                       \n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,                                   \n",
    "    lr_scheduler_type=\"linear\",                          \n",
    "    warmup_ratio=0.10,                                  \n",
    "    fp16=True,                                          \n",
    "\n",
    "    logging_steps=100,                                   \n",
    "    eval_steps=200,                                      \n",
    "    save_total_limit=3,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = token_model, \n",
    "    args=trainer_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,     \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "\n",
    "trainer.can_return_loss = True\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3e02ba",
   "metadata": {},
   "source": [
    "## Experiment: Seq Length 256, Batch 64\n",
    "Trained using sequence length = 256, batch size = 64, and 10% warmup.  \n",
    "Used default cross-entropy loss (equal weights).  \n",
    "No augmentation or freezing; trained for 12 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e5a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_24272\\3354301602.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_ira = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: romanbokhyan (romanbokhyan-hse-university). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\1\\vscode-projects\\coursework\\HyenaDNA config\\wandb\\run-20250503_102006-wacnnr83</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/wacnnr83' target=\"_blank\">./results/hyenadna32_finetune_len256_batch32_onlinedatagen</a></strong> to <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface' target=\"_blank\">https://wandb.ai/romanbokhyan-hse-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/wacnnr83' target=\"_blank\">https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/wacnnr83</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15936' max='15936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15936/15936 1:02:05, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.089800</td>\n",
       "      <td>0.055788</td>\n",
       "      <td>0.982397</td>\n",
       "      <td>0.469375</td>\n",
       "      <td>0.663766</td>\n",
       "      <td>0.512105</td>\n",
       "      <td>0.816698</td>\n",
       "      <td>0.329322</td>\n",
       "      <td>0.730213</td>\n",
       "      <td>0.982397</td>\n",
       "      <td>0.978717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>0.054411</td>\n",
       "      <td>0.982592</td>\n",
       "      <td>0.472899</td>\n",
       "      <td>0.664347</td>\n",
       "      <td>0.517936</td>\n",
       "      <td>0.832113</td>\n",
       "      <td>0.330309</td>\n",
       "      <td>0.732024</td>\n",
       "      <td>0.982592</td>\n",
       "      <td>0.978898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.983373</td>\n",
       "      <td>0.563980</td>\n",
       "      <td>0.725517</td>\n",
       "      <td>0.573371</td>\n",
       "      <td>0.741968</td>\n",
       "      <td>0.454864</td>\n",
       "      <td>0.777752</td>\n",
       "      <td>0.983373</td>\n",
       "      <td>0.981417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.051222</td>\n",
       "      <td>0.983936</td>\n",
       "      <td>0.578682</td>\n",
       "      <td>0.731551</td>\n",
       "      <td>0.588845</td>\n",
       "      <td>0.761526</td>\n",
       "      <td>0.466641</td>\n",
       "      <td>0.785247</td>\n",
       "      <td>0.983936</td>\n",
       "      <td>0.982045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.050200</td>\n",
       "      <td>0.050979</td>\n",
       "      <td>0.984051</td>\n",
       "      <td>0.564701</td>\n",
       "      <td>0.717429</td>\n",
       "      <td>0.583322</td>\n",
       "      <td>0.795947</td>\n",
       "      <td>0.437573</td>\n",
       "      <td>0.778289</td>\n",
       "      <td>0.984051</td>\n",
       "      <td>0.981778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.984069</td>\n",
       "      <td>0.584654</td>\n",
       "      <td>0.735341</td>\n",
       "      <td>0.593909</td>\n",
       "      <td>0.762013</td>\n",
       "      <td>0.474268</td>\n",
       "      <td>0.788266</td>\n",
       "      <td>0.984069</td>\n",
       "      <td>0.982252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.052011</td>\n",
       "      <td>0.983797</td>\n",
       "      <td>0.538297</td>\n",
       "      <td>0.698743</td>\n",
       "      <td>0.567539</td>\n",
       "      <td>0.824703</td>\n",
       "      <td>0.399542</td>\n",
       "      <td>0.765025</td>\n",
       "      <td>0.983797</td>\n",
       "      <td>0.981033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.046900</td>\n",
       "      <td>0.051103</td>\n",
       "      <td>0.984140</td>\n",
       "      <td>0.575793</td>\n",
       "      <td>0.726121</td>\n",
       "      <td>0.590095</td>\n",
       "      <td>0.783027</td>\n",
       "      <td>0.455296</td>\n",
       "      <td>0.783856</td>\n",
       "      <td>0.984140</td>\n",
       "      <td>0.982081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.053005</td>\n",
       "      <td>0.983815</td>\n",
       "      <td>0.573244</td>\n",
       "      <td>0.728155</td>\n",
       "      <td>0.584237</td>\n",
       "      <td>0.760984</td>\n",
       "      <td>0.459807</td>\n",
       "      <td>0.782498</td>\n",
       "      <td>0.983815</td>\n",
       "      <td>0.981857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>0.055426</td>\n",
       "      <td>0.983532</td>\n",
       "      <td>0.562200</td>\n",
       "      <td>0.721880</td>\n",
       "      <td>0.574380</td>\n",
       "      <td>0.756699</td>\n",
       "      <td>0.447243</td>\n",
       "      <td>0.776904</td>\n",
       "      <td>0.983532</td>\n",
       "      <td>0.981457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.062288</td>\n",
       "      <td>0.982617</td>\n",
       "      <td>0.544817</td>\n",
       "      <td>0.717898</td>\n",
       "      <td>0.552970</td>\n",
       "      <td>0.715081</td>\n",
       "      <td>0.440042</td>\n",
       "      <td>0.767978</td>\n",
       "      <td>0.982617</td>\n",
       "      <td>0.980588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.981223</td>\n",
       "      <td>0.519668</td>\n",
       "      <td>0.712116</td>\n",
       "      <td>0.522568</td>\n",
       "      <td>0.657393</td>\n",
       "      <td>0.429654</td>\n",
       "      <td>0.755046</td>\n",
       "      <td>0.981223</td>\n",
       "      <td>0.979295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15936, training_loss=0.04969053742397262, metrics={'train_runtime': 3730.2293, 'train_samples_per_second': 273.249, 'train_steps_per_second': 4.272, 'total_flos': 5145818086563840.0, 'train_loss': 0.04969053742397262, 'epoch': 12.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_args = TrainingArguments(\n",
    "    output_dir=\"./results/hyenadna32_finetune_len256_batch32_onlinedatagen\",            \n",
    "    num_train_epochs=12,                                \n",
    "    learning_rate=5e-4,                                  \n",
    "    per_device_train_batch_size=64,                      \n",
    "    per_device_eval_batch_size=64,                 \n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,                                \n",
    "    lr_scheduler_type=\"linear\",                          \n",
    "    warmup_ratio=0.10,                                   \n",
    "    fp16=True,                                           \n",
    "\n",
    "    logging_steps=100,                     \n",
    "    eval_steps=200,                                   \n",
    "    save_total_limit=3,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = token_model, \n",
    "    args=trainer_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,    \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "\n",
    "trainer.can_return_loss = True\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc752daf",
   "metadata": {},
   "source": [
    "## Experiment: Seq Length 32k, Electra-Small Head, Batch 32, Cosine LR Schedule  \n",
    "Used long DNA sequences (32,000 bp) with batch size = 32 and 10% warmup.  \n",
    "Replaced classification head with Electra-small https://huggingface.co/docs/transformers/en/model_doc/electra.  \n",
    "Used cosine learning rate schedule (base LR = 6e-4), trained for 12 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f313285a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_32376\\813748160.py:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_ira = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38723' max='49836' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38723/49836 37:29 < 10:45, 17.21 it/s, Epoch 9.32/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Mcc</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.320500</td>\n",
       "      <td>0.307105</td>\n",
       "      <td>0.868191</td>\n",
       "      <td>0.491232</td>\n",
       "      <td>0.496827</td>\n",
       "      <td>0.351742</td>\n",
       "      <td>0.845626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.296200</td>\n",
       "      <td>0.289125</td>\n",
       "      <td>0.897817</td>\n",
       "      <td>0.533356</td>\n",
       "      <td>0.549166</td>\n",
       "      <td>0.415730</td>\n",
       "      <td>0.808745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.287800</td>\n",
       "      <td>0.290366</td>\n",
       "      <td>0.878589</td>\n",
       "      <td>0.509593</td>\n",
       "      <td>0.516876</td>\n",
       "      <td>0.372504</td>\n",
       "      <td>0.843977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.287810</td>\n",
       "      <td>0.895816</td>\n",
       "      <td>0.531586</td>\n",
       "      <td>0.546183</td>\n",
       "      <td>0.410787</td>\n",
       "      <td>0.814712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.278100</td>\n",
       "      <td>0.284499</td>\n",
       "      <td>0.904939</td>\n",
       "      <td>0.547248</td>\n",
       "      <td>0.565043</td>\n",
       "      <td>0.436058</td>\n",
       "      <td>0.802386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.279625</td>\n",
       "      <td>0.885398</td>\n",
       "      <td>0.522383</td>\n",
       "      <td>0.530890</td>\n",
       "      <td>0.387509</td>\n",
       "      <td>0.842692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.269900</td>\n",
       "      <td>0.276699</td>\n",
       "      <td>0.888533</td>\n",
       "      <td>0.528369</td>\n",
       "      <td>0.537516</td>\n",
       "      <td>0.394816</td>\n",
       "      <td>0.841754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.264800</td>\n",
       "      <td>0.277931</td>\n",
       "      <td>0.891609</td>\n",
       "      <td>0.532301</td>\n",
       "      <td>0.542834</td>\n",
       "      <td>0.401842</td>\n",
       "      <td>0.836239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.258500</td>\n",
       "      <td>0.286954</td>\n",
       "      <td>0.894203</td>\n",
       "      <td>0.533117</td>\n",
       "      <td>0.545728</td>\n",
       "      <td>0.407515</td>\n",
       "      <td>0.825807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44298, 51, 2) (44298, 51)\n",
      "(44298, 51, 2) (44298, 51)\n",
      "(44298, 51, 2) (44298, 51)\n",
      "(44298, 51, 2) (44298, 51)\n",
      "(44298, 51, 2) (44298, 51)\n",
      "(44298, 51, 2) (44298, 51)\n",
      "(44298, 51, 2) (44298, 51)\n",
      "(44298, 51, 2) (44298, 51)\n",
      "(44298, 51, 2) (44298, 51)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32376\\813748160.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0mtrainer_ira\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcan_return_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m \u001b[0mtrainer_ira\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2240\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2241\u001b[1;33m             return inner_training_loop(\n\u001b[0m\u001b[0;32m   2242\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2243\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2546\u001b[0m                     \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2547\u001b[0m                     \u001b[1;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2548\u001b[1;33m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2550\u001b[0m                     if (\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3697\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3698\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3700\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3757\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"num_items_in_batch\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3758\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3759\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3760\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3761\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1735\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1736\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[1;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1746\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1747\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\accelerate\\utils\\operations.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\accelerate\\utils\\operations.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\amp\\autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\1\\vscode-projects\\coursework\\HyenaDNA config\\modeling_hyena.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, inputs_embeds, attention_mask, labels, output_hidden_states, return_dict, token_type_ids)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# 1) Прогоняем через бэкбон (HyenaDNAModel)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m         transformer_outputs = self.hyena(\n\u001b[0m\u001b[0;32m    659\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1735\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1736\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[1;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1746\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1747\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\1\\vscode-projects\\coursework\\HyenaDNA config\\modeling_hyena.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, inputs_embeds, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m             return BaseModelOutputWithNoAttention(last_hidden_state=hidden_states,\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1735\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1736\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[1;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1746\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1747\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\1\\vscode-projects\\coursework\\HyenaDNA config\\modeling_hyena.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, inputs_embeds, output_hidden_states)\u001b[0m\n\u001b[0;32m    373\u001b[0m                 \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m                 \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1735\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1736\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[1;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1746\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1747\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\1\\vscode-projects\\coursework\\HyenaDNA config\\modeling_hyena.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresidual\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[0mhyena_normed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresidual\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmixer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyena_normed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m         \u001b[1;31m# Tested above here and all is equivalent. That means the mixer is fine!!!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mresidual\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1735\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1736\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[1;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1746\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1747\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\1\\vscode-projects\\coursework\\HyenaDNA config\\modeling_hyena.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, u)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_filter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1735\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1736\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[1;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1746\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1747\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\1\\vscode-projects\\coursework\\HyenaDNA config\\modeling_hyena.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, L, k, bias, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;31m# 1) Применяем FFT‐свёртку: fftconv(u, k, bias)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfftconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[1;31m# 2) Возвращаем результат того же dtype, что и вход\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\1\\vscode-projects\\coursework\\HyenaDNA config\\modeling_hyena.py\u001b[0m in \u001b[0;36mfftconv\u001b[1;34m(u, k, D)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mu\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_args = TrainingArguments(\n",
    "    output_dir=\"./results/hyenadna32k_finetune_zdna_weights_from_online_batch32_lr6e-4_headelectra\", \n",
    "    label_names=[\"labels\"],\n",
    "    num_train_epochs=12,                               \n",
    "    learning_rate=6e-4,                             \n",
    "    per_device_train_batch_size=32,                     \n",
    "    per_device_eval_batch_size=32,             \n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,                             \n",
    "    lr_scheduler_type=\"cosine\",                          \n",
    "    warmup_ratio=0.10,                                 \n",
    "    fp16=True,                                          \n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"F1\",\n",
    "    greater_is_better=True,\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model, \n",
    "    args=trainer,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,     \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "\n",
    "trainer.can_return_loss = True\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c80c4f",
   "metadata": {},
   "source": [
    "## Experiment: No Special Tokens, Batch 128, LR=6e-4, ReduceLROnPlateau  \n",
    "Special tokens removed completely; trained with batch size = 128 and initial LR = 6e-4.  \n",
    "Used `ReduceLROnPlateau` to halve learning rate when F1 stagnated for 2 epochs.  \n",
    "Training lasted 12 epochs with FP16 and custom callback to monitor F1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b6ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_25628\\1558976234.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_ira = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9537' max='12468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9537/12468 16:43 < 05:08, 9.50 it/s, Epoch 9.18/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Mcc</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.309000</td>\n",
       "      <td>0.291467</td>\n",
       "      <td>0.892008</td>\n",
       "      <td>0.524823</td>\n",
       "      <td>0.538318</td>\n",
       "      <td>0.401122</td>\n",
       "      <td>0.818150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.290800</td>\n",
       "      <td>0.290680</td>\n",
       "      <td>0.880446</td>\n",
       "      <td>0.511828</td>\n",
       "      <td>0.519869</td>\n",
       "      <td>0.376196</td>\n",
       "      <td>0.841091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.281200</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.891674</td>\n",
       "      <td>0.529675</td>\n",
       "      <td>0.541190</td>\n",
       "      <td>0.401434</td>\n",
       "      <td>0.830225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>0.283255</td>\n",
       "      <td>0.879509</td>\n",
       "      <td>0.515003</td>\n",
       "      <td>0.521080</td>\n",
       "      <td>0.375346</td>\n",
       "      <td>0.851810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.270900</td>\n",
       "      <td>0.279929</td>\n",
       "      <td>0.902408</td>\n",
       "      <td>0.546489</td>\n",
       "      <td>0.562162</td>\n",
       "      <td>0.429291</td>\n",
       "      <td>0.814155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.267900</td>\n",
       "      <td>0.282277</td>\n",
       "      <td>0.874944</td>\n",
       "      <td>0.510701</td>\n",
       "      <td>0.514501</td>\n",
       "      <td>0.366846</td>\n",
       "      <td>0.861086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.260300</td>\n",
       "      <td>0.283494</td>\n",
       "      <td>0.884551</td>\n",
       "      <td>0.521813</td>\n",
       "      <td>0.529780</td>\n",
       "      <td>0.385814</td>\n",
       "      <td>0.845145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.254900</td>\n",
       "      <td>0.290365</td>\n",
       "      <td>0.894182</td>\n",
       "      <td>0.532509</td>\n",
       "      <td>0.545311</td>\n",
       "      <td>0.407349</td>\n",
       "      <td>0.824581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.245500</td>\n",
       "      <td>0.305073</td>\n",
       "      <td>0.890519</td>\n",
       "      <td>0.523605</td>\n",
       "      <td>0.536203</td>\n",
       "      <td>0.397775</td>\n",
       "      <td>0.822404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44298, 50, 2) (44298, 50)\n",
      "(44298, 50, 2) (44298, 50)\n",
      "(44298, 50, 2) (44298, 50)\n",
      "(44298, 50, 2) (44298, 50)\n",
      "(44298, 50, 2) (44298, 50)\n",
      "(44298, 50, 2) (44298, 50)\n",
      "(44298, 50, 2) (44298, 50)\n",
      "(44298, 50, 2) (44298, 50)\n",
      "(44298, 50, 2) (44298, 50)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25628\\1558976234.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;31m# 6) Тренируем\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[0mtrainer_ira\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcan_return_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m \u001b[0mtrainer_ira\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2240\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2241\u001b[1;33m             return inner_training_loop(\n\u001b[0m\u001b[0;32m   2242\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2243\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2498\u001b[0m                 \u001b[0mupdate_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2499\u001b[0m                 \u001b[0mnum_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mupdate_step\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtotal_updates\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mremainder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2500\u001b[1;33m                 \u001b[0mbatch_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_batch_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2501\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2502\u001b[0m                     \u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mget_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[0;32m   5178\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5179\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5180\u001b[1;33m                 \u001b[0mbatch_samples\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5181\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5182\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\accelerate\\data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    573\u001b[0m                     \u001b[0mcurrent_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msend_to_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_non_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m                 \u001b[0mnext_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mskip_batches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mcurrent_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    699\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             if (\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    757\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\u001b[0m in \u001b[0;36mpin_memory\u001b[1;34m(data, device)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir=\"./results/hyenadna_finetune_zdna_weights_from_safetensors_dynamiclr_128batch\",\n",
    "    num_train_epochs=12,\n",
    "    learning_rate=6e-4,\n",
    "    per_device_eval_batch_size=128,\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"F1\",\n",
    "    greater_is_better=True,\n",
    "    save_safetensors=False\n",
    ")\n",
    "\n",
    "optimiser = AdamW(model.parameters(), lr=6e-4, weight_decay=0.01, eps=1e-8)\n",
    "\n",
    "# if F1 does not grow for 2 consecutive epochs\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimiser,\n",
    "    mode=\"max\", # maximise F1\n",
    "    factor=0.5, # LR ← LR * 0.5\n",
    "    patience=2, \n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "class PlateauCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics is not None and \"eval_F1\" in metrics:\n",
    "            scheduler.step(metrics[\"eval_F1\"])\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=trainer_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    "    optimisers=(optimizer, scheduler),\n",
    "    callbacks=[PlateauCallback()],\n",
    ")\n",
    "\n",
    "trainer.can_return_loss = True\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d973cf",
   "metadata": {},
   "source": [
    "## Small experiments with loss function\n",
    "Basic params were cosine, adamw, lr 6e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de317ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20160' max='20160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20160/20160 32:31, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.279900</td>\n",
       "      <td>0.252806</td>\n",
       "      <td>0.930568</td>\n",
       "      <td>0.508955</td>\n",
       "      <td>0.735799</td>\n",
       "      <td>0.847231</td>\n",
       "      <td>0.507556</td>\n",
       "      <td>0.383830</td>\n",
       "      <td>0.755115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.250300</td>\n",
       "      <td>0.243347</td>\n",
       "      <td>0.940882</td>\n",
       "      <td>0.545212</td>\n",
       "      <td>0.756799</td>\n",
       "      <td>0.847197</td>\n",
       "      <td>0.538149</td>\n",
       "      <td>0.430372</td>\n",
       "      <td>0.743643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.240900</td>\n",
       "      <td>0.236158</td>\n",
       "      <td>0.947068</td>\n",
       "      <td>0.568871</td>\n",
       "      <td>0.770337</td>\n",
       "      <td>0.845319</td>\n",
       "      <td>0.558159</td>\n",
       "      <td>0.464856</td>\n",
       "      <td>0.732852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.236800</td>\n",
       "      <td>0.232937</td>\n",
       "      <td>0.940384</td>\n",
       "      <td>0.548037</td>\n",
       "      <td>0.758062</td>\n",
       "      <td>0.854001</td>\n",
       "      <td>0.542985</td>\n",
       "      <td>0.428996</td>\n",
       "      <td>0.758518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.232600</td>\n",
       "      <td>0.232972</td>\n",
       "      <td>0.931335</td>\n",
       "      <td>0.522649</td>\n",
       "      <td>0.742828</td>\n",
       "      <td>0.863662</td>\n",
       "      <td>0.525476</td>\n",
       "      <td>0.390776</td>\n",
       "      <td>0.788860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.228800</td>\n",
       "      <td>0.231512</td>\n",
       "      <td>0.943567</td>\n",
       "      <td>0.559388</td>\n",
       "      <td>0.764621</td>\n",
       "      <td>0.852464</td>\n",
       "      <td>0.552190</td>\n",
       "      <td>0.445409</td>\n",
       "      <td>0.751763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>0.230898</td>\n",
       "      <td>0.931686</td>\n",
       "      <td>0.524501</td>\n",
       "      <td>0.743850</td>\n",
       "      <td>0.864712</td>\n",
       "      <td>0.527426</td>\n",
       "      <td>0.392401</td>\n",
       "      <td>0.790682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.219200</td>\n",
       "      <td>0.234382</td>\n",
       "      <td>0.940442</td>\n",
       "      <td>0.549995</td>\n",
       "      <td>0.759053</td>\n",
       "      <td>0.856536</td>\n",
       "      <td>0.545570</td>\n",
       "      <td>0.429712</td>\n",
       "      <td>0.763791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.211200</td>\n",
       "      <td>0.235224</td>\n",
       "      <td>0.934573</td>\n",
       "      <td>0.531832</td>\n",
       "      <td>0.748330</td>\n",
       "      <td>0.861091</td>\n",
       "      <td>0.532030</td>\n",
       "      <td>0.403499</td>\n",
       "      <td>0.779867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.200600</td>\n",
       "      <td>0.249469</td>\n",
       "      <td>0.935798</td>\n",
       "      <td>0.532966</td>\n",
       "      <td>0.749248</td>\n",
       "      <td>0.856458</td>\n",
       "      <td>0.531171</td>\n",
       "      <td>0.407865</td>\n",
       "      <td>0.768759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.275293</td>\n",
       "      <td>0.933681</td>\n",
       "      <td>0.521731</td>\n",
       "      <td>0.743050</td>\n",
       "      <td>0.850763</td>\n",
       "      <td>0.519577</td>\n",
       "      <td>0.397446</td>\n",
       "      <td>0.759110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.184100</td>\n",
       "      <td>0.288055</td>\n",
       "      <td>0.933943</td>\n",
       "      <td>0.520545</td>\n",
       "      <td>0.742536</td>\n",
       "      <td>0.847773</td>\n",
       "      <td>0.517431</td>\n",
       "      <td>0.397888</td>\n",
       "      <td>0.752525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20160, training_loss=0.22491240728469122, metrics={'train_runtime': 1951.7222, 'train_samples_per_second': 660.709, 'train_steps_per_second': 10.329, 'total_flos': 2558451915694080.0, 'train_loss': 0.22491240728469122, 'epoch': 12.0})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()  # run training 1:3 classes generated online warmup 10 + added weights to classifier [1, 8] CE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5220a3ff",
   "metadata": {},
   "source": [
    "### Focal Loss (for imbalanced classification)\n",
    "\n",
    "Focal Loss is designed to address class imbalance by down-weighting easy examples and focusing training on hard, misclassified examples.  https://paperswithcode.com/method/focal-loss#:~:text=Focal%20loss%20applies%20a%20modulating,in%20the%20correct%20class%20increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a89f3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8457' max='33600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8457/33600 12:35 < 37:25, 11.20 it/s, Epoch 5.03/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.193400</td>\n",
       "      <td>0.405912</td>\n",
       "      <td>0.880728</td>\n",
       "      <td>0.399243</td>\n",
       "      <td>0.666518</td>\n",
       "      <td>0.857808</td>\n",
       "      <td>0.425665</td>\n",
       "      <td>0.262589</td>\n",
       "      <td>0.832475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.185100</td>\n",
       "      <td>0.372323</td>\n",
       "      <td>0.875730</td>\n",
       "      <td>0.393490</td>\n",
       "      <td>0.662132</td>\n",
       "      <td>0.861966</td>\n",
       "      <td>0.423407</td>\n",
       "      <td>0.256296</td>\n",
       "      <td>0.846754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.187300</td>\n",
       "      <td>0.412457</td>\n",
       "      <td>0.879520</td>\n",
       "      <td>0.397361</td>\n",
       "      <td>0.665216</td>\n",
       "      <td>0.858054</td>\n",
       "      <td>0.424364</td>\n",
       "      <td>0.260781</td>\n",
       "      <td>0.834328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.194600</td>\n",
       "      <td>0.348082</td>\n",
       "      <td>0.882708</td>\n",
       "      <td>0.407345</td>\n",
       "      <td>0.671129</td>\n",
       "      <td>0.865599</td>\n",
       "      <td>0.435678</td>\n",
       "      <td>0.268185</td>\n",
       "      <td>0.846690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.200900</td>\n",
       "      <td>0.369522</td>\n",
       "      <td>0.878574</td>\n",
       "      <td>0.396227</td>\n",
       "      <td>0.664363</td>\n",
       "      <td>0.858782</td>\n",
       "      <td>0.423871</td>\n",
       "      <td>0.259556</td>\n",
       "      <td>0.836908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17824\\389402736.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# запускаем обучение 1:3 классы сгенерил онлайн warmup 20 + добавил веса в классификатор [1, 1] focal loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2240\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2241\u001b[1;33m             return inner_training_loop(\n\u001b[0m\u001b[0;32m   2242\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2243\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2546\u001b[0m                     \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2547\u001b[0m                     \u001b[1;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2548\u001b[1;33m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2550\u001b[0m                     if (\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3738\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"scale_wrt_gas\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\accelerate\\accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2327\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2328\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2329\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m     _engine_run_backward(\n\u001b[0m\u001b[0;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m         \u001b[1;33m)\u001b[0m  \u001b[1;31m# Calls into the C++ engine to run the backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train() # start training 1:3 classes generated online warmup 20 + added weights to classifier [1, 1] focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759a56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15177' max='33600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15177/33600 22:29 < 27:18, 11.24 it/s, Epoch 9.03/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.311800</td>\n",
       "      <td>0.273547</td>\n",
       "      <td>0.858042</td>\n",
       "      <td>0.374703</td>\n",
       "      <td>0.647318</td>\n",
       "      <td>0.874847</td>\n",
       "      <td>0.416041</td>\n",
       "      <td>0.237065</td>\n",
       "      <td>0.893421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.281600</td>\n",
       "      <td>0.295222</td>\n",
       "      <td>0.889941</td>\n",
       "      <td>0.420872</td>\n",
       "      <td>0.680032</td>\n",
       "      <td>0.866235</td>\n",
       "      <td>0.446265</td>\n",
       "      <td>0.280772</td>\n",
       "      <td>0.840034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.279900</td>\n",
       "      <td>0.271724</td>\n",
       "      <td>0.865686</td>\n",
       "      <td>0.386583</td>\n",
       "      <td>0.655585</td>\n",
       "      <td>0.876763</td>\n",
       "      <td>0.425807</td>\n",
       "      <td>0.246994</td>\n",
       "      <td>0.889005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.906559</td>\n",
       "      <td>0.865346</td>\n",
       "      <td>0.192864</td>\n",
       "      <td>0.559705</td>\n",
       "      <td>0.614817</td>\n",
       "      <td>0.150893</td>\n",
       "      <td>0.134939</td>\n",
       "      <td>0.337923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.273100</td>\n",
       "      <td>0.271164</td>\n",
       "      <td>0.855961</td>\n",
       "      <td>0.372934</td>\n",
       "      <td>0.645785</td>\n",
       "      <td>0.876735</td>\n",
       "      <td>0.415728</td>\n",
       "      <td>0.235217</td>\n",
       "      <td>0.899695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.266224</td>\n",
       "      <td>0.883518</td>\n",
       "      <td>0.416185</td>\n",
       "      <td>0.675745</td>\n",
       "      <td>0.878091</td>\n",
       "      <td>0.448598</td>\n",
       "      <td>0.273307</td>\n",
       "      <td>0.872093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.261400</td>\n",
       "      <td>0.262352</td>\n",
       "      <td>0.859899</td>\n",
       "      <td>0.380176</td>\n",
       "      <td>0.650600</td>\n",
       "      <td>0.880139</td>\n",
       "      <td>0.422830</td>\n",
       "      <td>0.240807</td>\n",
       "      <td>0.902510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.258200</td>\n",
       "      <td>0.264262</td>\n",
       "      <td>0.861008</td>\n",
       "      <td>0.381618</td>\n",
       "      <td>0.651661</td>\n",
       "      <td>0.879939</td>\n",
       "      <td>0.423787</td>\n",
       "      <td>0.242084</td>\n",
       "      <td>0.900862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.254200</td>\n",
       "      <td>0.262665</td>\n",
       "      <td>0.861559</td>\n",
       "      <td>0.382886</td>\n",
       "      <td>0.652460</td>\n",
       "      <td>0.880827</td>\n",
       "      <td>0.425184</td>\n",
       "      <td>0.243014</td>\n",
       "      <td>0.902123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17824\\2893198740.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# запускаем обучение 1:3 классы сгенерил онлайн warmup 20 + добавил веса в классификатор [1, 20] кросс энтропии\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2240\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2241\u001b[1;33m             return inner_training_loop(\n\u001b[0m\u001b[0;32m   2242\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2243\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2498\u001b[0m                 \u001b[0mupdate_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2499\u001b[0m                 \u001b[0mnum_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mupdate_step\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtotal_updates\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mremainder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2500\u001b[1;33m                 \u001b[0mbatch_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_batch_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2501\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2502\u001b[0m                     \u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mget_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[0;32m   5178\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5179\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5180\u001b[1;33m                 \u001b[0mbatch_samples\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5181\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5182\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\accelerate\\data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    573\u001b[0m                     \u001b[0mcurrent_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msend_to_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_non_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m                 \u001b[0mnext_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mskip_batches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mcurrent_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    699\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m             if (\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    757\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\u001b[0m in \u001b[0;36mpin_memory\u001b[1;34m(data, device)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train() # run training 1:3 classes generated online warmup 20 + added weights to classifier [1, 20] cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd4f0af",
   "metadata": {},
   "source": [
    "# PEFT methods for Z-DNA task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "576b3f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyena.backbone.embeddings.word_embeddings.weight\n",
      "hyena.backbone.layers.0.mixer.in_proj.weight\n",
      "hyena.backbone.layers.0.mixer.in_proj.bias\n",
      "hyena.backbone.layers.0.mixer.out_proj.weight\n",
      "hyena.backbone.layers.0.mixer.out_proj.bias\n",
      "hyena.backbone.layers.0.mixer.short_filter.weight\n",
      "hyena.backbone.layers.0.mixer.short_filter.bias\n",
      "hyena.backbone.layers.0.mixer.filter_fn.bias\n",
      "hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.0.weight\n",
      "hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.0.bias\n",
      "hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.1.freq\n",
      "hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.2.weight\n",
      "hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.2.bias\n",
      "hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.4.weight\n",
      "hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.4.bias\n",
      "hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.6.weight\n",
      "hyena.backbone.layers.0.norm1.weight\n",
      "hyena.backbone.layers.0.norm1.bias\n",
      "hyena.backbone.layers.0.mlp.fc1.weight\n",
      "hyena.backbone.layers.0.mlp.fc1.bias\n",
      "hyena.backbone.layers.0.mlp.fc2.weight\n",
      "hyena.backbone.layers.0.mlp.fc2.bias\n",
      "hyena.backbone.layers.0.norm2.weight\n",
      "hyena.backbone.layers.0.norm2.bias\n",
      "hyena.backbone.layers.1.mixer.in_proj.weight\n",
      "hyena.backbone.layers.1.mixer.in_proj.bias\n",
      "hyena.backbone.layers.1.mixer.out_proj.weight\n",
      "hyena.backbone.layers.1.mixer.out_proj.bias\n",
      "hyena.backbone.layers.1.mixer.short_filter.weight\n",
      "hyena.backbone.layers.1.mixer.short_filter.bias\n",
      "hyena.backbone.layers.1.mixer.filter_fn.bias\n",
      "hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.0.weight\n",
      "hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.0.bias\n",
      "hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.1.freq\n",
      "hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.2.weight\n",
      "hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.2.bias\n",
      "hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.4.weight\n",
      "hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.4.bias\n",
      "hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.6.weight\n",
      "hyena.backbone.layers.1.norm1.weight\n",
      "hyena.backbone.layers.1.norm1.bias\n",
      "hyena.backbone.layers.1.mlp.fc1.weight\n",
      "hyena.backbone.layers.1.mlp.fc1.bias\n",
      "hyena.backbone.layers.1.mlp.fc2.weight\n",
      "hyena.backbone.layers.1.mlp.fc2.bias\n",
      "hyena.backbone.layers.1.norm2.weight\n",
      "hyena.backbone.layers.1.norm2.bias\n",
      "hyena.backbone.ln_f.weight\n",
      "hyena.backbone.ln_f.bias\n",
      "classifier.weight\n"
     ]
    }
   ],
   "source": [
    "for name, param in token_model.named_parameters():\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "339ad482",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_target_modules = [\n",
    "    # Mixer layers\n",
    "    \"hyena.backbone.layers.0.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.0.mixer.out_proj\",\n",
    "    \"hyena.backbone.layers.1.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.1.mixer.out_proj\",\n",
    "    \"hyena.backbone.layers.2.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.2.mixer.out_proj\",\n",
    "    \"hyena.backbone.layers.3.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.3.mixer.out_proj\",\n",
    "    \n",
    "    # MLP (FeedforwardNetwork) layers\n",
    "    \"hyena.backbone.layers.0.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.0.mlp.fc2\",\n",
    "    \"hyena.backbone.layers.1.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.1.mlp.fc2\",\n",
    "    \"hyena.backbone.layers.2.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.2.mlp.fc2\",\n",
    "    \"hyena.backbone.layers.3.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.3.mlp.fc2\",\n",
    "]\n",
    "mixer_only = [\n",
    "    \"hyena.backbone.layers.0.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.0.mixer.out_proj\",\n",
    "    \"hyena.backbone.layers.1.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.1.mixer.out_proj\",\n",
    "    \"hyena.backbone.layers.2.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.2.mixer.out_proj\",\n",
    "    \"hyena.backbone.layers.3.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.3.mixer.out_proj\",\n",
    "]\n",
    "ffn_only = [\n",
    "    \"hyena.backbone.layers.0.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.0.mlp.fc2\",\n",
    "    \"hyena.backbone.layers.1.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.1.mlp.fc2\",\n",
    "    \"hyena.backbone.layers.2.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.2.mlp.fc2\",\n",
    "    \"hyena.backbone.layers.3.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.3.mlp.fc2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685f9445",
   "metadata": {},
   "source": [
    "## LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f48a805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import TrainingArguments, Trainer\n",
    "base_model = token_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86a4041",
   "metadata": {},
   "source": [
    "### Experiment 1: full modules, 3 epochs, BS=64, LR=2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63100960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\peft\\mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 131,584 || all params: 3,409,664 || trainable%: 3.8591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5040' max='5040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5040/5040 10:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.160037</td>\n",
       "      <td>0.961979</td>\n",
       "      <td>0.617935</td>\n",
       "      <td>0.811808</td>\n",
       "      <td>0.598588</td>\n",
       "      <td>0.592348</td>\n",
       "      <td>0.645834</td>\n",
       "      <td>0.798965</td>\n",
       "      <td>0.961979</td>\n",
       "      <td>0.962758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.143300</td>\n",
       "      <td>0.156194</td>\n",
       "      <td>0.965812</td>\n",
       "      <td>0.634153</td>\n",
       "      <td>0.802687</td>\n",
       "      <td>0.616349</td>\n",
       "      <td>0.646365</td>\n",
       "      <td>0.622395</td>\n",
       "      <td>0.808111</td>\n",
       "      <td>0.965812</td>\n",
       "      <td>0.965505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.164100</td>\n",
       "      <td>0.155316</td>\n",
       "      <td>0.965774</td>\n",
       "      <td>0.636263</td>\n",
       "      <td>0.805698</td>\n",
       "      <td>0.618357</td>\n",
       "      <td>0.643932</td>\n",
       "      <td>0.628775</td>\n",
       "      <td>0.809153</td>\n",
       "      <td>0.965774</td>\n",
       "      <td>0.965581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_config1 = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS, inference_mode=False,\n",
    "    r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=full_target_modules,\n",
    ")\n",
    "model1 = get_peft_model(base_model, peft_config1)\n",
    "model1.print_trainable_parameters()\n",
    "\n",
    "training_args1 = TrainingArguments(\n",
    "    output_dir=\"./results/exp1_full_3ep_6e-4\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs/exp1_full_3ep_6e-4\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "trainer1 = Trainer(\n",
    "    model=model1,\n",
    "    args=training_args1,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "trainer1.train()\n",
    "model1 = model1.unload()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b9a76e",
   "metadata": {},
   "source": [
    " [5040/5040 10:32, Epoch 3/3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60740e6",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8532bf",
   "metadata": {},
   "source": [
    "### Experiment 2: full modules, 10 epochs, BS=64, LR=5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "407f20a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 131,584 || all params: 3,409,664 || trainable%: 3.8591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16800' max='16800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16800/16800 35:38, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.164612</td>\n",
       "      <td>0.964204</td>\n",
       "      <td>0.612460</td>\n",
       "      <td>0.788419</td>\n",
       "      <td>0.594025</td>\n",
       "      <td>0.631950</td>\n",
       "      <td>0.594136</td>\n",
       "      <td>0.796848</td>\n",
       "      <td>0.964204</td>\n",
       "      <td>0.963679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.151400</td>\n",
       "      <td>0.159206</td>\n",
       "      <td>0.964871</td>\n",
       "      <td>0.626139</td>\n",
       "      <td>0.800059</td>\n",
       "      <td>0.607771</td>\n",
       "      <td>0.634597</td>\n",
       "      <td>0.617903</td>\n",
       "      <td>0.803854</td>\n",
       "      <td>0.964871</td>\n",
       "      <td>0.964648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.170400</td>\n",
       "      <td>0.156798</td>\n",
       "      <td>0.964906</td>\n",
       "      <td>0.631038</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>0.612615</td>\n",
       "      <td>0.631707</td>\n",
       "      <td>0.630370</td>\n",
       "      <td>0.806308</td>\n",
       "      <td>0.964906</td>\n",
       "      <td>0.964889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.164600</td>\n",
       "      <td>0.157563</td>\n",
       "      <td>0.962563</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0.815243</td>\n",
       "      <td>0.604945</td>\n",
       "      <td>0.597892</td>\n",
       "      <td>0.652419</td>\n",
       "      <td>0.802134</td>\n",
       "      <td>0.962563</td>\n",
       "      <td>0.963337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.134300</td>\n",
       "      <td>0.157063</td>\n",
       "      <td>0.962942</td>\n",
       "      <td>0.626714</td>\n",
       "      <td>0.815926</td>\n",
       "      <td>0.607815</td>\n",
       "      <td>0.602089</td>\n",
       "      <td>0.653439</td>\n",
       "      <td>0.803609</td>\n",
       "      <td>0.962942</td>\n",
       "      <td>0.963660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.144400</td>\n",
       "      <td>0.157187</td>\n",
       "      <td>0.966399</td>\n",
       "      <td>0.635891</td>\n",
       "      <td>0.800106</td>\n",
       "      <td>0.618639</td>\n",
       "      <td>0.656752</td>\n",
       "      <td>0.616314</td>\n",
       "      <td>0.809139</td>\n",
       "      <td>0.966399</td>\n",
       "      <td>0.965891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.135500</td>\n",
       "      <td>0.158635</td>\n",
       "      <td>0.963429</td>\n",
       "      <td>0.625672</td>\n",
       "      <td>0.810739</td>\n",
       "      <td>0.606677</td>\n",
       "      <td>0.610172</td>\n",
       "      <td>0.641981</td>\n",
       "      <td>0.803224</td>\n",
       "      <td>0.963429</td>\n",
       "      <td>0.963870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.131600</td>\n",
       "      <td>0.160245</td>\n",
       "      <td>0.963050</td>\n",
       "      <td>0.624941</td>\n",
       "      <td>0.812746</td>\n",
       "      <td>0.605909</td>\n",
       "      <td>0.604664</td>\n",
       "      <td>0.646625</td>\n",
       "      <td>0.802754</td>\n",
       "      <td>0.963050</td>\n",
       "      <td>0.963637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.162865</td>\n",
       "      <td>0.964397</td>\n",
       "      <td>0.624198</td>\n",
       "      <td>0.801317</td>\n",
       "      <td>0.605520</td>\n",
       "      <td>0.627352</td>\n",
       "      <td>0.621075</td>\n",
       "      <td>0.802756</td>\n",
       "      <td>0.964397</td>\n",
       "      <td>0.964312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.167941</td>\n",
       "      <td>0.963396</td>\n",
       "      <td>0.618135</td>\n",
       "      <td>0.801368</td>\n",
       "      <td>0.598928</td>\n",
       "      <td>0.614036</td>\n",
       "      <td>0.622289</td>\n",
       "      <td>0.799456</td>\n",
       "      <td>0.963396</td>\n",
       "      <td>0.963512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_config2 = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS, inference_mode=False,\n",
    "    r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=full_target_modules,\n",
    ")\n",
    "model2 = get_peft_model(base_model, peft_config2)\n",
    "model2.print_trainable_parameters()\n",
    "\n",
    "training_args2 = TrainingArguments(\n",
    "    output_dir=\"./results/exp2_full_10ep_6e-4\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs/exp2_full_10ep_6e-4\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.10,\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "trainer2 = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args2,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "trainer2.train()\n",
    "model2 = model2.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018242c3",
   "metadata": {},
   "source": [
    " [16800/16800 35:08, Epoch 10/10]\n",
    " [16800/16800 35:38, Epoch 10/10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a85819c",
   "metadata": {},
   "source": [
    "### Experiment 3: mixer only, 10 epochs, BS=64, LR=5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a104d105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 49,664 || all params: 3,327,744 || trainable%: 1.4924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16800' max='16800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16800/16800 32:28, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.160752</td>\n",
       "      <td>0.961717</td>\n",
       "      <td>0.615811</td>\n",
       "      <td>0.811021</td>\n",
       "      <td>0.596361</td>\n",
       "      <td>0.589594</td>\n",
       "      <td>0.644467</td>\n",
       "      <td>0.797833</td>\n",
       "      <td>0.961717</td>\n",
       "      <td>0.962524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.149100</td>\n",
       "      <td>0.159237</td>\n",
       "      <td>0.965671</td>\n",
       "      <td>0.628372</td>\n",
       "      <td>0.796549</td>\n",
       "      <td>0.610711</td>\n",
       "      <td>0.648303</td>\n",
       "      <td>0.609629</td>\n",
       "      <td>0.805188</td>\n",
       "      <td>0.965671</td>\n",
       "      <td>0.965169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.171100</td>\n",
       "      <td>0.157576</td>\n",
       "      <td>0.963898</td>\n",
       "      <td>0.627236</td>\n",
       "      <td>0.809099</td>\n",
       "      <td>0.608367</td>\n",
       "      <td>0.616819</td>\n",
       "      <td>0.638011</td>\n",
       "      <td>0.804133</td>\n",
       "      <td>0.963898</td>\n",
       "      <td>0.964187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.162900</td>\n",
       "      <td>0.157823</td>\n",
       "      <td>0.964282</td>\n",
       "      <td>0.627983</td>\n",
       "      <td>0.807034</td>\n",
       "      <td>0.609247</td>\n",
       "      <td>0.622814</td>\n",
       "      <td>0.633238</td>\n",
       "      <td>0.804611</td>\n",
       "      <td>0.964282</td>\n",
       "      <td>0.964423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.136500</td>\n",
       "      <td>0.157460</td>\n",
       "      <td>0.963550</td>\n",
       "      <td>0.628059</td>\n",
       "      <td>0.812917</td>\n",
       "      <td>0.609183</td>\n",
       "      <td>0.610702</td>\n",
       "      <td>0.646432</td>\n",
       "      <td>0.804447</td>\n",
       "      <td>0.963550</td>\n",
       "      <td>0.964041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>0.157375</td>\n",
       "      <td>0.965467</td>\n",
       "      <td>0.632575</td>\n",
       "      <td>0.803464</td>\n",
       "      <td>0.614518</td>\n",
       "      <td>0.640954</td>\n",
       "      <td>0.624412</td>\n",
       "      <td>0.807228</td>\n",
       "      <td>0.965467</td>\n",
       "      <td>0.965252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.139600</td>\n",
       "      <td>0.157525</td>\n",
       "      <td>0.964434</td>\n",
       "      <td>0.629517</td>\n",
       "      <td>0.807810</td>\n",
       "      <td>0.610861</td>\n",
       "      <td>0.624414</td>\n",
       "      <td>0.634704</td>\n",
       "      <td>0.805419</td>\n",
       "      <td>0.964434</td>\n",
       "      <td>0.964572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.133500</td>\n",
       "      <td>0.158189</td>\n",
       "      <td>0.963845</td>\n",
       "      <td>0.628271</td>\n",
       "      <td>0.810863</td>\n",
       "      <td>0.609428</td>\n",
       "      <td>0.615318</td>\n",
       "      <td>0.641782</td>\n",
       "      <td>0.804635</td>\n",
       "      <td>0.963845</td>\n",
       "      <td>0.964206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.148600</td>\n",
       "      <td>0.158479</td>\n",
       "      <td>0.965402</td>\n",
       "      <td>0.632323</td>\n",
       "      <td>0.803664</td>\n",
       "      <td>0.614220</td>\n",
       "      <td>0.639919</td>\n",
       "      <td>0.624905</td>\n",
       "      <td>0.807085</td>\n",
       "      <td>0.965402</td>\n",
       "      <td>0.965207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.160700</td>\n",
       "      <td>0.160094</td>\n",
       "      <td>0.964733</td>\n",
       "      <td>0.629127</td>\n",
       "      <td>0.804928</td>\n",
       "      <td>0.610614</td>\n",
       "      <td>0.629950</td>\n",
       "      <td>0.628306</td>\n",
       "      <td>0.805307</td>\n",
       "      <td>0.964733</td>\n",
       "      <td>0.964712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_config3 = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS, inference_mode=False,\n",
    "    r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=mixer_only,\n",
    ")\n",
    "model3 = get_peft_model(base_model, peft_config3)\n",
    "model3.print_trainable_parameters()\n",
    "\n",
    "training_args3 = TrainingArguments(\n",
    "    output_dir=\"./results/exp3_mixer_only_5ep_6e-4\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs/exp3_mixer_only_5ep_6e-4\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "trainer3 = Trainer(\n",
    "    model=model3,\n",
    "    args=training_args3,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "trainer3.train()\n",
    "model3 = model3.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6bd666",
   "metadata": {},
   "source": [
    " [8400/8400 16:10, Epoch 5/5]\n",
    " [16800/16800 32:28, Epoch 10/10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0144b825",
   "metadata": {},
   "source": [
    "### Experiment 4: ffn only, 10 epochs, BS=64, LR=5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "145f8291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,736 || all params: 457,088 || trainable%: 4.5365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16800' max='16800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16800/16800 23:19, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.166469</td>\n",
       "      <td>0.961644</td>\n",
       "      <td>0.606797</td>\n",
       "      <td>0.800150</td>\n",
       "      <td>0.586833</td>\n",
       "      <td>0.592627</td>\n",
       "      <td>0.621662</td>\n",
       "      <td>0.793318</td>\n",
       "      <td>0.961644</td>\n",
       "      <td>0.962079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.155300</td>\n",
       "      <td>0.163733</td>\n",
       "      <td>0.965401</td>\n",
       "      <td>0.618710</td>\n",
       "      <td>0.786917</td>\n",
       "      <td>0.601436</td>\n",
       "      <td>0.650782</td>\n",
       "      <td>0.589650</td>\n",
       "      <td>0.800294</td>\n",
       "      <td>0.965401</td>\n",
       "      <td>0.964589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.184400</td>\n",
       "      <td>0.162027</td>\n",
       "      <td>0.963700</td>\n",
       "      <td>0.619446</td>\n",
       "      <td>0.800706</td>\n",
       "      <td>0.600388</td>\n",
       "      <td>0.618337</td>\n",
       "      <td>0.620559</td>\n",
       "      <td>0.800194</td>\n",
       "      <td>0.963700</td>\n",
       "      <td>0.963731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.161944</td>\n",
       "      <td>0.964062</td>\n",
       "      <td>0.620789</td>\n",
       "      <td>0.799626</td>\n",
       "      <td>0.601934</td>\n",
       "      <td>0.623719</td>\n",
       "      <td>0.617885</td>\n",
       "      <td>0.800963</td>\n",
       "      <td>0.964062</td>\n",
       "      <td>0.963982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.142100</td>\n",
       "      <td>0.161307</td>\n",
       "      <td>0.963433</td>\n",
       "      <td>0.620025</td>\n",
       "      <td>0.803468</td>\n",
       "      <td>0.600857</td>\n",
       "      <td>0.613520</td>\n",
       "      <td>0.626670</td>\n",
       "      <td>0.800409</td>\n",
       "      <td>0.963433</td>\n",
       "      <td>0.963617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.157600</td>\n",
       "      <td>0.160426</td>\n",
       "      <td>0.964982</td>\n",
       "      <td>0.625304</td>\n",
       "      <td>0.798151</td>\n",
       "      <td>0.607062</td>\n",
       "      <td>0.637288</td>\n",
       "      <td>0.613763</td>\n",
       "      <td>0.803468</td>\n",
       "      <td>0.964982</td>\n",
       "      <td>0.964669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.145200</td>\n",
       "      <td>0.160416</td>\n",
       "      <td>0.963772</td>\n",
       "      <td>0.621595</td>\n",
       "      <td>0.802858</td>\n",
       "      <td>0.602581</td>\n",
       "      <td>0.618217</td>\n",
       "      <td>0.625010</td>\n",
       "      <td>0.801285</td>\n",
       "      <td>0.963772</td>\n",
       "      <td>0.963866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>0.159653</td>\n",
       "      <td>0.964438</td>\n",
       "      <td>0.624926</td>\n",
       "      <td>0.801913</td>\n",
       "      <td>0.606266</td>\n",
       "      <td>0.627590</td>\n",
       "      <td>0.622283</td>\n",
       "      <td>0.803130</td>\n",
       "      <td>0.964438</td>\n",
       "      <td>0.964367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.157700</td>\n",
       "      <td>0.159402</td>\n",
       "      <td>0.964590</td>\n",
       "      <td>0.625555</td>\n",
       "      <td>0.801519</td>\n",
       "      <td>0.606988</td>\n",
       "      <td>0.629882</td>\n",
       "      <td>0.621287</td>\n",
       "      <td>0.803486</td>\n",
       "      <td>0.964590</td>\n",
       "      <td>0.964475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.172900</td>\n",
       "      <td>0.159317</td>\n",
       "      <td>0.964523</td>\n",
       "      <td>0.625713</td>\n",
       "      <td>0.802249</td>\n",
       "      <td>0.607100</td>\n",
       "      <td>0.628553</td>\n",
       "      <td>0.622899</td>\n",
       "      <td>0.803546</td>\n",
       "      <td>0.964523</td>\n",
       "      <td>0.964446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_config4 = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS, inference_mode=False,\n",
    "    r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=ffn_only,\n",
    ")\n",
    "model4 = get_peft_model(base_model, peft_config4)\n",
    "model4.print_trainable_parameters()\n",
    "\n",
    "training_args4 = TrainingArguments(\n",
    "    output_dir=\"./results/exp4_ffn_only_5ep_6e-4\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs/exp4_ffn_only_5ep_6e-4\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "trainer4 = Trainer(\n",
    "    model=model4,\n",
    "    args=training_args4,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "trainer4.train()\n",
    "model4 = model4.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f622de2",
   "metadata": {},
   "source": [
    " [8400/8400 15:56, Epoch 5/5]\n",
    " [16800/16800 23:19, Epoch 10/10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cea419b",
   "metadata": {},
   "source": [
    "### Experiment 5: short_filter only, 10 epochs, BS=64, LR=5e-4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6393903a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 24,832 || all params: 461,184 || trainable%: 5.3844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16800' max='16800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16800/16800 23:17, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.166600</td>\n",
       "      <td>0.169956</td>\n",
       "      <td>0.964363</td>\n",
       "      <td>0.608199</td>\n",
       "      <td>0.782263</td>\n",
       "      <td>0.590284</td>\n",
       "      <td>0.638068</td>\n",
       "      <td>0.581000</td>\n",
       "      <td>0.794766</td>\n",
       "      <td>0.964363</td>\n",
       "      <td>0.963569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.165104</td>\n",
       "      <td>0.965043</td>\n",
       "      <td>0.618326</td>\n",
       "      <td>0.789166</td>\n",
       "      <td>0.600555</td>\n",
       "      <td>0.643812</td>\n",
       "      <td>0.594781</td>\n",
       "      <td>0.800004</td>\n",
       "      <td>0.965043</td>\n",
       "      <td>0.964384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.183400</td>\n",
       "      <td>0.164056</td>\n",
       "      <td>0.965777</td>\n",
       "      <td>0.621382</td>\n",
       "      <td>0.787228</td>\n",
       "      <td>0.604459</td>\n",
       "      <td>0.656426</td>\n",
       "      <td>0.589890</td>\n",
       "      <td>0.801730</td>\n",
       "      <td>0.965777</td>\n",
       "      <td>0.964907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.169500</td>\n",
       "      <td>0.162801</td>\n",
       "      <td>0.963843</td>\n",
       "      <td>0.620868</td>\n",
       "      <td>0.801405</td>\n",
       "      <td>0.601885</td>\n",
       "      <td>0.619866</td>\n",
       "      <td>0.621873</td>\n",
       "      <td>0.800942</td>\n",
       "      <td>0.963843</td>\n",
       "      <td>0.963870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.144700</td>\n",
       "      <td>0.163211</td>\n",
       "      <td>0.961128</td>\n",
       "      <td>0.613046</td>\n",
       "      <td>0.811821</td>\n",
       "      <td>0.593537</td>\n",
       "      <td>0.582640</td>\n",
       "      <td>0.646801</td>\n",
       "      <td>0.796291</td>\n",
       "      <td>0.961128</td>\n",
       "      <td>0.962088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.156900</td>\n",
       "      <td>0.163572</td>\n",
       "      <td>0.966554</td>\n",
       "      <td>0.625789</td>\n",
       "      <td>0.786461</td>\n",
       "      <td>0.609789</td>\n",
       "      <td>0.669527</td>\n",
       "      <td>0.587416</td>\n",
       "      <td>0.804142</td>\n",
       "      <td>0.966554</td>\n",
       "      <td>0.965513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.161179</td>\n",
       "      <td>0.963815</td>\n",
       "      <td>0.622346</td>\n",
       "      <td>0.803482</td>\n",
       "      <td>0.603357</td>\n",
       "      <td>0.618465</td>\n",
       "      <td>0.626277</td>\n",
       "      <td>0.801672</td>\n",
       "      <td>0.963815</td>\n",
       "      <td>0.963922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.160800</td>\n",
       "      <td>0.161534</td>\n",
       "      <td>0.961099</td>\n",
       "      <td>0.615802</td>\n",
       "      <td>0.815633</td>\n",
       "      <td>0.596569</td>\n",
       "      <td>0.581143</td>\n",
       "      <td>0.654858</td>\n",
       "      <td>0.797657</td>\n",
       "      <td>0.961099</td>\n",
       "      <td>0.962197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.159741</td>\n",
       "      <td>0.964815</td>\n",
       "      <td>0.626889</td>\n",
       "      <td>0.801444</td>\n",
       "      <td>0.608459</td>\n",
       "      <td>0.633013</td>\n",
       "      <td>0.620882</td>\n",
       "      <td>0.804213</td>\n",
       "      <td>0.964815</td>\n",
       "      <td>0.964653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.174600</td>\n",
       "      <td>0.159435</td>\n",
       "      <td>0.964970</td>\n",
       "      <td>0.627892</td>\n",
       "      <td>0.801490</td>\n",
       "      <td>0.609558</td>\n",
       "      <td>0.635141</td>\n",
       "      <td>0.620806</td>\n",
       "      <td>0.804756</td>\n",
       "      <td>0.964970</td>\n",
       "      <td>0.964780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "short_filter_only = [f\"hyena.backbone.layers.{i}.mixer.short_filter\" for i in range(4)]\n",
    "peft_config5 = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS, inference_mode=False,\n",
    "    r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=short_filter_only,\n",
    ")\n",
    "model5 = get_peft_model(base_model, peft_config5)\n",
    "model5.print_trainable_parameters()\n",
    "\n",
    "training_args5 = TrainingArguments(\n",
    "    output_dir=\"./results/exp5_short_filter_only_5ep_6e-4\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs/exp5_short_filter_only_5ep_6e-4\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_safetensors=False\n",
    ")\n",
    "\n",
    "trainer5 = Trainer(\n",
    "    model=model5,\n",
    "    args=training_args5,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "trainer5.train()\n",
    "model5 = model5.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3997b60",
   "metadata": {},
   "source": [
    " [8400/8400 16:54, Epoch 5/5]\n",
    "  [16800/16800 23:17, Epoch 10/10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dabfdb",
   "metadata": {},
   "source": [
    "### Experiment 6: FFN modules, 10 epochs, BS=32, LR=5e-4, r=16, r-alpha = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4089febd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 45,568 || all params: 477,568 || trainable%: 9.5417\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33590' max='33590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33590/33590 31:13, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.166500</td>\n",
       "      <td>0.163317</td>\n",
       "      <td>0.965660</td>\n",
       "      <td>0.620755</td>\n",
       "      <td>0.787376</td>\n",
       "      <td>0.603700</td>\n",
       "      <td>0.654485</td>\n",
       "      <td>0.590330</td>\n",
       "      <td>0.801385</td>\n",
       "      <td>0.965660</td>\n",
       "      <td>0.964817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.156300</td>\n",
       "      <td>0.160406</td>\n",
       "      <td>0.965599</td>\n",
       "      <td>0.624784</td>\n",
       "      <td>0.792703</td>\n",
       "      <td>0.607280</td>\n",
       "      <td>0.649812</td>\n",
       "      <td>0.601613</td>\n",
       "      <td>0.803379</td>\n",
       "      <td>0.965599</td>\n",
       "      <td>0.964968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.153300</td>\n",
       "      <td>0.158879</td>\n",
       "      <td>0.964964</td>\n",
       "      <td>0.626390</td>\n",
       "      <td>0.799643</td>\n",
       "      <td>0.608093</td>\n",
       "      <td>0.636150</td>\n",
       "      <td>0.616924</td>\n",
       "      <td>0.804005</td>\n",
       "      <td>0.964964</td>\n",
       "      <td>0.964709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.144600</td>\n",
       "      <td>0.158632</td>\n",
       "      <td>0.964616</td>\n",
       "      <td>0.626592</td>\n",
       "      <td>0.802632</td>\n",
       "      <td>0.608028</td>\n",
       "      <td>0.629610</td>\n",
       "      <td>0.623603</td>\n",
       "      <td>0.804010</td>\n",
       "      <td>0.964616</td>\n",
       "      <td>0.964535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.159000</td>\n",
       "      <td>0.158173</td>\n",
       "      <td>0.962669</td>\n",
       "      <td>0.622262</td>\n",
       "      <td>0.812192</td>\n",
       "      <td>0.603100</td>\n",
       "      <td>0.600311</td>\n",
       "      <td>0.645880</td>\n",
       "      <td>0.801313</td>\n",
       "      <td>0.962669</td>\n",
       "      <td>0.963316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.158516</td>\n",
       "      <td>0.966434</td>\n",
       "      <td>0.631597</td>\n",
       "      <td>0.794454</td>\n",
       "      <td>0.614735</td>\n",
       "      <td>0.661387</td>\n",
       "      <td>0.604375</td>\n",
       "      <td>0.807007</td>\n",
       "      <td>0.966434</td>\n",
       "      <td>0.965715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.155700</td>\n",
       "      <td>0.158078</td>\n",
       "      <td>0.963785</td>\n",
       "      <td>0.624866</td>\n",
       "      <td>0.806923</td>\n",
       "      <td>0.605907</td>\n",
       "      <td>0.616413</td>\n",
       "      <td>0.633554</td>\n",
       "      <td>0.802920</td>\n",
       "      <td>0.963785</td>\n",
       "      <td>0.964021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.150100</td>\n",
       "      <td>0.157569</td>\n",
       "      <td>0.963614</td>\n",
       "      <td>0.625798</td>\n",
       "      <td>0.809463</td>\n",
       "      <td>0.606829</td>\n",
       "      <td>0.613048</td>\n",
       "      <td>0.639090</td>\n",
       "      <td>0.803338</td>\n",
       "      <td>0.963614</td>\n",
       "      <td>0.963973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.147800</td>\n",
       "      <td>0.157193</td>\n",
       "      <td>0.965029</td>\n",
       "      <td>0.629649</td>\n",
       "      <td>0.803248</td>\n",
       "      <td>0.611322</td>\n",
       "      <td>0.634945</td>\n",
       "      <td>0.624441</td>\n",
       "      <td>0.805649</td>\n",
       "      <td>0.965029</td>\n",
       "      <td>0.964890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.158300</td>\n",
       "      <td>0.157206</td>\n",
       "      <td>0.965057</td>\n",
       "      <td>0.630105</td>\n",
       "      <td>0.803605</td>\n",
       "      <td>0.611789</td>\n",
       "      <td>0.635126</td>\n",
       "      <td>0.625163</td>\n",
       "      <td>0.805883</td>\n",
       "      <td>0.965057</td>\n",
       "      <td>0.964925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_config6 = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS, inference_mode=False,\n",
    "    r=16, lora_alpha=16, lora_dropout=0.1,\n",
    "    target_modules=ffn_only, bias='all'\n",
    ")\n",
    "model6 = get_peft_model(base_model, peft_config6)\n",
    "model6.print_trainable_parameters()\n",
    "\n",
    "training_args6 = TrainingArguments(\n",
    "    output_dir=\"./results/exp4_ffn_only_5ep_6e-4\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs/exp4_ffn_only_5ep_6e-4\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_safetensors=False,\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "trainer6 = Trainer(\n",
    "    model=model6,\n",
    "    args=training_args6,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "trainer6.train()\n",
    "model6 = model6.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b9a7ff",
   "metadata": {},
   "source": [
    " [33590/33590 31:13, Epoch 10/10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b5003",
   "metadata": {},
   "source": [
    "### Experiment 7: embeddings only, 10 epochs, BS=64, LR=5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e0e95685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,560 || all params: 438,912 || trainable%: 0.5833\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16800' max='16800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16800/16800 23:09, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.187100</td>\n",
       "      <td>0.187850</td>\n",
       "      <td>0.955005</td>\n",
       "      <td>0.555693</td>\n",
       "      <td>0.782114</td>\n",
       "      <td>0.533143</td>\n",
       "      <td>0.524345</td>\n",
       "      <td>0.591028</td>\n",
       "      <td>0.765998</td>\n",
       "      <td>0.955005</td>\n",
       "      <td>0.956279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.180100</td>\n",
       "      <td>0.185794</td>\n",
       "      <td>0.959161</td>\n",
       "      <td>0.568080</td>\n",
       "      <td>0.771518</td>\n",
       "      <td>0.546663</td>\n",
       "      <td>0.572087</td>\n",
       "      <td>0.564129</td>\n",
       "      <td>0.773324</td>\n",
       "      <td>0.959161</td>\n",
       "      <td>0.959025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.201400</td>\n",
       "      <td>0.184021</td>\n",
       "      <td>0.958303</td>\n",
       "      <td>0.572039</td>\n",
       "      <td>0.781154</td>\n",
       "      <td>0.550292</td>\n",
       "      <td>0.559308</td>\n",
       "      <td>0.585363</td>\n",
       "      <td>0.775061</td>\n",
       "      <td>0.958303</td>\n",
       "      <td>0.958753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.183557</td>\n",
       "      <td>0.959548</td>\n",
       "      <td>0.578449</td>\n",
       "      <td>0.780672</td>\n",
       "      <td>0.557224</td>\n",
       "      <td>0.573997</td>\n",
       "      <td>0.582971</td>\n",
       "      <td>0.778602</td>\n",
       "      <td>0.959548</td>\n",
       "      <td>0.959697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.166800</td>\n",
       "      <td>0.182512</td>\n",
       "      <td>0.959851</td>\n",
       "      <td>0.579647</td>\n",
       "      <td>0.780114</td>\n",
       "      <td>0.558569</td>\n",
       "      <td>0.577841</td>\n",
       "      <td>0.581464</td>\n",
       "      <td>0.779283</td>\n",
       "      <td>0.959851</td>\n",
       "      <td>0.959910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.177700</td>\n",
       "      <td>0.181824</td>\n",
       "      <td>0.959934</td>\n",
       "      <td>0.580757</td>\n",
       "      <td>0.780841</td>\n",
       "      <td>0.559723</td>\n",
       "      <td>0.578628</td>\n",
       "      <td>0.582900</td>\n",
       "      <td>0.779859</td>\n",
       "      <td>0.959934</td>\n",
       "      <td>0.960004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.169200</td>\n",
       "      <td>0.181440</td>\n",
       "      <td>0.958984</td>\n",
       "      <td>0.578705</td>\n",
       "      <td>0.784537</td>\n",
       "      <td>0.557308</td>\n",
       "      <td>0.566240</td>\n",
       "      <td>0.591732</td>\n",
       "      <td>0.778574</td>\n",
       "      <td>0.958984</td>\n",
       "      <td>0.959412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.185700</td>\n",
       "      <td>0.181081</td>\n",
       "      <td>0.959894</td>\n",
       "      <td>0.581890</td>\n",
       "      <td>0.782393</td>\n",
       "      <td>0.560845</td>\n",
       "      <td>0.577630</td>\n",
       "      <td>0.586214</td>\n",
       "      <td>0.780413</td>\n",
       "      <td>0.959894</td>\n",
       "      <td>0.960034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.172500</td>\n",
       "      <td>0.180858</td>\n",
       "      <td>0.960267</td>\n",
       "      <td>0.583430</td>\n",
       "      <td>0.781748</td>\n",
       "      <td>0.562570</td>\n",
       "      <td>0.582421</td>\n",
       "      <td>0.584443</td>\n",
       "      <td>0.781284</td>\n",
       "      <td>0.960267</td>\n",
       "      <td>0.960300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.195300</td>\n",
       "      <td>0.180785</td>\n",
       "      <td>0.959985</td>\n",
       "      <td>0.582975</td>\n",
       "      <td>0.783051</td>\n",
       "      <td>0.561979</td>\n",
       "      <td>0.578521</td>\n",
       "      <td>0.587498</td>\n",
       "      <td>0.780980</td>\n",
       "      <td>0.959985</td>\n",
       "      <td>0.960131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_only = [\"hyena.backbone.embeddings.word_embeddings\"]\n",
    "peft_config7 = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS, inference_mode=False,\n",
    "    r=16, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=embeddings_only,\n",
    ")\n",
    "model7 = get_peft_model(base_model, peft_config7)\n",
    "model7.print_trainable_parameters()\n",
    "\n",
    "training_args7 = TrainingArguments(\n",
    "    output_dir=\"./results/exp7_embeddings_only_5ep_6e-4\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs/exp7_embeddings_only_5ep_6e-4\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_safetensors=False,\n",
    "    label_names=[\"labels\"],\n",
    "\n",
    ")\n",
    "trainer7 = Trainer(\n",
    "    model=model7,\n",
    "    args=training_args7,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    "\n",
    ")\n",
    "trainer7.train()\n",
    "model7 = model7.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d451739e",
   "metadata": {},
   "source": [
    " [8400/8400 15:28, Epoch 5/5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfb86d7",
   "metadata": {},
   "source": [
    "# Interpretation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac483dcd",
   "metadata": {},
   "source": [
    "Loading best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "10691e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ad3a1b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_36532\\2237946429.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f\"{dir_best}/pytorch_model.bin\", map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: []\n",
      "Unexpected keys: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HyenaDNAForTokenClassification(\n",
       "  (hyena): HyenaDNAModel(\n",
       "    (backbone): HyenaLMBackbone(\n",
       "      (embeddings): HyenaEmbeddings(\n",
       "        (word_embeddings): Embedding(16, 256)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x HyenaBlock(\n",
       "          (mixer): HyenaOperator(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (pos_emb): HyenaPositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "                (1): HyenaSin()\n",
       "                (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (3): HyenaSin()\n",
       "                (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (5): HyenaSin()\n",
       "                (6): Linear(in_features=64, out_features=256, bias=False)\n",
       "              )\n",
       "              (modulation): HyenaExponentialModulation()\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): HyenaMlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=256, out_features=2, bias=False)\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig\n",
    "\n",
    "dir_best = \"./models/hyenadna-small-32k_len100_bs64_freeze_3_epochs_unfreeze_6_epochs_bestmodel\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(dir_best, trust_remote_code=True)\n",
    "config.num_labels = 2\n",
    "\n",
    "token_model_best = HyenaDNAForTokenClassification(config)\n",
    "\n",
    "state_dict = torch.load(f\"{dir_best}/pytorch_model.bin\", map_location=\"cpu\")\n",
    "tokenizer_best = AutoTokenizer.from_pretrained(dir_best, trust_remote_code=True)\n",
    "missing, unexpected = token_model_best.load_state_dict(state_dict, strict=False)\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "def new_build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "    cls = [self.cls_token_id]\n",
    "    sep = [self.sep_token_id]\n",
    "    result = cls + token_ids_0 + sep\n",
    "    if token_ids_1 is not None:\n",
    "        result += token_ids_1 + sep\n",
    "    return result\n",
    "tokenizer_best.build_inputs_with_special_tokens = new_build_inputs_with_special_tokens.__get__(tokenizer_best, AutoTokenizer)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "token_model_best.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2404ba0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3cd5e",
   "metadata": {},
   "source": [
    "## Interpretation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86ef8f9",
   "metadata": {},
   "source": [
    "### IG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21cde4b",
   "metadata": {},
   "source": [
    "#### Token-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce18891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35820/35820 [12:05<00:00, 49.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Token  Count  AvgScore\n",
      "0     C  40494  3.083872\n",
      "1     G  40020  2.312610\n",
      "2     T  13845 -0.078975\n",
      "3     A  12655 -1.789643\n"
     ]
    }
   ],
   "source": [
    "def make_custom_forward_tp(tp_mask):\n",
    "    def custom_forward_tp(inputs_embeds, attention_mask):\n",
    "        logits = token_model_best(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        ).logits[..., 1]  # [1, seq_len]\n",
    "        return (logits * tp_mask).sum(dim=-1)\n",
    "\n",
    "    return custom_forward_tp\n",
    "\n",
    "ig = IntegratedGradients(lambda x, y: x.sum(dim=-1))\n",
    "\n",
    "idxs = np.arange(len(test_ds)) \n",
    "total_counts = defaultdict(int)\n",
    "tp_counts = defaultdict(int)\n",
    "score_sums = defaultdict(float)\n",
    "\n",
    "# Loop and compute IG for TP tokens\n",
    "for i in trange(len(idxs)):\n",
    "    sample = test_ds[idxs[i]]\n",
    "    raw_seq = sample[\"seq\"]\n",
    "    input_ids = sample[\"input_ids\"].unsqueeze(0).to(device)  # [1, L]\n",
    "    attention_mask = sample[\"attention_mask\"].unsqueeze(0).to(device)  # [1, L]\n",
    "    labels = sample[\"labels\"].tolist()  # L\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = token_model_best(\n",
    "            inputs_embeds=token_model_best.get_input_embeddings()(input_ids),\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        logits = out.logits  # [1, L, 2]\n",
    "    preds = logits.argmax(dim=-1)[0]  # [L]\n",
    "\n",
    "    lbl_tensor = torch.tensor(labels, device=device).unsqueeze(0)  # [1, L]\n",
    "    tp_mask = ((preds == 1) & (lbl_tensor == 1)).float()  # [1, L]\n",
    "\n",
    "    if tp_mask.sum() == 0:\n",
    "        continue\n",
    "\n",
    "    inputs_embeds = token_model_best.get_input_embeddings()(input_ids)  # [1, L, D]\n",
    "    baseline = torch.zeros_like(inputs_embeds)\n",
    "\n",
    "    custom_forward = make_custom_forward_tp(tp_mask)\n",
    "\n",
    "    ig.forward_func = custom_forward\n",
    "\n",
    "    attributions, delta = ig.attribute(\n",
    "        inputs=inputs_embeds,\n",
    "        baselines=baseline,\n",
    "        additional_forward_args=(attention_mask,),\n",
    "        n_steps=50,\n",
    "        return_convergence_delta=True\n",
    "    )\n",
    "    # [1, L, D]\n",
    "\n",
    "    token_scores = attributions.sum(dim=-1)[0].detach().cpu().tolist()  # L\n",
    "\n",
    "    tokens = tokenizer_best.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    for pos, (tok, true_lbl, pred_lbl, score) in enumerate(zip(tokens, labels, preds.cpu().tolist(), token_scores)):\n",
    "        if true_lbl == pred_lbl == 1:\n",
    "            total_counts[tok] += 1\n",
    "            tp_counts[tok] += 1\n",
    "            score_sums[tok] += score\n",
    "\n",
    "records = []\n",
    "for tok, cnt in tp_counts.items():\n",
    "    records.append({\n",
    "        \"Token\": tok,\n",
    "        \"Count\": cnt,\n",
    "        \"AvgScore\": score_sums[tok] / cnt\n",
    "    })\n",
    "df_results = pd.DataFrame(records).sort_values(\"AvgScore\", ascending=False)\n",
    "\n",
    "print(df_results.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b921d73b",
   "metadata": {},
   "source": [
    "100%|██████████| 35820/35820 [12:05<00:00, 49.37it/s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df121b",
   "metadata": {},
   "source": [
    "#### K-mer, k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "11363f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35820/35820 [12:38<00:00, 47.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-20 K-mers by AvgScore:\n",
      "      kmer  Count   AvgScore\n",
      "0    CGCGC   1597  23.666333\n",
      "6    GCGCG   1622  21.965001\n",
      "106  CGTGC    604  21.688671\n",
      "61   CGCGT    427  21.604626\n",
      "36   CGTGT    532  20.109420\n",
      "120  CGCAC    844  19.500836\n",
      "35   GCGTG    763  19.419284\n",
      "747  AATGC      1  17.065805\n",
      "20   GTGCG    769  16.964307\n",
      "895  AAGAC      1  16.882907\n",
      "338  CGTAC     42  16.815580\n",
      "111  ACGCG    200  16.572558\n",
      "27   GCGCA    756  16.046912\n",
      "745  CAAAT      1  15.960244\n",
      "625  AGCGT      9  15.875856\n",
      "872  ATTTG      1  15.832479\n",
      "44   TGCGT    279  15.662055\n",
      "406  CGTAT     48  15.418951\n",
      "21   TGCGC    495  15.238591\n",
      "427  GCGTA     72  15.206563\n"
     ]
    }
   ],
   "source": [
    "def make_custom_forward_tp(tp_mask):\n",
    "    def custom_forward_tp(inputs_embeds, attention_mask):\n",
    "        logits = token_model_best(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        ).logits[..., 1]  # [1, L]\n",
    "        return (logits * tp_mask).sum(dim=-1)  # [1]\n",
    "\n",
    "    return custom_forward_tp\n",
    "\n",
    "ig = IntegratedGradients(lambda x, y: x.sum(dim=-1))\n",
    "\n",
    "K = 5\n",
    "kmer_counts = defaultdict(int)\n",
    "kmer_score_sums = defaultdict(float)\n",
    "\n",
    "for i in trange(len(test_ds)):\n",
    "    sample = test_ds[i]\n",
    "    raw_seq = sample[\"seq\"]\n",
    "    input_ids = sample[\"input_ids\"].unsqueeze(0).to(device)  # [1, L]\n",
    "    attention_mask = sample[\"attention_mask\"].unsqueeze(0).to(device)  # [1, L]\n",
    "    labels = sample[\"labels\"].tolist()  # L\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = token_model_best(\n",
    "            inputs_embeds=token_model_best\n",
    "            .get_input_embeddings()(input_ids),\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "    logits = out.logits  # [1, L, 2]\n",
    "    preds = logits.argmax(dim=-1)[0]  # [L]\n",
    "\n",
    "    lbl_tensor = torch.tensor(labels, device=device).unsqueeze(0)\n",
    "    tp_mask = ((preds == 1) & (lbl_tensor == 1)).float()  # [1, L]\n",
    "    if tp_mask.sum() == 0:\n",
    "        continue\n",
    "\n",
    "    inputs_embeds = token_model_best.get_input_embeddings()(input_ids)  # [1, L, D]\n",
    "    baseline = torch.zeros_like(inputs_embeds)\n",
    "\n",
    "    ig.forward_func = make_custom_forward_tp(tp_mask)\n",
    "\n",
    "    attributions, delta = ig.attribute(\n",
    "        inputs=inputs_embeds,\n",
    "        baselines=baseline,\n",
    "        additional_forward_args=(attention_mask,),\n",
    "        n_steps=50,\n",
    "        return_convergence_delta=True\n",
    "    )\n",
    "\n",
    "    token_scores = attributions.sum(dim=-1)[0].detach().cpu().tolist()  # L\n",
    "\n",
    "    L = len(raw_seq)\n",
    "    for start in range(0, L - K + 1):\n",
    "        window_lbls = labels[start:start + K]\n",
    "        window_preds = preds[start:start + K].cpu().tolist()\n",
    "        if all(l == 1 and p == 1 for l, p in zip(window_lbls, window_preds)):\n",
    "            kmer = raw_seq[start:start + K]\n",
    "            score = sum(token_scores[start:start + K])\n",
    "            kmer_counts[kmer] += 1\n",
    "            kmer_score_sums[kmer] += score\n",
    "\n",
    "records_k = []\n",
    "for kmer, cnt in kmer_counts.items():\n",
    "    records_k.append({\n",
    "        \"kmer\": kmer,\n",
    "        \"Count\": cnt,\n",
    "        \"AvgScore\": kmer_score_sums[kmer] / cnt\n",
    "    })\n",
    "df_kmers = pd.DataFrame(records_k).sort_values(\"AvgScore\", ascending=False)\n",
    "\n",
    "print(\"Top-20 K-mers by AvgScore:\")\n",
    "print(df_kmers.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a5f59",
   "metadata": {},
   "source": [
    "100%|██████████| 35820/35820 [12:31<00:00, 47.67it/s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a016be6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmers_top20 = df_kmers.head(20)\n",
    "df_kmers_top20.to_csv(\"top_20_kmers_IG.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8bd974",
   "metadata": {},
   "source": [
    "### SmoothGrad "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdfabb2",
   "metadata": {},
   "source": [
    "#### Token-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5bd0e18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35820/35820 [10:04<00:00, 59.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   token   count     avg_score\n",
      "1      A  937503  1.150335e-01\n",
      "0      T  939210  1.034314e-01\n",
      "3      G  852053  7.560379e-02\n",
      "2      C  853198  5.478451e-02\n",
      "4  [SEP]   35820  2.853514e-07\n",
      "5      N      36  1.332133e-07\n"
     ]
    }
   ],
   "source": [
    "def custom_forward_token(inputs_embeds, attention_mask):\n",
    "    \"\"\"\n",
    "    inputs_embeds: [B, L, D]\n",
    "    attention_mask: [B, L]\n",
    "    returns logits[:, :, 1] → [B, L]\n",
    "    \"\"\"\n",
    "    out = token_model_best(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=attention_mask,\n",
    "        return_dict=True\n",
    "    )\n",
    "    # [B, L, 2]\n",
    "    return out.logits[..., 1]\n",
    "\n",
    "sal = Saliency(custom_forward_token)\n",
    "\n",
    "token_counts = defaultdict(int)\n",
    "token_score_sums = defaultdict(float)\n",
    "\n",
    "for i in trange(len(test_ds)):\n",
    "    sample = test_ds[i]\n",
    "    seq = sample[\"seq\"]\n",
    "    input_ids = sample[\"input_ids\"].unsqueeze(0).to(device)  # [1, L]\n",
    "    attention_mask = sample[\"attention_mask\"].unsqueeze(0).to(device)  # [1, L]\n",
    "\n",
    "    inputs_embeds = token_model_best.get_input_embeddings()(input_ids)  # [1, L, D]\n",
    "\n",
    "    grads = sal.attribute(\n",
    "        inputs=inputs_embeds,\n",
    "        additional_forward_args=(attention_mask,),\n",
    "        target=1\n",
    "    )  # [1, L, D]\n",
    "    # [L]\n",
    "    token_importances = grads.abs().sum(dim=-1)[0].detach().cpu().tolist()\n",
    "\n",
    "    # input_ids → символы\n",
    "    tokens = tokenizer_best.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    for tok, score in zip(tokens, token_importances):\n",
    "        token_counts[tok] += 1\n",
    "        token_score_sums[tok] += score\n",
    "\n",
    "records = []\n",
    "for tok, cnt in token_counts.items():\n",
    "    records.append({\n",
    "        \"token\": tok,\n",
    "        \"count\": cnt,\n",
    "        \"avg_score\": token_score_sums[tok] / cnt\n",
    "    })\n",
    "df = pd.DataFrame(records).sort_values(\"avg_score\", ascending=False)\n",
    "\n",
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09acc7ef",
   "metadata": {},
   "source": [
    "100%|██████████| 35820/35820 [10:04<00:00, 59.29it/s]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea1151c",
   "metadata": {},
   "source": [
    "#### K-mers, k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fd7528c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35820/35820 [15:44<00:00, 37.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-20 5-mers by SmoothGrad AvgScore:\n",
      "      kmer  Count    AvgScore\n",
      "452  ATACG     60  540.442892\n",
      "6    GCGCG   1622  528.512274\n",
      "0    CGCGC   1597  523.728575\n",
      "453  TACGT     61  458.670171\n",
      "111  ACGCG    200  454.864555\n",
      "61   CGCGT    427  451.970907\n",
      "35   GCGTG    763  418.638043\n",
      "323  ATGTA     38  418.405754\n",
      "20   GTGCG    769  412.622652\n",
      "406  CGTAT     48  409.277096\n",
      "106  CGTGC    604  400.911653\n",
      "27   GCGCA    756  400.580428\n",
      "405  ACGTA     44  399.600466\n",
      "120  CGCAC    844  397.113080\n",
      "100  GCACG    598  394.345951\n",
      "98   ACATG    253  391.353791\n",
      "36   CGTGT    532  390.493981\n",
      "99   ATGCA    206  383.577271\n",
      "102  ACGTG    297  380.179867\n",
      "119  ACGCA    286  380.138466\n"
     ]
    }
   ],
   "source": [
    "saliency = Saliency(token_model_best)\n",
    "nt = NoiseTunnel(saliency)\n",
    "\n",
    "def make_custom_forward_tp(tp_mask):\n",
    "    def custom_forward_tp(inputs_embeds, attention_mask):\n",
    "        logits_pos = token_model_best(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        ).logits[..., 1]  # [1, L]\n",
    "        return (logits_pos * tp_mask).sum(dim=-1)\n",
    "\n",
    "    return custom_forward_tp\n",
    "\n",
    "K = 5\n",
    "\n",
    "kmer_counts = defaultdict(int)\n",
    "kmer_score_sums = defaultdict(float)\n",
    "\n",
    "for i in trange(len(test_ds)):\n",
    "    sample = test_ds[i]\n",
    "    raw_seq = sample[\"seq\"]  # L\n",
    "    input_ids = sample[\"input_ids\"].unsqueeze(0).to(device) # [1, L]\n",
    "    attention_mask = sample[\"attention_mask\"].unsqueeze(0).to(device)  # [1, L]\n",
    "    labels = sample[\"labels\"].tolist()  # L\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = token_model_best(\n",
    "            inputs_embeds=token_model_best.get_input_embeddings()(input_ids),\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "    preds = out.logits.argmax(dim=-1)[0].cpu().tolist()  # L\n",
    "\n",
    "    lbl_tensor = torch.tensor(labels, device=device).unsqueeze(0)\n",
    "    tp_mask = ((torch.tensor(preds, device=device) == 1) & (lbl_tensor == 1)).float()  # [1, L]\n",
    "    if tp_mask.sum() == 0:\n",
    "        continue\n",
    "\n",
    "    inputs_embeds = token_model_best.get_input_embeddings()(input_ids)  # [1, L, D]\n",
    "\n",
    "    saliency.forward_func = make_custom_forward_tp(tp_mask)\n",
    "\n",
    "    attributions = nt.attribute(\n",
    "        inputs=inputs_embeds,\n",
    "        nt_type='smoothgrad',  # SmoothGrad\n",
    "        nt_samples=50,  \n",
    "        nt_samples_batch_size=10,  # how many at once\n",
    "        stdevs=0.1,  # noise standard deviation\n",
    "        additional_forward_args=(attention_mask,),\n",
    "    )  # [1, L, D]\n",
    "\n",
    "    token_scores = attributions.sum(dim=-1)[0].cpu().tolist()  \n",
    "\n",
    "    L = len(raw_seq)\n",
    "    for start in range(L - K + 1):\n",
    "        window_labels = labels[start:start + K]\n",
    "        window_preds = preds[start:start + K]\n",
    "        if all(l == 1 and p == 1 for l, p in zip(window_labels, window_preds)):\n",
    "            kmer = raw_seq[start:start + K]\n",
    "            kmer_score = sum(token_scores[start:start + K])\n",
    "            kmer_counts[kmer] += 1\n",
    "            kmer_score_sums[kmer] += kmer_score\n",
    "\n",
    "records = [\n",
    "    {\"kmer\": k, \"Count\": c, \"AvgScore\": kmer_score_sums[k] / c}\n",
    "    for k, c in kmer_counts.items()\n",
    "]\n",
    "df_kmers = pd.DataFrame(records).sort_values(\"AvgScore\", ascending=False).head(20)\n",
    "\n",
    "print(\"Top-20 5-mers by SmoothGrad AvgScore:\")\n",
    "print(df_kmers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1d5f66",
   "metadata": {},
   "source": [
    "100%|██████████| 35820/35820 [15:44<00:00, 37.91it/s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cdf07d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmers.to_csv(\"top_20_kmers_smoothgrad.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb738dc8",
   "metadata": {},
   "source": [
    "### TP Rate per k-mer, k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c76ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting k-mers: 100%|██████████| 35820/35820 [05:15<00:00, 113.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      kmer  TP_count  Occurrences  Importance\n",
      "857  CGCAC       844         2895    0.291537\n",
      "731  GTGCG       769         2992    0.257019\n",
      "887  CGCGT       427         1676    0.254773\n",
      "733  GCGCA       756         3006    0.251497\n",
      "848  CGTGT       532         2140    0.248598\n",
      "313  CGCGG      1069         4390    0.243508\n",
      "642  GCGCG      1622         6925    0.234224\n",
      "365  CGCGC      1597         6897    0.231550\n",
      "423  CGTGC       604         2612    0.231240\n",
      "362  GCACG       598         2669    0.224054\n",
      "112  GTGTG      1965         8880    0.221284\n",
      "914  CACAC      1921         8716    0.220399\n",
      "422  GCGTG       763         3472    0.219758\n",
      "347  GCGCC      1150         5269    0.218258\n",
      "363  CACGC       695         3413    0.203633\n",
      "330  GCGCT       502         2494    0.201283\n",
      "868  ACGCA       286         1449    0.197378\n",
      "729  ACACG       383         1942    0.197219\n",
      "983  CGCGA       183          954    0.191824\n",
      "314  GCGGG      1037         5467    0.189684\n"
     ]
    }
   ],
   "source": [
    "K = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tp_counts = defaultdict(int)\n",
    "occ_counts = defaultdict(int)\n",
    "\n",
    "for sample in trange(len(test_ds), desc=\"Counting k-mers\"):\n",
    "    seq = test_ds[sample][\"seq\"]\n",
    "    labels = test_ds[sample][\"labels\"].tolist()\n",
    "\n",
    "    input_ids = test_ds[sample][\"input_ids\"].unsqueeze(0).to(device)\n",
    "    attention_mask = test_ds[sample][\"attention_mask\"].unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = token_model_best(\n",
    "            inputs_embeds=token_model_best.get_input_embeddings()(input_ids),\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "    preds = outputs.logits.argmax(dim=-1)[0].cpu().tolist()\n",
    "\n",
    "    L = len(seq)\n",
    "    for i in range(L - K + 1):\n",
    "        kmer = seq[i: i + K]\n",
    "        occ_counts[kmer] += 1\n",
    "\n",
    "        window_labels = labels[i: i + K]\n",
    "        window_preds = preds[i: i + K]\n",
    "        if all(l == 1 and p == 1 for l, p in zip(window_labels, window_preds)):\n",
    "            tp_counts[kmer] += 1\n",
    "\n",
    "records = []\n",
    "for kmer, total in occ_counts.items():\n",
    "    tp = tp_counts.get(kmer, 0)\n",
    "    importance = tp / total\n",
    "    records.append({\n",
    "        \"kmer\": kmer,\n",
    "        \"TP_count\": tp,\n",
    "        \"Occurrences\": total,\n",
    "        \"Importance\": importance\n",
    "    })\n",
    "\n",
    "df_kmers = pd.DataFrame(records)\n",
    "df_kmers = df_kmers.sort_values(\"Importance\", ascending=False).head(20)\n",
    "print(df_kmers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d318cc",
   "metadata": {},
   "source": [
    "Counting k-mers: 100%|██████████| 35820/35820 [05:15<00:00, 113.47it/s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2b398f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmers.to_csv(\"top_20_kmers_TP_importance.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af949634",
   "metadata": {},
   "source": [
    "## Interpets arrangment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9afe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → saved ranking_IG_vs_TP.csv\n",
      "  → saved ranking_TP_vs_SG.csv\n",
      "  → saved ranking_IG_vs_SG.csv\n"
     ]
    }
   ],
   "source": [
    "def get_ranked_features(fw: pd.DataFrame, id_col: str = \"kmer\") -> pd.DataFrame:\n",
    "    num = fw.drop(columns=[id_col])\n",
    "    means = num.mean()\n",
    "    dev = num.sub(means).div(means) * 100\n",
    "    dev[id_col] = fw[id_col]\n",
    "    dev[\"mean_dev\"] = dev.drop(columns=[id_col]).mean(axis=1)\n",
    "    return dev[[id_col, \"mean_dev\"]].sort_values(\"mean_dev\", ascending=False)\n",
    "\n",
    "\n",
    "df_tp = pd.read_csv(\"top_20_kmers_TP_importance.csv\").rename(columns={\"Importance\": \"TP_imp\"})\n",
    "df_ig = pd.read_csv(\"top_20_kmers_IG.csv\").rename(columns={\"AvgScore\": \"IG_imp\"})\n",
    "df_sg = pd.read_csv(\"top_20_kmers_smoothgrad.csv\").rename(columns={\"AvgScore\": \"SG_imp\"})\n",
    "\n",
    "pairs = [\n",
    "    (\"IG_vs_TP\", df_ig[[\"kmer\", \"IG_imp\"]], df_tp[[\"kmer\", \"TP_imp\"]]),\n",
    "    (\"TP_vs_SG\", df_tp[[\"kmer\", \"TP_imp\"]], df_sg[[\"kmer\", \"SG_imp\"]]),\n",
    "    (\"IG_vs_SG\", df_ig[[\"kmer\", \"IG_imp\"]], df_sg[[\"kmer\", \"SG_imp\"]]),\n",
    "]\n",
    "\n",
    "for name, left, right in pairs:\n",
    "    merged = left.merge(right, on=\"kmer\")\n",
    "    ranked = get_ranked_features(merged, id_col=\"kmer\")\n",
    "    out_fn = f\"ranking_{name}.csv\"\n",
    "    ranked.to_csv(out_fn, index=False)\n",
    "    print(f\"  → saved {out_fn}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
