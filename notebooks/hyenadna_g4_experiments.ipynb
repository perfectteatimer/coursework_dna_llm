{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f238530",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce373bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\1\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9380959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "sys.path.append(os.path.abspath(\"../data\"))\n",
    "\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "from joblib import load\n",
    "\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "import sparse_vector\n",
    "import sparse_vector.sparse_vector\n",
    "\n",
    "sys.modules[\"Sparse_vector\"] = sparse_vector\n",
    "sys.modules[\"Sparse_vector.sparse_vector\"] = sparse_vector.sparse_vector\n",
    "\n",
    "from modeling_hyena import HyenaDNAForTokenClassification\n",
    "\n",
    "mcc = evaluate.load(\"matthews_correlation\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "333f1d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSEMBLY_d = {}\n",
    "chroms_d = {}\n",
    "all_features_d = {}\n",
    "groups_d = {}\n",
    "feature_names_d = {}\n",
    "G4_d = {}\n",
    "black_list_d = {}\n",
    "DNA_d = {}\n",
    "DNA_features_d = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd57249",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3c44c7",
   "metadata": {},
   "source": [
    "## metadata\n",
    "\n",
    "Define the assembly version of quadruplexes from kidney tissue, target chromosomes, and feature files. Metadata structures are then populated for lookup by assembly mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac15cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroms = [f\"chr{i}\" for i in list(range(1, 23)) + [\"X\", \"Y\"]]\n",
    "G4 = load(\"../data/g4.pkl\")\n",
    "black_list = load(\"../data/blacklist_hg38_v2.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a533295",
   "metadata": {},
   "source": [
    "## DNA sequence assembly\n",
    "\n",
    "Concatenate individual chromosome fragment files into complete sequences. This section builds a full-genome DNA string for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fcce566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [00:04<00:00,  5.94it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_chrom_sequence(chrom: str) -> str:\n",
    "    base_dir = os.path.abspath(os.path.join(\"..\",\"data\" ,\"z_dna\", \"hg38_dna\"))\n",
    "    files = sorted(f for f in os.listdir(base_dir) if f.startswith(f\"{chrom}_\"))\n",
    "    return \"\".join(load(os.path.join(base_dir, f)) for f in files)\n",
    "\n",
    "\n",
    "DNA = {chrom: load_chrom_sequence(chrom) for chrom in tqdm(chroms)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64ccc03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'hg38'\n",
    "chroms_d[mode] = chroms\n",
    "G4_d[mode] = G4\n",
    "black_list_d[mode] = black_list\n",
    "DNA_d[mode] = DNA\n",
    "# DNA_features_d[mode] = DNA_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a257fa9",
   "metadata": {},
   "source": [
    "## Creating and labeling windows\n",
    "\n",
    "Generate fixed-length windows across the genome and assign labels based on Z-DNA predictions. This prepares the dataset for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b1c8dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2ff4170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2489564/2489564 [01:02<00:00, 39715.28it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2421935/2421935 [01:01<00:00, 39553.75it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1982955/1982955 [00:49<00:00, 39767.52it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1902145/1902145 [00:48<00:00, 38999.97it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1815382/1815382 [00:45<00:00, 40213.00it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1708059/1708059 [00:43<00:00, 39125.99it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1593459/1593459 [00:40<00:00, 39366.04it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1451386/1451386 [00:35<00:00, 40335.83it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1383947/1383947 [00:31<00:00, 43380.38it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1337974/1337974 [00:34<00:00, 38723.29it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1350866/1350866 [00:33<00:00, 40129.54it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1332753/1332753 [00:33<00:00, 40143.97it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1143643/1143643 [00:26<00:00, 42571.46it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1070437/1070437 [00:26<00:00, 40982.35it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1019911/1019911 [00:22<00:00, 44419.14it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 903383/903383 [00:21<00:00, 41772.80it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 832574/832574 [00:20<00:00, 40537.63it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 803732/803732 [00:19<00:00, 40982.15it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 586176/586176 [00:14<00:00, 40697.31it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 644441/644441 [00:15<00:00, 40788.52it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 467099/467099 [00:10<00:00, 44750.19it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 508184/508184 [00:12<00:00, 39644.66it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1560408/1560408 [00:38<00:00, 40845.48it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 572274/572274 [00:10<00:00, 53491.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139966\n",
      "28172318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "ints_in = []\n",
    "ints_out = []\n",
    "\n",
    "for chrm in chroms:\n",
    "    for st in trange(0, G4[chrm].shape - width, width):\n",
    "        interval = [st, min(st + width, G4[chrm].shape)]\n",
    "        N_count = sum([bp == \"N\" for bp in DNA[chrm][interval[0] : interval[1]]])\n",
    "        bl_count = black_list[chrm][interval[0] : interval[1]].sum()\n",
    "        if N_count > width / 2 or bl_count > 0:\n",
    "            continue\n",
    "        else:\n",
    "            if G4[chrm][interval[0] : interval[1]].any():\n",
    "                ints_in.append([chrm, int(interval[0]), int(interval[1]), 1])\n",
    "            else:\n",
    "                ints_out.append([chrm, int(interval[0]), int(interval[1]), 0])\n",
    "print(len(ints_in))\n",
    "print(len(ints_out))\n",
    "\n",
    "ints_in_full = ints_in\n",
    "ints_out_full = ints_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a494871e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139966\n",
      "28172318\n"
     ]
    }
   ],
   "source": [
    "print(len(ints_in_full))\n",
    "print(len(ints_out_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6e7efa",
   "metadata": {},
   "source": [
    "## Balance of classes\n",
    "\n",
    "Compute and display the class distribution to assess dataset imbalance. Helps in deciding whether to apply sampling strategies or class weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44dbdb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139966\n",
      "419898\n"
     ]
    }
   ],
   "source": [
    "ints_in = ints_in_full\n",
    "ints_out = [ints_out_full[i] for i in np.random.choice(range(len(ints_out_full)),\n",
    "                                                    size=len(ints_in) * 3, replace=False)] # 3:1 ratio\n",
    "print(len(ints_in)) \n",
    "print(len(ints_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "338966bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chr1', 827400, 827500, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ints_in[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803260e6",
   "metadata": {},
   "source": [
    "# Dateset class and division of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790aab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DNATokenClassificationDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        chroms: List[str],\n",
    "        dna_source: Dict[str, str],\n",
    "        labels_source: Dict[str, torch.Tensor],\n",
    "        intervals: List[Tuple[str, int, int]],\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        max_length: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        chroms - list of available chromosomes\n",
    "        dna_source - {chrom: dna_string}\n",
    "        labels_source - {chrom: Tensor[length_of_chrom]} with 0/1 labels by nucleotide\n",
    "        intervals - [(chrom, start, end), ...]\n",
    "        tokeniser - HyenaDNATokenizer\n",
    "        max_length - Lmax for padding/truncation\n",
    "        \"\"\"\n",
    "        self.intervals = intervals\n",
    "        self.dna_source = dna_source\n",
    "        self.labels_source = labels_source\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.intervals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        interval = self.intervals[idx]\n",
    "        chrom = interval[0]\n",
    "        start = interval[1]\n",
    "        end = interval[2]\n",
    "        seq = self.dna_source[chrom][start:end].upper()\n",
    "        char_labels = self.labels_source[chrom][start:end]  # Tensor of shape (L,)\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            seq,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length + 1,\n",
    "            return_special_tokens_mask=True,\n",
    "            return_attention_mask=True,\n",
    "            # add_special_tokens=False,\n",
    "        )\n",
    "        input_ids = torch.tensor(enc[\"input_ids\"], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(enc[\"attention_mask\"], dtype=torch.long)\n",
    "        special_tokens_mask = torch.tensor(enc[\"special_tokens_mask\"], dtype=torch.long)\n",
    "\n",
    "        # Align labels: one character per token\n",
    "        labels_by_tok = []\n",
    "        char_ptr = 0\n",
    "        for is_special in special_tokens_mask.tolist():\n",
    "            if is_special:\n",
    "                # CLS, SEP, PAD ‚Üí ignore\n",
    "                labels_by_tok.append(-100)\n",
    "            else:\n",
    "                # If there are any more character labels, take the next one\n",
    "                if char_ptr < len(char_labels):\n",
    "                    labels_by_tok.append(int(char_labels[char_ptr]))\n",
    "                    char_ptr += 1\n",
    "                else:\n",
    "                    # The original string has been truncated ‚Üí put -100\n",
    "                    labels_by_tok.append(-100)\n",
    "\n",
    "        labels_by_tok = torch.tensor(labels_by_tok, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels_by_tok,\n",
    "            \"seq\": seq,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f2430",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c6ddd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f9f316",
   "metadata": {},
   "source": [
    "I was training on the HyenaDNA with context window length 1k and 32k, there's no change in quality, 32k just has params to work with larger context which are unused in my case (since length of my seq <1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07e21ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of HyenaDNAForSequenceClassification were not initialized from the model checkpoint at LongSafari/hyenadna-tiny-1k-seqlen-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['hyena.backbone.embeddings.word_embeddings.weight', 'hyena.backbone.layers.0.mixer.in_proj.weight', 'hyena.backbone.layers.0.mixer.in_proj.bias', 'hyena.backbone.layers.0.mixer.out_proj.weight', 'hyena.backbone.layers.0.mixer.out_proj.bias', 'hyena.backbone.layers.0.mixer.short_filter.weight', 'hyena.backbone.layers.0.mixer.short_filter.bias', 'hyena.backbone.layers.0.mixer.filter_fn.bias', 'hyena.backbone.layers.0.mixer.filter_fn.pos_emb.z', 'hyena.backbone.layers.0.mixer.filter_fn.pos_emb.t', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.0.weight', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.0.bias', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.1.freq', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.2.weight', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.2.bias', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.3.freq', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.4.weight', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.4.bias', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.5.freq', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.6.weight', 'hyena.backbone.layers.0.mixer.filter_fn.modulation.deltas', 'hyena.backbone.layers.0.norm1.weight', 'hyena.backbone.layers.0.norm1.bias', 'hyena.backbone.layers.0.mlp.fc1.weight', 'hyena.backbone.layers.0.mlp.fc1.bias', 'hyena.backbone.layers.0.mlp.fc2.weight', 'hyena.backbone.layers.0.mlp.fc2.bias', 'hyena.backbone.layers.0.norm2.weight', 'hyena.backbone.layers.0.norm2.bias', 'hyena.backbone.layers.1.mixer.in_proj.weight', 'hyena.backbone.layers.1.mixer.in_proj.bias', 'hyena.backbone.layers.1.mixer.out_proj.weight', 'hyena.backbone.layers.1.mixer.out_proj.bias', 'hyena.backbone.layers.1.mixer.short_filter.weight', 'hyena.backbone.layers.1.mixer.short_filter.bias', 'hyena.backbone.layers.1.mixer.filter_fn.bias', 'hyena.backbone.layers.1.mixer.filter_fn.pos_emb.z', 'hyena.backbone.layers.1.mixer.filter_fn.pos_emb.t', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.0.weight', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.0.bias', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.1.freq', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.2.weight', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.2.bias', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.3.freq', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.4.weight', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.4.bias', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.5.freq', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.6.weight', 'hyena.backbone.layers.1.mixer.filter_fn.modulation.deltas', 'hyena.backbone.layers.1.norm1.weight', 'hyena.backbone.layers.1.norm1.bias', 'hyena.backbone.layers.1.mlp.fc1.weight', 'hyena.backbone.layers.1.mlp.fc1.bias', 'hyena.backbone.layers.1.mlp.fc2.weight', 'hyena.backbone.layers.1.mlp.fc2.bias', 'hyena.backbone.layers.1.norm2.weight', 'hyena.backbone.layers.1.norm2.bias', 'hyena.backbone.ln_f.weight', 'hyena.backbone.ln_f.bias', 'score.weight'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HyenaDNAForTokenClassification(\n",
       "  (hyena): HyenaDNAModel(\n",
       "    (backbone): HyenaLMBackbone(\n",
       "      (embeddings): HyenaEmbeddings(\n",
       "        (word_embeddings): Embedding(16, 128)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x HyenaBlock(\n",
       "          (mixer): HyenaOperator(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (in_proj): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (short_filter): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(2,), groups=384)\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (pos_emb): HyenaPositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "                (1): HyenaSin()\n",
       "                (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (3): HyenaSin()\n",
       "                (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (5): HyenaSin()\n",
       "                (6): Linear(in_features=64, out_features=128, bias=False)\n",
       "              )\n",
       "              (modulation): HyenaExponentialModulation()\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): HyenaMlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=False)\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"LongSafari/hyenadna-tiny-1k-seqlen-hf\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# SequenceClassification model to get the \"hyena\" + head score\n",
    "config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "config.num_labels = 2\n",
    "seq_model, seq_loading_info = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    trust_remote_code=True,\n",
    "    output_loading_info=True,\n",
    ")\n",
    "# print(config)\n",
    "# TokenClassificationPerToken without loading weights\n",
    "token_model = HyenaDNAForTokenClassification(config)\n",
    "\n",
    "# weights from seq_model to token_model\n",
    "seq_sd = seq_model.state_dict()\n",
    "token_sd = token_model.state_dict()\n",
    "\n",
    "# Backbone: all parameters \"hyena.\"\n",
    "for k, v in seq_sd.items():\n",
    "    if k.startswith(\"hyena.\"):\n",
    "        token_sd[k] = v.clone()\n",
    "print(seq_sd.keys())\n",
    "# rename score ‚Üí classifier\n",
    "token_sd[\"classifier.weight\"] = seq_sd[\"score.weight\"].clone()\n",
    "\n",
    "missing, unexpected = token_model.load_state_dict(token_sd, strict=False)\n",
    "\n",
    "token_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224437a3",
   "metadata": {},
   "source": [
    "Check if models have some weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7b100f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model.to(device)\n",
    "for p1, p2 in zip(seq_model.parameters(), token_model.parameters()):\n",
    "    if p1.data.ne(p2.data).sum() > 0:\n",
    "        print(\"Parameters are not equal\")\n",
    "        print(\"parameter name:\", p1.name)\n",
    "        print(\"parameter name:\", p2.name)\n",
    "for p1, p2 in zip(seq_sd.keys(), token_sd.keys()):\n",
    "    if p1 != p2 and p2 != \"classifier.weight\" and p1 != \"score.weight\":\n",
    "        print(\"Parameters are not equal\")\n",
    "        print(\"parameter name:\", p1)\n",
    "        print(\"parameter name:\", p2)\n",
    "for (name1, param1), (name2, param2) in zip(seq_sd.items(), token_sd.items()):\n",
    "    if name2 != \"classifier.weight\" and name1 != \"score.weight\":\n",
    "        if not torch.equal(param1, param2):\n",
    "            print(\"Parameters are not equal\")\n",
    "            print(\"parameter name in seq_sd:\", name1)\n",
    "            print(\"parameter name in token_sd:\", name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cd94bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['hyena.backbone.embeddings.word_embeddings.weight', 'hyena.backbone.layers.0.mixer.in_proj.weight', 'hyena.backbone.layers.0.mixer.in_proj.bias', 'hyena.backbone.layers.0.mixer.out_proj.weight', 'hyena.backbone.layers.0.mixer.out_proj.bias', 'hyena.backbone.layers.0.mixer.short_filter.weight', 'hyena.backbone.layers.0.mixer.short_filter.bias', 'hyena.backbone.layers.0.mixer.filter_fn.bias', 'hyena.backbone.layers.0.mixer.filter_fn.pos_emb.z', 'hyena.backbone.layers.0.mixer.filter_fn.pos_emb.t', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.0.weight', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.0.bias', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.1.freq', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.2.weight', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.2.bias', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.3.freq', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.4.weight', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.4.bias', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.5.freq', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.6.weight', 'hyena.backbone.layers.0.mixer.filter_fn.modulation.deltas', 'hyena.backbone.layers.0.norm1.weight', 'hyena.backbone.layers.0.norm1.bias', 'hyena.backbone.layers.0.mlp.fc1.weight', 'hyena.backbone.layers.0.mlp.fc1.bias', 'hyena.backbone.layers.0.mlp.fc2.weight', 'hyena.backbone.layers.0.mlp.fc2.bias', 'hyena.backbone.layers.0.norm2.weight', 'hyena.backbone.layers.0.norm2.bias', 'hyena.backbone.layers.1.mixer.in_proj.weight', 'hyena.backbone.layers.1.mixer.in_proj.bias', 'hyena.backbone.layers.1.mixer.out_proj.weight', 'hyena.backbone.layers.1.mixer.out_proj.bias', 'hyena.backbone.layers.1.mixer.short_filter.weight', 'hyena.backbone.layers.1.mixer.short_filter.bias', 'hyena.backbone.layers.1.mixer.filter_fn.bias', 'hyena.backbone.layers.1.mixer.filter_fn.pos_emb.z', 'hyena.backbone.layers.1.mixer.filter_fn.pos_emb.t', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.0.weight', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.0.bias', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.1.freq', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.2.weight', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.2.bias', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.3.freq', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.4.weight', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.4.bias', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.5.freq', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.6.weight', 'hyena.backbone.layers.1.mixer.filter_fn.modulation.deltas', 'hyena.backbone.layers.1.norm1.weight', 'hyena.backbone.layers.1.norm1.bias', 'hyena.backbone.layers.1.mlp.fc1.weight', 'hyena.backbone.layers.1.mlp.fc1.bias', 'hyena.backbone.layers.1.mlp.fc2.weight', 'hyena.backbone.layers.1.mlp.fc2.bias', 'hyena.backbone.layers.1.norm2.weight', 'hyena.backbone.layers.1.norm2.bias', 'hyena.backbone.ln_f.weight', 'hyena.backbone.ln_f.bias', 'score.weight'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_sd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eff790",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_intervals = ints_in + ints_out\n",
    "\n",
    "# 20 % for test\n",
    "train_val, test_intervals = train_test_split(\n",
    "    all_intervals,\n",
    "    test_size=0.2,\n",
    "    stratify=[f\"{label}_{chrom}\" for chrom, _, _, label in all_intervals],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# 80 % for train + val\n",
    "train_intervals, val_intervals = train_test_split(\n",
    "    train_val,\n",
    "    test_size=0.25,  # 0.25 –æ—Ç 80% = 20% \n",
    "    stratify=[f\"{label}_{chrom}\" for chrom, _, _, label in train_val],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "train_ds = DNATokenClassificationDataset(\n",
    "    chroms=chroms,\n",
    "    dna_source=DNA,\n",
    "    labels_source=G4,\n",
    "    intervals=[(c, s, e) for c, s, e, _ in train_intervals],\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=width,\n",
    ")\n",
    "val_ds = DNATokenClassificationDataset(\n",
    "    chroms=chroms,\n",
    "    dna_source=DNA,\n",
    "    labels_source=G4,\n",
    "    intervals=[(c, s, e) for c, s, e, _ in val_intervals],\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=width,\n",
    ")\n",
    "test_ds = DNATokenClassificationDataset(\n",
    "    chroms=chroms,\n",
    "    dna_source=DNA,\n",
    "    labels_source=G4,\n",
    "    intervals=[(c, s, e) for c, s, e, _ in test_intervals],\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=width,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27ac0c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor([ 8, 10,  9,  7,  8,  8,  8,  8, 10,  9,  8,  8, 10,  9,  7,  8,  8,  7,\n",
      "         7,  9,  8,  8,  7, 10,  9, 10,  8, 10,  9,  7,  7,  8,  7,  9,  9,  7,\n",
      "         9,  9,  8, 10,  8,  7,  7,  9,  8,  8,  8,  8,  7,  9,  9,  9,  9,  9,\n",
      "         8,  8,  9,  9,  9,  9,  9,  8, 10,  9,  8,  8,  8,  8,  8,  9,  9,  7,\n",
      "         8,  7, 10,  9,  8, 10,  9,  9,  8,  7,  9,  7,  9,  8,  7,  9,  9, 10,\n",
      "         9,  9,  7,  9,  8, 10,  9, 10,  9,  9,  1])\n",
      "tensor([   1,    1,    1,    1,    1,    1,    1,    1,    1,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    0,    0,    0,\n",
      "           0,    0,    0,    0, -100])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_ds)):\n",
    "    if sum(train_ds[i]['labels'].numpy()) != 0 and sum(train_ds[i]['labels'].numpy()) != -100:\n",
    "        print(i, train_ds[i]['input_ids'])\n",
    "        print(train_ds[i]['labels'])\n",
    "        print(train_ds[i]['attention_mask'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb318586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CGAGTGCAGGGCTGCTGCTCAGAGGCCCGCCCAGGCGCCCCGCAGGGAGATGGCCCACCACAGAGCGCCAGGGGAACTGTTCTTCCAGCGCCAGGGAACG'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]['seq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9745eccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input_ids: 101\n",
      "Length of labels: 101\n",
      "Length of attention_mask: 101\n"
     ]
    }
   ],
   "source": [
    "example = train_ds[1]\n",
    "print(\"Length of input_ids:\", len(example[\"input_ids\"]))\n",
    "print(\"Length of labels:\", len(example[\"labels\"]))\n",
    "print(\"Length of attention_mask:\", len(example[\"attention_mask\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eb865c",
   "metadata": {},
   "source": [
    "# Defining function for metrics calculations\n",
    "Evaluated using accuracy, F1 (binary, macro, micro, weighted), precision, recall, ROC AUC, and Matthews correlation.  \n",
    "In some cases did not calculate all metrics, but the most important for every case of experiments are F1 binary and MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3303b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics2(p):\n",
    "    preds = p.predictions.argmax(-1).flatten()\n",
    "    labs = p.label_ids.flatten()\n",
    "    mask = labs != -100\n",
    "    preds = preds[mask]\n",
    "    labs = labs[mask]\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labs, preds),\n",
    "        \"f1\": f1_score(labs, preds, average=\"binary\"),\n",
    "        \"roc_auc\": roc_auc_score(labs, preds),\n",
    "        \"matthews\": matthews_corrcoef(labs, preds),\n",
    "        \"precision\": precision_metric.compute(predictions=preds, references=labs)[\n",
    "            \"precision\"\n",
    "        ],\n",
    "        \"recall\": recall_metric.compute(predictions=preds, references=labs)[\"recall\"],\n",
    "        \"f1_macro\": f1_metric.compute(\n",
    "            predictions=preds, references=labs, average=\"macro\"\n",
    "        )[\"f1\"],\n",
    "        \"f1_micro\": f1_metric.compute(\n",
    "            predictions=preds, references=labs, average=\"micro\"\n",
    "        )[\"f1\"],\n",
    "        \"f1_weighted\": f1_metric.compute(\n",
    "            predictions=preds, references=labs, average=\"weighted\"\n",
    "        )[\"f1\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a7ebed",
   "metadata": {},
   "source": [
    "# Experiments for finding best params for G4 task without PEFT methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4192d845",
   "metadata": {},
   "source": [
    "## Important note before checking code below\n",
    "- Verify that your environment uses the correct versions of libraries and that file paths match your system. Otherwise, subsequent cells may error out due to missing dependencies.\n",
    "- It may look dirty, but I specifically titled experiments everywhere for better understanding, in particular, I highlighted the settings from the best training.\n",
    "- TLDR of exps below.\n",
    "- In the case of HyenaDNA, LoRa showed that this was useless since the code speed is the same (thanks to the hyena operator), but it was interesting to verify this with a few different experiments\n",
    "### Overview of Conducted Experiments\n",
    "\n",
    "I ran four main experiment groups to benchmark training strategies and parameter-efficient fine-tuning:\n",
    "\n",
    "1. **Head-only warmup (Stage 1)**\n",
    "\n",
    "   * **Frozen backbone**, train head for 3 epochs\n",
    "   * Large LR (1e-3) to quickly adapt classification layer\n",
    "\n",
    "2. **Full fine-tuning (Stage 2)**\n",
    "\n",
    "   * **Unfrozen backbone and head**, 6 epochs\n",
    "   * LR = 1e-5 for backbone, 5e-4 for head; weight decay = 0.01\n",
    "\n",
    "3. **Extended full fine-tuning**\n",
    "\n",
    "   * 12 epochs at LR = 3e-4 (and variants at 6e-4, 5e-4)\n",
    "   * Batch sizes 32‚Äì64, linear or plateau schedulers, FP16, online data augmentation\n",
    "\n",
    "4. **LoRA parameter-efficient tuning**\n",
    "\n",
    "   * Rank r=8, Œ±=32, dropout=0.1\n",
    "   * **Exp 1:** Full Hyena modules, 3 epochs\n",
    "   * **Exp 2:** Full Hyena modules, 10 epochs, linear warmup\n",
    "   * **Exp 3:** Mixer-only modules, 5 epochs\n",
    "\n",
    "Each experiment logs F1 on validation and saves the best model. \n",
    "\n",
    "#### Result\n",
    "Best result was achieved in the following case. \n",
    "\n",
    "\n",
    "*Performed standard training with batch size = 32, sequence length = 100, and 10% warmup.*\n",
    "*Used class-weighted cross-entropy loss (CE=1 and 8) with two-class weights.*  \n",
    "*Trained for 12 epochs; achieved F1 score of **0.5788**.*\n",
    "\n",
    "\n",
    "Also, it can be seen that all experiments ended with results close to each other, ~F1 0.57. This result shows that model is not ready for DNA sub-domain task - classification of G4, so these results will not be interpreted.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8edc17",
   "metadata": {},
   "source": [
    "## Experiment : CE 07 and 2 Two-Stage Fine-Tuning (F1 = 0.5779)  \n",
    "Loss weighting: CE = 0.7 and 2; model trained in two stages:\n",
    "\n",
    "- **Stage 1 (Frozen Backbone):**  \n",
    "  Trained only the classification head for 3 epochs using LR = 1e-3.  \n",
    "  All Hyena backbone layers frozen.\n",
    "\n",
    "- **Stage 2 (Full Fine-Tuning):**  \n",
    "  Unfroze the entire model and fine-tuned for 6 more epochs.  \n",
    "  Used discriminative learning rates: 1e-5 (backbone), 5e-4 (head).  \n",
    "  Achieved F1 score of **0.5779** on the evaluation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a31e85bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_31308\\666881719.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_stage1_augdata = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15747' max='15747' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15747/15747 23:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.228400</td>\n",
       "      <td>0.224746</td>\n",
       "      <td>0.914438</td>\n",
       "      <td>0.576013</td>\n",
       "      <td>0.821901</td>\n",
       "      <td>0.542513</td>\n",
       "      <td>0.483970</td>\n",
       "      <td>0.711288</td>\n",
       "      <td>0.764216</td>\n",
       "      <td>0.914438</td>\n",
       "      <td>0.921661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.222800</td>\n",
       "      <td>0.224792</td>\n",
       "      <td>0.913778</td>\n",
       "      <td>0.575643</td>\n",
       "      <td>0.823547</td>\n",
       "      <td>0.542607</td>\n",
       "      <td>0.481435</td>\n",
       "      <td>0.715689</td>\n",
       "      <td>0.763829</td>\n",
       "      <td>0.913778</td>\n",
       "      <td>0.921260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>0.224746</td>\n",
       "      <td>0.914416</td>\n",
       "      <td>0.576196</td>\n",
       "      <td>0.822218</td>\n",
       "      <td>0.542772</td>\n",
       "      <td>0.483894</td>\n",
       "      <td>0.712010</td>\n",
       "      <td>0.764299</td>\n",
       "      <td>0.914416</td>\n",
       "      <td>0.921661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_31308\\666881719.py:67: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_stage2_augdata = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31494' max='31494' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31494/31494 53:22, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.226200</td>\n",
       "      <td>0.224132</td>\n",
       "      <td>0.915699</td>\n",
       "      <td>0.577818</td>\n",
       "      <td>0.820185</td>\n",
       "      <td>0.543797</td>\n",
       "      <td>0.489024</td>\n",
       "      <td>0.706012</td>\n",
       "      <td>0.765496</td>\n",
       "      <td>0.915699</td>\n",
       "      <td>0.922503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.224558</td>\n",
       "      <td>0.914550</td>\n",
       "      <td>0.576981</td>\n",
       "      <td>0.822825</td>\n",
       "      <td>0.543683</td>\n",
       "      <td>0.484461</td>\n",
       "      <td>0.713182</td>\n",
       "      <td>0.764728</td>\n",
       "      <td>0.914550</td>\n",
       "      <td>0.921793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.222800</td>\n",
       "      <td>0.225966</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.577268</td>\n",
       "      <td>0.816302</td>\n",
       "      <td>0.542323</td>\n",
       "      <td>0.492976</td>\n",
       "      <td>0.696332</td>\n",
       "      <td>0.765523</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.923012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.224400</td>\n",
       "      <td>0.225085</td>\n",
       "      <td>0.915894</td>\n",
       "      <td>0.577363</td>\n",
       "      <td>0.818949</td>\n",
       "      <td>0.543050</td>\n",
       "      <td>0.489791</td>\n",
       "      <td>0.703068</td>\n",
       "      <td>0.765332</td>\n",
       "      <td>0.915894</td>\n",
       "      <td>0.922582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.224200</td>\n",
       "      <td>0.225170</td>\n",
       "      <td>0.915660</td>\n",
       "      <td>0.577188</td>\n",
       "      <td>0.819480</td>\n",
       "      <td>0.543013</td>\n",
       "      <td>0.488841</td>\n",
       "      <td>0.704511</td>\n",
       "      <td>0.765173</td>\n",
       "      <td>0.915660</td>\n",
       "      <td>0.922437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.225195</td>\n",
       "      <td>0.915331</td>\n",
       "      <td>0.577008</td>\n",
       "      <td>0.820317</td>\n",
       "      <td>0.543052</td>\n",
       "      <td>0.487516</td>\n",
       "      <td>0.706743</td>\n",
       "      <td>0.764983</td>\n",
       "      <td>0.915331</td>\n",
       "      <td>0.922238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=31494, training_loss=0.22400511576121457, metrics={'train_runtime': 3202.4452, 'train_samples_per_second': 629.365, 'train_steps_per_second': 9.834, 'total_flos': 530457970977792.0, 'train_loss': 0.22400511576121457, 'epoch': 6.0})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- STEP 1: Freeze-backbone, train only the head ---\n",
    "# Freeze all backbone parameters\n",
    "for param in token_model.hyena.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Check that the classifier still requires_grad=True\n",
    "for name, param in token_model.named_parameters():\n",
    "    if \"classifier\" in name:\n",
    "        assert param.requires_grad\n",
    "\n",
    "# Re-create Trainer with a large LR for the head\n",
    "training_args_stage1_augdata = TrainingArguments(\n",
    "    output_dir=\"./stage1_g4\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-3, # train the head quickly\n",
    "    per_device_train_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "\n",
    "trainer_stage1_augdata = Trainer(\n",
    "    model=token_model,\n",
    "    args=training_args_stage1_augdata,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "trainer_stage1_augdata.train()\n",
    "\n",
    "# After that, trainer_stage1.model stores the weights of the backbone (frozen)\n",
    "# and trained head (F1 best).\n",
    "\n",
    "# --- STEP 2: unfreeze-backbone and retrain the whole model ---\n",
    "# Remove freeze\n",
    "for param in token_model.hyena.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(\n",
    "    [\n",
    "      { \"params\": token_model.hyena.parameters(), \"lr\": 1e-5},\n",
    "      { \"params\": token_model.classifier.parameters(), \"lr\": 5e-4},\n",
    "    ],\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# New TrainingArguments with a lower base LR\n",
    "training_args_stage2_augdata = TrainingArguments(\n",
    "    output_dir=\"./stage2_g4\",\n",
    "    num_train_epochs=6,\n",
    "    learning_rate=5e-5, # since the backbone is now also learning\n",
    "    per_device_train_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer with custom optimiser\n",
    "trainer_stage2_augdata = Trainer(\n",
    "    model=token_model,\n",
    "    args=training_args_stage2_augdata,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None), \n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "\n",
    "trainer_stage2_augdata.can_return_loss = True\n",
    "trainer_stage2_augdata.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "97047aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='13997' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    2/13997 00:00 < 01:33, 150.34 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== –¢–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ===\n",
      "eval_loss: 0.2242\n",
      "eval_accuracy: 0.9159\n",
      "eval_f1: 0.5779\n",
      "eval_roc_auc: 0.8206\n",
      "eval_matthews: 0.5441\n",
      "eval_precision: 0.4887\n",
      "eval_recall: 0.7068\n",
      "eval_f1_macro: 0.7656\n",
      "eval_f1_micro: 0.9159\n",
      "eval_f1_weighted: 0.9227\n",
      "eval_runtime: 323.4118\n",
      "eval_samples_per_second: 346.2240\n",
      "eval_steps_per_second: 43.2790\n",
      "epoch: 6.0000\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer_stage2_augdata.evaluate(eval_dataset=test_ds)\n",
    "print(\"=== –¢–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ===\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "10cb9ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/hyenadna-small-1k_g4_len100_bs64_freeze_3_epochs_unfreeze_6_epochs_bestmodel\\\\tokenizer_config.json',\n",
       " './models/hyenadna-small-1k_g4_len100_bs64_freeze_3_epochs_unfreeze_6_epochs_bestmodel\\\\special_tokens_map.json',\n",
       " './models/hyenadna-small-1k_g4_len100_bs64_freeze_3_epochs_unfreeze_6_epochs_bestmodel\\\\added_tokens.json')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = \"./models/hyenadna-small-1k_g4_len100_bs64_freeze_3_epochs_unfreeze_6_epochs_bestmodel\"\n",
    "trainer_stage2_augdata.save_model(save_dir)     \n",
    "tokenizer.save_pretrained(save_dir)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287538b",
   "metadata": {},
   "source": [
    "## Experiment: Seq Length 100, Batch 64, CE=1 and 8, F1 = 0.5772  \n",
    "Performed standard training with batch size = 64, sequence length = 100, and 10% warmup.  \n",
    "Used class-weighted cross-entropy loss (CE=1 and 8) with two-class weights.  \n",
    "Trained for 12 epochs; achieved F1 score of **0.5772**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c48e106e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_31308\\3617298154.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='62988' max='62988' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [62988/62988 1:31:55, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.240900</td>\n",
       "      <td>0.236566</td>\n",
       "      <td>0.906686</td>\n",
       "      <td>0.565824</td>\n",
       "      <td>0.832640</td>\n",
       "      <td>0.536303</td>\n",
       "      <td>0.456452</td>\n",
       "      <td>0.744128</td>\n",
       "      <td>0.756775</td>\n",
       "      <td>0.906686</td>\n",
       "      <td>0.916520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.235300</td>\n",
       "      <td>0.235566</td>\n",
       "      <td>0.910368</td>\n",
       "      <td>0.567600</td>\n",
       "      <td>0.823633</td>\n",
       "      <td>0.535054</td>\n",
       "      <td>0.468464</td>\n",
       "      <td>0.719955</td>\n",
       "      <td>0.758801</td>\n",
       "      <td>0.910368</td>\n",
       "      <td>0.918755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.233600</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.915958</td>\n",
       "      <td>0.573499</td>\n",
       "      <td>0.813720</td>\n",
       "      <td>0.538054</td>\n",
       "      <td>0.489896</td>\n",
       "      <td>0.691510</td>\n",
       "      <td>0.763443</td>\n",
       "      <td>0.915958</td>\n",
       "      <td>0.922345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.232600</td>\n",
       "      <td>0.233451</td>\n",
       "      <td>0.907738</td>\n",
       "      <td>0.570200</td>\n",
       "      <td>0.835423</td>\n",
       "      <td>0.541235</td>\n",
       "      <td>0.460321</td>\n",
       "      <td>0.748983</td>\n",
       "      <td>0.759261</td>\n",
       "      <td>0.907738</td>\n",
       "      <td>0.917425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.231700</td>\n",
       "      <td>0.234475</td>\n",
       "      <td>0.920056</td>\n",
       "      <td>0.575371</td>\n",
       "      <td>0.802892</td>\n",
       "      <td>0.537808</td>\n",
       "      <td>0.508296</td>\n",
       "      <td>0.662839</td>\n",
       "      <td>0.765623</td>\n",
       "      <td>0.920056</td>\n",
       "      <td>0.924783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.230800</td>\n",
       "      <td>0.232428</td>\n",
       "      <td>0.916248</td>\n",
       "      <td>0.576922</td>\n",
       "      <td>0.817219</td>\n",
       "      <td>0.542203</td>\n",
       "      <td>0.491222</td>\n",
       "      <td>0.698846</td>\n",
       "      <td>0.765223</td>\n",
       "      <td>0.916248</td>\n",
       "      <td>0.922751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.230100</td>\n",
       "      <td>0.232675</td>\n",
       "      <td>0.912409</td>\n",
       "      <td>0.573416</td>\n",
       "      <td>0.824977</td>\n",
       "      <td>0.540907</td>\n",
       "      <td>0.476219</td>\n",
       "      <td>0.720465</td>\n",
       "      <td>0.762305</td>\n",
       "      <td>0.912409</td>\n",
       "      <td>0.920325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.229200</td>\n",
       "      <td>0.232428</td>\n",
       "      <td>0.915379</td>\n",
       "      <td>0.576317</td>\n",
       "      <td>0.819252</td>\n",
       "      <td>0.542118</td>\n",
       "      <td>0.487672</td>\n",
       "      <td>0.704347</td>\n",
       "      <td>0.764656</td>\n",
       "      <td>0.915379</td>\n",
       "      <td>0.922216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.228200</td>\n",
       "      <td>0.233581</td>\n",
       "      <td>0.917190</td>\n",
       "      <td>0.576511</td>\n",
       "      <td>0.813620</td>\n",
       "      <td>0.540975</td>\n",
       "      <td>0.495176</td>\n",
       "      <td>0.689817</td>\n",
       "      <td>0.765310</td>\n",
       "      <td>0.917190</td>\n",
       "      <td>0.923254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.227100</td>\n",
       "      <td>0.234361</td>\n",
       "      <td>0.914161</td>\n",
       "      <td>0.573882</td>\n",
       "      <td>0.819979</td>\n",
       "      <td>0.539969</td>\n",
       "      <td>0.482763</td>\n",
       "      <td>0.707400</td>\n",
       "      <td>0.763077</td>\n",
       "      <td>0.914161</td>\n",
       "      <td>0.921354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.225700</td>\n",
       "      <td>0.235234</td>\n",
       "      <td>0.913923</td>\n",
       "      <td>0.571700</td>\n",
       "      <td>0.817875</td>\n",
       "      <td>0.537331</td>\n",
       "      <td>0.481698</td>\n",
       "      <td>0.703063</td>\n",
       "      <td>0.761927</td>\n",
       "      <td>0.913923</td>\n",
       "      <td>0.921066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.224300</td>\n",
       "      <td>0.234451</td>\n",
       "      <td>0.916409</td>\n",
       "      <td>0.574448</td>\n",
       "      <td>0.813493</td>\n",
       "      <td>0.538927</td>\n",
       "      <td>0.491806</td>\n",
       "      <td>0.690472</td>\n",
       "      <td>0.764050</td>\n",
       "      <td>0.916409</td>\n",
       "      <td>0.922667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=62988, training_loss=0.23078379985634664, metrics={'train_runtime': 5515.0981, 'train_samples_per_second': 730.906, 'train_steps_per_second': 11.421, 'total_flos': 1060915941955584.0, 'train_loss': 0.23078379985634664, 'epoch': 12.0})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/hyenadna1k_g4_finetune_len100_batch64_onlinedatagen\",\n",
    "    label_names=[\"labels\"],\n",
    "    num_train_epochs=12,\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"reduce_lr_on_plateau\",\n",
    "    warmup_ratio=0.10,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_safetensors=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=token_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "\n",
    "trainer.can_return_loss = True\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "32963cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1750' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1750/1750 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== –¢–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ===\n",
      "eval_loss: 0.2322\n",
      "eval_accuracy: 0.9165\n",
      "eval_f1: 0.5772\n",
      "eval_roc_auc: 0.8178\n",
      "eval_matthews: 0.5427\n",
      "eval_precision: 0.4911\n",
      "eval_recall: 0.6999\n",
      "eval_f1_macro: 0.7654\n",
      "eval_f1_micro: 0.9165\n",
      "eval_f1_weighted: 0.9230\n",
      "eval_runtime: 249.1007\n",
      "eval_samples_per_second: 449.5090\n",
      "eval_steps_per_second: 7.0250\n",
      "epoch: 12.0000\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer.evaluate(eval_dataset=test_ds)\n",
    "print(\"=== –¢–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ===\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9663612",
   "metadata": {},
   "source": [
    "## Experiment: Seq Length 100, Batch 32, CE=1 and 8, F1 =0.5788\n",
    "Performed standard training with batch size = 32, sequence length = 100, and 10% warmup.  \n",
    "Used class-weighted cross-entropy loss (CE=1 and 8) with two-class weights.  \n",
    "Trained for 12 epochs; achieved F1 score of **0.5788**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dc931d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_31308\\500989467.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: romanbokhyan (romanbokhyan-hse-university). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\1\\vscode-projects\\coursework\\HyenaDNA config\\wandb\\run-20250511_104352-ki4q4r22</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/ki4q4r22' target=\"_blank\">./results/hyenadna1k_g4_finetune_len100_batch64_onlinedatagen</a></strong> to <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface' target=\"_blank\">https://wandb.ai/romanbokhyan-hse-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/ki4q4r22' target=\"_blank\">https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/ki4q4r22</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125976' max='125976' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125976/125976 1:56:07, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.251800</td>\n",
       "      <td>0.245746</td>\n",
       "      <td>0.904058</td>\n",
       "      <td>0.553074</td>\n",
       "      <td>0.823184</td>\n",
       "      <td>0.521473</td>\n",
       "      <td>0.446486</td>\n",
       "      <td>0.726510</td>\n",
       "      <td>0.749667</td>\n",
       "      <td>0.904058</td>\n",
       "      <td>0.914133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.237900</td>\n",
       "      <td>0.236336</td>\n",
       "      <td>0.905447</td>\n",
       "      <td>0.563980</td>\n",
       "      <td>0.833899</td>\n",
       "      <td>0.535066</td>\n",
       "      <td>0.452490</td>\n",
       "      <td>0.748373</td>\n",
       "      <td>0.755477</td>\n",
       "      <td>0.905447</td>\n",
       "      <td>0.915679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.235000</td>\n",
       "      <td>0.233849</td>\n",
       "      <td>0.912659</td>\n",
       "      <td>0.571039</td>\n",
       "      <td>0.821012</td>\n",
       "      <td>0.537540</td>\n",
       "      <td>0.476910</td>\n",
       "      <td>0.711463</td>\n",
       "      <td>0.761209</td>\n",
       "      <td>0.912659</td>\n",
       "      <td>0.920301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.233600</td>\n",
       "      <td>0.235474</td>\n",
       "      <td>0.908744</td>\n",
       "      <td>0.569142</td>\n",
       "      <td>0.830796</td>\n",
       "      <td>0.538722</td>\n",
       "      <td>0.463316</td>\n",
       "      <td>0.737621</td>\n",
       "      <td>0.759055</td>\n",
       "      <td>0.908744</td>\n",
       "      <td>0.917932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.232300</td>\n",
       "      <td>0.233805</td>\n",
       "      <td>0.920653</td>\n",
       "      <td>0.574863</td>\n",
       "      <td>0.800343</td>\n",
       "      <td>0.536913</td>\n",
       "      <td>0.511266</td>\n",
       "      <td>0.656531</td>\n",
       "      <td>0.765553</td>\n",
       "      <td>0.920653</td>\n",
       "      <td>0.925080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.231100</td>\n",
       "      <td>0.231246</td>\n",
       "      <td>0.911705</td>\n",
       "      <td>0.573945</td>\n",
       "      <td>0.827946</td>\n",
       "      <td>0.542278</td>\n",
       "      <td>0.473778</td>\n",
       "      <td>0.727824</td>\n",
       "      <td>0.762347</td>\n",
       "      <td>0.911705</td>\n",
       "      <td>0.919960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.230200</td>\n",
       "      <td>0.230720</td>\n",
       "      <td>0.912415</td>\n",
       "      <td>0.575271</td>\n",
       "      <td>0.827455</td>\n",
       "      <td>0.543373</td>\n",
       "      <td>0.476413</td>\n",
       "      <td>0.725898</td>\n",
       "      <td>0.763222</td>\n",
       "      <td>0.912415</td>\n",
       "      <td>0.920458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.229200</td>\n",
       "      <td>0.230132</td>\n",
       "      <td>0.915315</td>\n",
       "      <td>0.577769</td>\n",
       "      <td>0.821375</td>\n",
       "      <td>0.544054</td>\n",
       "      <td>0.487491</td>\n",
       "      <td>0.709083</td>\n",
       "      <td>0.765353</td>\n",
       "      <td>0.915315</td>\n",
       "      <td>0.922282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.228200</td>\n",
       "      <td>0.230322</td>\n",
       "      <td>0.916973</td>\n",
       "      <td>0.578766</td>\n",
       "      <td>0.817251</td>\n",
       "      <td>0.544001</td>\n",
       "      <td>0.494301</td>\n",
       "      <td>0.698048</td>\n",
       "      <td>0.766357</td>\n",
       "      <td>0.916973</td>\n",
       "      <td>0.923291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.226900</td>\n",
       "      <td>0.230461</td>\n",
       "      <td>0.915354</td>\n",
       "      <td>0.577780</td>\n",
       "      <td>0.821262</td>\n",
       "      <td>0.544035</td>\n",
       "      <td>0.487645</td>\n",
       "      <td>0.708789</td>\n",
       "      <td>0.765371</td>\n",
       "      <td>0.915354</td>\n",
       "      <td>0.922305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.225500</td>\n",
       "      <td>0.230761</td>\n",
       "      <td>0.915185</td>\n",
       "      <td>0.577809</td>\n",
       "      <td>0.821852</td>\n",
       "      <td>0.544216</td>\n",
       "      <td>0.486981</td>\n",
       "      <td>0.710285</td>\n",
       "      <td>0.765333</td>\n",
       "      <td>0.915185</td>\n",
       "      <td>0.922212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.224200</td>\n",
       "      <td>0.231439</td>\n",
       "      <td>0.916164</td>\n",
       "      <td>0.578293</td>\n",
       "      <td>0.819286</td>\n",
       "      <td>0.544031</td>\n",
       "      <td>0.490928</td>\n",
       "      <td>0.703484</td>\n",
       "      <td>0.765874</td>\n",
       "      <td>0.916164</td>\n",
       "      <td>0.922800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125976, training_loss=0.2321651761793913, metrics={'train_runtime': 6972.0694, 'train_samples_per_second': 578.166, 'train_steps_per_second': 18.069, 'total_flos': 1060915941955584.0, 'train_loss': 0.2321651761793913, 'epoch': 12.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/hyenadna1k_g4_finetune_len100_batch64_onlinedatagen\",\n",
    "    label_names=[\"labels\"],\n",
    "    num_train_epochs=12,\n",
    "    learning_rate=6e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.10,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_safetensors=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=token_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "\n",
    "trainer.can_return_loss = True\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f465fc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3500' max='3500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3500/3500 00:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== –¢–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ===\n",
      "eval_loss: 0.2304\n",
      "eval_accuracy: 0.9172\n",
      "eval_f1: 0.5788\n",
      "eval_roc_auc: 0.8177\n",
      "eval_matthews: 0.5443\n",
      "eval_precision: 0.4939\n",
      "eval_recall: 0.6988\n",
      "eval_f1_macro: 0.7664\n",
      "eval_f1_micro: 0.9172\n",
      "eval_f1_weighted: 0.9235\n",
      "eval_runtime: 261.7581\n",
      "eval_samples_per_second: 427.7730\n",
      "eval_steps_per_second: 13.3710\n",
      "epoch: 12.0000\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer.evaluate(eval_dataset=test_ds)\n",
    "print(\"=== –¢–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ===\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b4e84f",
   "metadata": {},
   "source": [
    "## Experiment: Seq Length 100, Batch 64, CE=0.7 and 2, F1 = 0.5761  \n",
    "Performed standard training with batch size = 64, sequence length = 100, and 10% warmup.  \n",
    "Used class-weighted cross-entropy loss (CE=0.7 and 2) with two-class weights.  \n",
    "Trained for 12 epochs; achieved F1 score of **0.5761**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3dfb6fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_31308\\2725842987.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='62988' max='62988' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [62988/62988 1:31:25, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.228600</td>\n",
       "      <td>0.232987</td>\n",
       "      <td>0.913603</td>\n",
       "      <td>0.574058</td>\n",
       "      <td>0.822005</td>\n",
       "      <td>0.540680</td>\n",
       "      <td>0.480658</td>\n",
       "      <td>0.712513</td>\n",
       "      <td>0.762992</td>\n",
       "      <td>0.913603</td>\n",
       "      <td>0.921050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.231500</td>\n",
       "      <td>0.234150</td>\n",
       "      <td>0.911258</td>\n",
       "      <td>0.570870</td>\n",
       "      <td>0.825223</td>\n",
       "      <td>0.538589</td>\n",
       "      <td>0.471895</td>\n",
       "      <td>0.722381</td>\n",
       "      <td>0.760691</td>\n",
       "      <td>0.911258</td>\n",
       "      <td>0.919491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.230400</td>\n",
       "      <td>0.232874</td>\n",
       "      <td>0.916301</td>\n",
       "      <td>0.576099</td>\n",
       "      <td>0.815974</td>\n",
       "      <td>0.541108</td>\n",
       "      <td>0.491413</td>\n",
       "      <td>0.696050</td>\n",
       "      <td>0.764832</td>\n",
       "      <td>0.916301</td>\n",
       "      <td>0.922723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.229100</td>\n",
       "      <td>0.233184</td>\n",
       "      <td>0.907832</td>\n",
       "      <td>0.570572</td>\n",
       "      <td>0.835644</td>\n",
       "      <td>0.541648</td>\n",
       "      <td>0.460665</td>\n",
       "      <td>0.749354</td>\n",
       "      <td>0.759474</td>\n",
       "      <td>0.907832</td>\n",
       "      <td>0.917505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.235524</td>\n",
       "      <td>0.918647</td>\n",
       "      <td>0.575368</td>\n",
       "      <td>0.807446</td>\n",
       "      <td>0.538587</td>\n",
       "      <td>0.501630</td>\n",
       "      <td>0.674522</td>\n",
       "      <td>0.765191</td>\n",
       "      <td>0.918647</td>\n",
       "      <td>0.923992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.235654</td>\n",
       "      <td>0.916092</td>\n",
       "      <td>0.574257</td>\n",
       "      <td>0.814264</td>\n",
       "      <td>0.538917</td>\n",
       "      <td>0.490482</td>\n",
       "      <td>0.692544</td>\n",
       "      <td>0.763858</td>\n",
       "      <td>0.916092</td>\n",
       "      <td>0.922475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.222400</td>\n",
       "      <td>0.235506</td>\n",
       "      <td>0.913508</td>\n",
       "      <td>0.572743</td>\n",
       "      <td>0.820565</td>\n",
       "      <td>0.539038</td>\n",
       "      <td>0.480202</td>\n",
       "      <td>0.709466</td>\n",
       "      <td>0.762313</td>\n",
       "      <td>0.913508</td>\n",
       "      <td>0.920904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.219200</td>\n",
       "      <td>0.239553</td>\n",
       "      <td>0.912980</td>\n",
       "      <td>0.570117</td>\n",
       "      <td>0.818785</td>\n",
       "      <td>0.536058</td>\n",
       "      <td>0.478012</td>\n",
       "      <td>0.706189</td>\n",
       "      <td>0.760854</td>\n",
       "      <td>0.912980</td>\n",
       "      <td>0.920419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>0.244790</td>\n",
       "      <td>0.913875</td>\n",
       "      <td>0.566908</td>\n",
       "      <td>0.811824</td>\n",
       "      <td>0.531201</td>\n",
       "      <td>0.481164</td>\n",
       "      <td>0.689837</td>\n",
       "      <td>0.759546</td>\n",
       "      <td>0.913875</td>\n",
       "      <td>0.920702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.211400</td>\n",
       "      <td>0.250650</td>\n",
       "      <td>0.912201</td>\n",
       "      <td>0.563657</td>\n",
       "      <td>0.812809</td>\n",
       "      <td>0.528309</td>\n",
       "      <td>0.474532</td>\n",
       "      <td>0.694002</td>\n",
       "      <td>0.757423</td>\n",
       "      <td>0.912201</td>\n",
       "      <td>0.919524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.207200</td>\n",
       "      <td>0.256527</td>\n",
       "      <td>0.911938</td>\n",
       "      <td>0.561336</td>\n",
       "      <td>0.810637</td>\n",
       "      <td>0.525525</td>\n",
       "      <td>0.473327</td>\n",
       "      <td>0.689548</td>\n",
       "      <td>0.756196</td>\n",
       "      <td>0.911938</td>\n",
       "      <td>0.919212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.203400</td>\n",
       "      <td>0.265199</td>\n",
       "      <td>0.912242</td>\n",
       "      <td>0.558470</td>\n",
       "      <td>0.806101</td>\n",
       "      <td>0.521646</td>\n",
       "      <td>0.474169</td>\n",
       "      <td>0.679227</td>\n",
       "      <td>0.754874</td>\n",
       "      <td>0.912242</td>\n",
       "      <td>0.919182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=62988, training_loss=0.22091344958253456, metrics={'train_runtime': 5485.1892, 'train_samples_per_second': 734.891, 'train_steps_per_second': 11.483, 'total_flos': 1060915941955584.0, 'train_loss': 0.22091344958253456, 'epoch': 12.0})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/hyenadna1k_g4_finetune_len100_batch64_onlinedatagen\",            \n",
    "    label_names=[\"labels\"],\n",
    "    num_train_epochs=12,                                 \n",
    "    learning_rate=5e-4,                                  \n",
    "    per_device_train_batch_size=64,                     \n",
    "    per_device_eval_batch_size=64,                     \n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,                                   \n",
    "    lr_scheduler_type=\"linear\",                          \n",
    "    warmup_ratio=0.10,                                    \n",
    "    fp16=True,                                           \n",
    "\n",
    "    save_total_limit=3,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = token_model, \n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,   \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "\n",
    "trainer.can_return_loss = True\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dc1f1450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1750' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1750/1750 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== –¢–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ===\n",
      "eval_loss: 0.2327\n",
      "eval_accuracy: 0.9164\n",
      "eval_f1: 0.5761\n",
      "eval_roc_auc: 0.8166\n",
      "eval_matthews: 0.5414\n",
      "eval_precision: 0.4908\n",
      "eval_recall: 0.6974\n",
      "eval_f1_macro: 0.7649\n",
      "eval_f1_micro: 0.9164\n",
      "eval_f1_weighted: 0.9229\n",
      "eval_runtime: 248.5710\n",
      "eval_samples_per_second: 450.4670\n",
      "eval_steps_per_second: 7.0400\n",
      "epoch: 12.0000\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer.evaluate(eval_dataset=test_ds)\n",
    "print(\"=== –¢–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ===\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5f9c7228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/hyenadna-small-1k-g4_len100_bs64_lr5e-4_12epochs_linear_adamw_07_2weights_best2\\\\tokenizer_config.json',\n",
       " './models/hyenadna-small-1k-g4_len100_bs64_lr5e-4_12epochs_linear_adamw_07_2weights_best2\\\\special_tokens_map.json',\n",
       " './models/hyenadna-small-1k-g4_len100_bs64_lr5e-4_12epochs_linear_adamw_07_2weights_best2\\\\added_tokens.json')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = \"./models/hyenadna-small-1k-g4_len100_bs64_lr5e-4_12epochs_linear_adamw_07_2weights_best2\"\n",
    "\n",
    "trainer.save_model(save_dir)      \n",
    "tokenizer.save_pretrained(save_dir)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "339ad482",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_target_modules = [\n",
    "    # Mixer layers\n",
    "    \"hyena.backbone.layers.0.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.0.mixer.out_proj\",\n",
    "    \"hyena.backbone.layers.1.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.1.mixer.out_proj\",\n",
    "    \"hyena.backbone.layers.2.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.2.mixer.out_proj\",\n",
    "    \"hyena.backbone.layers.3.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.3.mixer.out_proj\",\n",
    "    \n",
    "    # MLP (FeedforwardNetwork) layers\n",
    "    \"hyena.backbone.layers.0.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.0.mlp.fc2\",\n",
    "    \"hyena.backbone.layers.1.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.1.mlp.fc2\",\n",
    "    \"hyena.backbone.layers.2.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.2.mlp.fc2\",\n",
    "    \"hyena.backbone.layers.3.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.3.mlp.fc2\",\n",
    "]\n",
    "mixer_only = [\n",
    "    \"hyena.backbone.layers.0.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.0.mixer.out_proj\",\n",
    "    \"hyena.backbone.layers.1.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.1.mixer.out_proj\",\n",
    "    \"hyena.backbone.layers.2.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.2.mixer.out_proj\",\n",
    "    \"hyena.backbone.layers.3.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.3.mixer.out_proj\",\n",
    "]\n",
    "ffn_only = [\n",
    "    \"hyena.backbone.layers.0.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.0.mlp.fc2\",\n",
    "    \"hyena.backbone.layers.1.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.1.mlp.fc2\",\n",
    "    \"hyena.backbone.layers.2.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.2.mlp.fc2\",\n",
    "    \"hyena.backbone.layers.3.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.3.mlp.fc2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685f9445",
   "metadata": {},
   "source": [
    "## LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48a805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_model = token_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86a4041",
   "metadata": {},
   "source": [
    "### Experiment 1: full modules, 3 epochs, BS=64, LR=2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63100960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,024 || all params: 469,376 || trainable%: 7.0357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: romanbokhyan (romanbokhyan-hse-university). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\1\\vscode-projects\\coursework\\HyenaDNA config\\wandb\\run-20250514_140541-lpqp5nbs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/lpqp5nbs' target=\"_blank\">./results/exp1_full_3ep_6e-4</a></strong> to <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface' target=\"_blank\">https://wandb.ai/romanbokhyan-hse-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/lpqp5nbs' target=\"_blank\">https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/lpqp5nbs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15747' max='15747' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15747/15747 22:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.254500</td>\n",
       "      <td>0.236807</td>\n",
       "      <td>0.907718</td>\n",
       "      <td>0.565709</td>\n",
       "      <td>0.829297</td>\n",
       "      <td>0.535078</td>\n",
       "      <td>0.459587</td>\n",
       "      <td>0.735556</td>\n",
       "      <td>0.757042</td>\n",
       "      <td>0.907718</td>\n",
       "      <td>0.917106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.219600</td>\n",
       "      <td>0.233340</td>\n",
       "      <td>0.912256</td>\n",
       "      <td>0.572262</td>\n",
       "      <td>0.823919</td>\n",
       "      <td>0.539515</td>\n",
       "      <td>0.475562</td>\n",
       "      <td>0.718325</td>\n",
       "      <td>0.761688</td>\n",
       "      <td>0.912256</td>\n",
       "      <td>0.920157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.235200</td>\n",
       "      <td>0.232391</td>\n",
       "      <td>0.913054</td>\n",
       "      <td>0.574202</td>\n",
       "      <td>0.823958</td>\n",
       "      <td>0.541357</td>\n",
       "      <td>0.478632</td>\n",
       "      <td>0.717457</td>\n",
       "      <td>0.762893</td>\n",
       "      <td>0.913054</td>\n",
       "      <td>0.920747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_config1 = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS, inference_mode=False,\n",
    "    r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=full_target_modules,\n",
    ")\n",
    "model1 = get_peft_model(base_model, peft_config1)\n",
    "model1.print_trainable_parameters()\n",
    "\n",
    "training_args1 = TrainingArguments(\n",
    "    output_dir=\"./results/exp1_full_3ep_6e-4\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs/exp1_full_3ep_6e-4\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "trainer1 = Trainer(\n",
    "    model=model1,\n",
    "    args=training_args1,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "trainer1.train()\n",
    "model1 = model1.unload()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b9a76e",
   "metadata": {},
   "source": [
    " [15747/15747 22:07, Epoch 3/3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8532bf",
   "metadata": {},
   "source": [
    "### Experiment 2: full modules, 10 epochs, BS=64, LR=5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "407f20a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,024 || all params: 469,376 || trainable%: 7.0357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='52490' max='52490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [52490/52490 1:13:39, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.258300</td>\n",
       "      <td>0.239716</td>\n",
       "      <td>0.900766</td>\n",
       "      <td>0.557883</td>\n",
       "      <td>0.839478</td>\n",
       "      <td>0.531672</td>\n",
       "      <td>0.438621</td>\n",
       "      <td>0.766218</td>\n",
       "      <td>0.750997</td>\n",
       "      <td>0.900766</td>\n",
       "      <td>0.912552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.220500</td>\n",
       "      <td>0.235335</td>\n",
       "      <td>0.907475</td>\n",
       "      <td>0.566469</td>\n",
       "      <td>0.831087</td>\n",
       "      <td>0.536367</td>\n",
       "      <td>0.458951</td>\n",
       "      <td>0.739777</td>\n",
       "      <td>0.757340</td>\n",
       "      <td>0.907475</td>\n",
       "      <td>0.917018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.238700</td>\n",
       "      <td>0.234458</td>\n",
       "      <td>0.913472</td>\n",
       "      <td>0.571816</td>\n",
       "      <td>0.819458</td>\n",
       "      <td>0.537858</td>\n",
       "      <td>0.479994</td>\n",
       "      <td>0.707079</td>\n",
       "      <td>0.761845</td>\n",
       "      <td>0.913472</td>\n",
       "      <td>0.920818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.250700</td>\n",
       "      <td>0.233895</td>\n",
       "      <td>0.907192</td>\n",
       "      <td>0.568583</td>\n",
       "      <td>0.834889</td>\n",
       "      <td>0.539582</td>\n",
       "      <td>0.458413</td>\n",
       "      <td>0.748461</td>\n",
       "      <td>0.758293</td>\n",
       "      <td>0.907192</td>\n",
       "      <td>0.917000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.193600</td>\n",
       "      <td>0.234363</td>\n",
       "      <td>0.919804</td>\n",
       "      <td>0.575430</td>\n",
       "      <td>0.803782</td>\n",
       "      <td>0.538010</td>\n",
       "      <td>0.507069</td>\n",
       "      <td>0.665095</td>\n",
       "      <td>0.765575</td>\n",
       "      <td>0.919804</td>\n",
       "      <td>0.924646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.242200</td>\n",
       "      <td>0.232800</td>\n",
       "      <td>0.916466</td>\n",
       "      <td>0.575508</td>\n",
       "      <td>0.814674</td>\n",
       "      <td>0.540231</td>\n",
       "      <td>0.492081</td>\n",
       "      <td>0.692998</td>\n",
       "      <td>0.764591</td>\n",
       "      <td>0.916466</td>\n",
       "      <td>0.922774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.236300</td>\n",
       "      <td>0.232368</td>\n",
       "      <td>0.913214</td>\n",
       "      <td>0.573854</td>\n",
       "      <td>0.822979</td>\n",
       "      <td>0.540754</td>\n",
       "      <td>0.479194</td>\n",
       "      <td>0.715117</td>\n",
       "      <td>0.762771</td>\n",
       "      <td>0.913214</td>\n",
       "      <td>0.920814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.215600</td>\n",
       "      <td>0.231802</td>\n",
       "      <td>0.913676</td>\n",
       "      <td>0.575912</td>\n",
       "      <td>0.824238</td>\n",
       "      <td>0.543054</td>\n",
       "      <td>0.481071</td>\n",
       "      <td>0.717330</td>\n",
       "      <td>0.763930</td>\n",
       "      <td>0.913676</td>\n",
       "      <td>0.921221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>0.231412</td>\n",
       "      <td>0.915003</td>\n",
       "      <td>0.576942</td>\n",
       "      <td>0.821297</td>\n",
       "      <td>0.543241</td>\n",
       "      <td>0.486219</td>\n",
       "      <td>0.709285</td>\n",
       "      <td>0.764849</td>\n",
       "      <td>0.915003</td>\n",
       "      <td>0.922047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.230900</td>\n",
       "      <td>0.231346</td>\n",
       "      <td>0.914013</td>\n",
       "      <td>0.576672</td>\n",
       "      <td>0.824161</td>\n",
       "      <td>0.543753</td>\n",
       "      <td>0.482393</td>\n",
       "      <td>0.716757</td>\n",
       "      <td>0.764409</td>\n",
       "      <td>0.914013</td>\n",
       "      <td>0.921466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_config2 = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS, inference_mode=False,\n",
    "    r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=full_target_modules,\n",
    ")\n",
    "model2 = get_peft_model(base_model, peft_config2)\n",
    "model2.print_trainable_parameters()\n",
    "\n",
    "training_args2 = TrainingArguments(\n",
    "    output_dir=\"./results/exp2_full_10ep_6e-4\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs/exp2_full_10ep_6e-4\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.10,\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "trainer2 = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args2,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "trainer2.train()\n",
    "model2 = model2.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018242c3",
   "metadata": {},
   "source": [
    " [52490/52490 1:13:39, Epoch 10/10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a85819c",
   "metadata": {},
   "source": [
    "### Experiment 3: mixer only, 10 epochs, BS=64, LR=5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a104d105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,544 || all params: 448,896 || trainable%: 2.7944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='52490' max='52490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [52490/52490 1:12:04, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Matthews</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>0.237459</td>\n",
       "      <td>0.905700</td>\n",
       "      <td>0.563645</td>\n",
       "      <td>0.832662</td>\n",
       "      <td>0.534331</td>\n",
       "      <td>0.453167</td>\n",
       "      <td>0.745356</td>\n",
       "      <td>0.755392</td>\n",
       "      <td>0.905700</td>\n",
       "      <td>0.915802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.221700</td>\n",
       "      <td>0.235106</td>\n",
       "      <td>0.909120</td>\n",
       "      <td>0.568025</td>\n",
       "      <td>0.828097</td>\n",
       "      <td>0.536824</td>\n",
       "      <td>0.464372</td>\n",
       "      <td>0.731246</td>\n",
       "      <td>0.758622</td>\n",
       "      <td>0.909120</td>\n",
       "      <td>0.918070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.237600</td>\n",
       "      <td>0.234449</td>\n",
       "      <td>0.915005</td>\n",
       "      <td>0.572213</td>\n",
       "      <td>0.815100</td>\n",
       "      <td>0.537131</td>\n",
       "      <td>0.485967</td>\n",
       "      <td>0.695678</td>\n",
       "      <td>0.762514</td>\n",
       "      <td>0.915005</td>\n",
       "      <td>0.921716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.250600</td>\n",
       "      <td>0.234470</td>\n",
       "      <td>0.908200</td>\n",
       "      <td>0.568622</td>\n",
       "      <td>0.831787</td>\n",
       "      <td>0.538570</td>\n",
       "      <td>0.461524</td>\n",
       "      <td>0.740446</td>\n",
       "      <td>0.758628</td>\n",
       "      <td>0.908200</td>\n",
       "      <td>0.917583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.193600</td>\n",
       "      <td>0.235796</td>\n",
       "      <td>0.921273</td>\n",
       "      <td>0.573034</td>\n",
       "      <td>0.796131</td>\n",
       "      <td>0.534487</td>\n",
       "      <td>0.514534</td>\n",
       "      <td>0.646542</td>\n",
       "      <td>0.764836</td>\n",
       "      <td>0.921273</td>\n",
       "      <td>0.925294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.243300</td>\n",
       "      <td>0.233094</td>\n",
       "      <td>0.916692</td>\n",
       "      <td>0.575392</td>\n",
       "      <td>0.813795</td>\n",
       "      <td>0.539919</td>\n",
       "      <td>0.493027</td>\n",
       "      <td>0.690798</td>\n",
       "      <td>0.764604</td>\n",
       "      <td>0.916692</td>\n",
       "      <td>0.922894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.238100</td>\n",
       "      <td>0.232779</td>\n",
       "      <td>0.913406</td>\n",
       "      <td>0.574049</td>\n",
       "      <td>0.822625</td>\n",
       "      <td>0.540841</td>\n",
       "      <td>0.479920</td>\n",
       "      <td>0.714111</td>\n",
       "      <td>0.762926</td>\n",
       "      <td>0.913406</td>\n",
       "      <td>0.920937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.217300</td>\n",
       "      <td>0.232290</td>\n",
       "      <td>0.913449</td>\n",
       "      <td>0.574895</td>\n",
       "      <td>0.823614</td>\n",
       "      <td>0.541917</td>\n",
       "      <td>0.480147</td>\n",
       "      <td>0.716230</td>\n",
       "      <td>0.763357</td>\n",
       "      <td>0.913449</td>\n",
       "      <td>0.921020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.251700</td>\n",
       "      <td>0.232074</td>\n",
       "      <td>0.915041</td>\n",
       "      <td>0.576150</td>\n",
       "      <td>0.820127</td>\n",
       "      <td>0.542179</td>\n",
       "      <td>0.486326</td>\n",
       "      <td>0.706671</td>\n",
       "      <td>0.764469</td>\n",
       "      <td>0.915041</td>\n",
       "      <td>0.922013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>0.231933</td>\n",
       "      <td>0.913689</td>\n",
       "      <td>0.575553</td>\n",
       "      <td>0.823717</td>\n",
       "      <td>0.542569</td>\n",
       "      <td>0.481092</td>\n",
       "      <td>0.716170</td>\n",
       "      <td>0.763756</td>\n",
       "      <td>0.913689</td>\n",
       "      <td>0.921203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_config3 = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS, inference_mode=False,\n",
    "    r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=mixer_only,\n",
    ")\n",
    "model3 = get_peft_model(base_model, peft_config3)\n",
    "model3.print_trainable_parameters()\n",
    "\n",
    "training_args3 = TrainingArguments(\n",
    "    output_dir=\"./results/exp3_mixer_only_5ep_6e-4\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs/exp3_mixer_only_5ep_6e-4\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "trainer3 = Trainer(\n",
    "    model=model3,\n",
    "    args=training_args3,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "trainer3.train()\n",
    "model3 = model3.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6bd666",
   "metadata": {},
   "source": [
    " [52490/52490 1:12:04, Epoch 10/10]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
