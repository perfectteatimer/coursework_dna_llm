{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import cuda\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    PreTrainedTokenizer\n",
    ")\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from captum.attr import IntegratedGradients, NoiseTunnel\n",
    "import evaluate\n",
    "mcc = evaluate.load(\"matthews_correlation\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "import inspect\n",
    "from transformers_interpret import SequenceClassificationExplainer\n",
    "from collections import defaultdict\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "from modeling_hyena import HyenaDNAForSequenceClassification\n",
    "\n",
    "from collections import defaultdict\n",
    "from captum.attr import IntegratedGradients, Saliency, NoiseTunnel\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits: dict_keys(['train', 'test', 'dev'])\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"leannmlindsey/GUE\", name=\"prom_core_all\")\n",
    "print(\"Available splits:\", dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in each split:\n",
      "train: 47356 samples, average sequence length: 70.00\n",
      "test: 5920 samples, average sequence length: 70.00\n",
      "dev: 5920 samples, average sequence length: 70.00\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of samples in each split:\")\n",
    "for split, dataset_split in dataset.items():\n",
    "    sequences = dataset_split['sequence']\n",
    "    avg_length = sum(len(seq) for seq in sequences) / len(sequences)\n",
    "    print(f\"{split}: {len(dataset_split)} samples, average sequence length: {avg_length:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of HyenaDNAForSequenceClassification were not initialized from the model checkpoint at LongSafari/hyenadna-tiny-1k-seqlen-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"LongSafari/hyenadna-tiny-1k-seqlen-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, trust_remote_code=True, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HyenaDNAForSequenceClassification(\n",
       "  (hyena): HyenaDNAModel(\n",
       "    (backbone): HyenaLMBackbone(\n",
       "      (embeddings): HyenaEmbeddings(\n",
       "        (word_embeddings): Embedding(16, 128)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x HyenaBlock(\n",
       "          (mixer): HyenaOperator(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (in_proj): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (short_filter): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(2,), groups=384)\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (pos_emb): HyenaPositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "                (1): HyenaSin()\n",
       "                (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (3): HyenaSin()\n",
       "                (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (5): HyenaSin()\n",
       "                (6): Linear(in_features=64, out_features=128, bias=False)\n",
       "              )\n",
       "              (modulation): HyenaExponentialModulation()\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): HyenaMlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (score): Linear(in_features=128, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['sequence'], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd39bf90277d49b0bc6bf8bfae62e85c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/47356 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29061071461940cfa82638014d4c4ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5920 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56647bf932b34057a68995bcdf01ed4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5920 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "train_dataset = encoded_dataset[\"train\"]\n",
    "test_dataset = encoded_dataset[\"test\"]\n",
    "val_dataset = encoded_dataset[\"dev\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    logits = p.predictions\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    labels = p.label_ids\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds),\n",
    "        \"recall\": recall_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds),\n",
    "        \"mcc\": matthews_corrcoef(labels, preds),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1 -- best model F1 = 0.82\n",
    "epochs 10, lr 6e-4, lr scheduler linear, bs64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_25484\\1065170849.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7400' max='7400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7400/7400 03:17, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.474400</td>\n",
       "      <td>0.449067</td>\n",
       "      <td>0.786824</td>\n",
       "      <td>0.843263</td>\n",
       "      <td>0.707956</td>\n",
       "      <td>0.769708</td>\n",
       "      <td>0.581692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.425700</td>\n",
       "      <td>0.428839</td>\n",
       "      <td>0.803041</td>\n",
       "      <td>0.820431</td>\n",
       "      <td>0.779121</td>\n",
       "      <td>0.799242</td>\n",
       "      <td>0.606972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.405400</td>\n",
       "      <td>0.419246</td>\n",
       "      <td>0.809122</td>\n",
       "      <td>0.787023</td>\n",
       "      <td>0.850957</td>\n",
       "      <td>0.817742</td>\n",
       "      <td>0.620105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.388400</td>\n",
       "      <td>0.422016</td>\n",
       "      <td>0.805068</td>\n",
       "      <td>0.792562</td>\n",
       "      <td>0.829809</td>\n",
       "      <td>0.810758</td>\n",
       "      <td>0.610685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.370700</td>\n",
       "      <td>0.419422</td>\n",
       "      <td>0.810642</td>\n",
       "      <td>0.810702</td>\n",
       "      <td>0.813696</td>\n",
       "      <td>0.812196</td>\n",
       "      <td>0.621263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.349800</td>\n",
       "      <td>0.418055</td>\n",
       "      <td>0.813514</td>\n",
       "      <td>0.799234</td>\n",
       "      <td>0.840551</td>\n",
       "      <td>0.819372</td>\n",
       "      <td>0.627738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.318500</td>\n",
       "      <td>0.434708</td>\n",
       "      <td>0.809628</td>\n",
       "      <td>0.802614</td>\n",
       "      <td>0.824438</td>\n",
       "      <td>0.813380</td>\n",
       "      <td>0.619406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.277000</td>\n",
       "      <td>0.477532</td>\n",
       "      <td>0.800507</td>\n",
       "      <td>0.787772</td>\n",
       "      <td>0.826116</td>\n",
       "      <td>0.806489</td>\n",
       "      <td>0.601594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.228000</td>\n",
       "      <td>0.536310</td>\n",
       "      <td>0.798311</td>\n",
       "      <td>0.787070</td>\n",
       "      <td>0.821417</td>\n",
       "      <td>0.803876</td>\n",
       "      <td>0.597068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.177200</td>\n",
       "      <td>0.610527</td>\n",
       "      <td>0.793243</td>\n",
       "      <td>0.794364</td>\n",
       "      <td>0.794898</td>\n",
       "      <td>0.794631</td>\n",
       "      <td>0.586468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7400, training_loss=0.341518507261534, metrics={'train_runtime': 197.8185, 'train_samples_per_second': 2393.911, 'train_steps_per_second': 37.408, 'total_flos': 87614994954240.0, 'train_loss': 0.341518507261534, 'epoch': 10.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/hyenadna-tiny-1k-seqlen-promoter\",\n",
    "    label_names=[\"labels\"],\n",
    "    num_train_epochs=10,                \n",
    "    learning_rate=6e-4,                  \n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",         \n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_safetensors=False,         \n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model, \n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,   \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.can_return_loss = True\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr6e-4-linear-bs64\\\\tokenizer_config.json',\n",
       " './results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr6e-4-linear-bs64\\\\special_tokens_map.json',\n",
       " './results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr6e-4-linear-bs64\\\\added_tokens.json')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\n",
    "    \"./results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr6e-4-linear-bs64\",\n",
    "    safe_serialization=False\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained(\n",
    "    \"./results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr6e-4-linear-bs64\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [7400/7400 03:19, Epoch 10/10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test metrics ===\n",
      "eval_loss: 0.4013\n",
      "eval_accuracy: 0.8167\n",
      "eval_precision: 0.8024\n",
      "eval_recall: 0.8416\n",
      "eval_f1: 0.8216\n",
      "eval_mcc: 0.6342\n",
      "eval_runtime: 0.8965\n",
      "eval_samples_per_second: 6603.3390\n",
      "eval_steps_per_second: 103.7350\n",
      "epoch: 10.0000\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(\"=== Test metrics ===\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2\n",
    "epochs 10, lr 5e-4, lr scheduler linear, bs32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_25484\\1916912596.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer2 = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7400' max='7400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7400/7400 03:17, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.323800</td>\n",
       "      <td>0.447361</td>\n",
       "      <td>0.797635</td>\n",
       "      <td>0.803201</td>\n",
       "      <td>0.791876</td>\n",
       "      <td>0.797498</td>\n",
       "      <td>0.595350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.348900</td>\n",
       "      <td>0.432583</td>\n",
       "      <td>0.804561</td>\n",
       "      <td>0.822592</td>\n",
       "      <td>0.779792</td>\n",
       "      <td>0.800620</td>\n",
       "      <td>0.610073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.326500</td>\n",
       "      <td>0.454681</td>\n",
       "      <td>0.797466</td>\n",
       "      <td>0.770352</td>\n",
       "      <td>0.851292</td>\n",
       "      <td>0.808802</td>\n",
       "      <td>0.597998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.292100</td>\n",
       "      <td>0.470336</td>\n",
       "      <td>0.796115</td>\n",
       "      <td>0.801361</td>\n",
       "      <td>0.790869</td>\n",
       "      <td>0.796080</td>\n",
       "      <td>0.592299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.251000</td>\n",
       "      <td>0.531072</td>\n",
       "      <td>0.803209</td>\n",
       "      <td>0.796988</td>\n",
       "      <td>0.817053</td>\n",
       "      <td>0.806895</td>\n",
       "      <td>0.606533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.205200</td>\n",
       "      <td>0.563590</td>\n",
       "      <td>0.796284</td>\n",
       "      <td>0.803284</td>\n",
       "      <td>0.788184</td>\n",
       "      <td>0.795662</td>\n",
       "      <td>0.592706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.151800</td>\n",
       "      <td>0.655575</td>\n",
       "      <td>0.793243</td>\n",
       "      <td>0.785738</td>\n",
       "      <td>0.810003</td>\n",
       "      <td>0.797686</td>\n",
       "      <td>0.586671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.106800</td>\n",
       "      <td>0.768097</td>\n",
       "      <td>0.791723</td>\n",
       "      <td>0.791000</td>\n",
       "      <td>0.796576</td>\n",
       "      <td>0.793778</td>\n",
       "      <td>0.583424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.067900</td>\n",
       "      <td>0.936919</td>\n",
       "      <td>0.785811</td>\n",
       "      <td>0.786216</td>\n",
       "      <td>0.788855</td>\n",
       "      <td>0.787534</td>\n",
       "      <td>0.571598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>1.109110</td>\n",
       "      <td>0.785473</td>\n",
       "      <td>0.788196</td>\n",
       "      <td>0.784491</td>\n",
       "      <td>0.786339</td>\n",
       "      <td>0.570948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7400, training_loss=0.21118232933250633, metrics={'train_runtime': 197.7669, 'train_samples_per_second': 2394.536, 'train_steps_per_second': 37.418, 'total_flos': 87614994954240.0, 'train_loss': 0.21118232933250633, 'epoch': 10.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args2 = TrainingArguments(\n",
    "    output_dir=\"./results/hyenadna-tiny-1k-seqlen-promoter2\",\n",
    "    label_names=[\"labels\"],\n",
    "    num_train_epochs=10,                \n",
    "    learning_rate=5e-4,                  \n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",         \n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_safetensors=False,         \n",
    ")\n",
    "\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model = model, \n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,   \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer2.can_return_loss = True\n",
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr5e-4-linear-bs32\\\\tokenizer_config.json',\n",
       " './results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr5e-4-linear-bs32\\\\special_tokens_map.json',\n",
       " './results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr5e-4-linear-bs32\\\\added_tokens.json')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\n",
    "    \"./results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr5e-4-linear-bs32\",\n",
    "    safe_serialization=False\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained(\n",
    "    \"./results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr5e-4-linear-bs32\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [7400/7400 03:23, Epoch 10/10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test metrics ===\n",
      "eval_loss: 0.4429\n",
      "eval_accuracy: 0.8027\n",
      "eval_precision: 0.7793\n",
      "eval_recall: 0.8460\n",
      "eval_f1: 0.8113\n",
      "eval_mcc: 0.6076\n",
      "eval_runtime: 1.3285\n",
      "eval_samples_per_second: 4456.0200\n",
      "eval_steps_per_second: 70.0020\n",
      "epoch: 10.0000\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer2.evaluate(eval_dataset=test_dataset)\n",
    "print(\"=== Test metrics ===\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 3\n",
    "epochs 10, lr 2e-5, lr scheduler cosine, bs64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_25484\\2167424115.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer3 = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7400' max='7400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7400/7400 03:19, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.266200</td>\n",
       "      <td>0.475093</td>\n",
       "      <td>0.800338</td>\n",
       "      <td>0.810577</td>\n",
       "      <td>0.787177</td>\n",
       "      <td>0.798706</td>\n",
       "      <td>0.600987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.291300</td>\n",
       "      <td>0.460298</td>\n",
       "      <td>0.794932</td>\n",
       "      <td>0.802330</td>\n",
       "      <td>0.786170</td>\n",
       "      <td>0.794168</td>\n",
       "      <td>0.590023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.518748</td>\n",
       "      <td>0.784966</td>\n",
       "      <td>0.763434</td>\n",
       "      <td>0.829809</td>\n",
       "      <td>0.795239</td>\n",
       "      <td>0.571869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.227800</td>\n",
       "      <td>0.556730</td>\n",
       "      <td>0.786993</td>\n",
       "      <td>0.768605</td>\n",
       "      <td>0.825109</td>\n",
       "      <td>0.795856</td>\n",
       "      <td>0.575345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.181200</td>\n",
       "      <td>0.631780</td>\n",
       "      <td>0.791385</td>\n",
       "      <td>0.790473</td>\n",
       "      <td>0.796576</td>\n",
       "      <td>0.793513</td>\n",
       "      <td>0.582750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.139600</td>\n",
       "      <td>0.694781</td>\n",
       "      <td>0.788345</td>\n",
       "      <td>0.790768</td>\n",
       "      <td>0.787848</td>\n",
       "      <td>0.789306</td>\n",
       "      <td>0.576686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.095700</td>\n",
       "      <td>0.844527</td>\n",
       "      <td>0.783108</td>\n",
       "      <td>0.779242</td>\n",
       "      <td>0.793891</td>\n",
       "      <td>0.786498</td>\n",
       "      <td>0.566247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.064900</td>\n",
       "      <td>1.023687</td>\n",
       "      <td>0.779561</td>\n",
       "      <td>0.776603</td>\n",
       "      <td>0.788855</td>\n",
       "      <td>0.782681</td>\n",
       "      <td>0.559129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>1.185608</td>\n",
       "      <td>0.784122</td>\n",
       "      <td>0.783973</td>\n",
       "      <td>0.788184</td>\n",
       "      <td>0.786073</td>\n",
       "      <td>0.568219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>1.288606</td>\n",
       "      <td>0.785473</td>\n",
       "      <td>0.792337</td>\n",
       "      <td>0.777442</td>\n",
       "      <td>0.784819</td>\n",
       "      <td>0.571083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7400, training_loss=0.15828348649514687, metrics={'train_runtime': 199.8985, 'train_samples_per_second': 2369.002, 'train_steps_per_second': 37.019, 'total_flos': 87614994954240.0, 'train_loss': 0.15828348649514687, 'epoch': 10.0})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args3 = TrainingArguments(\n",
    "    output_dir=\"./results/hyenadna-tiny-1k-seqlen-promoter3\",\n",
    "    label_names=[\"labels\"],\n",
    "    num_train_epochs=10,                \n",
    "    learning_rate=2e-5,                  \n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",         \n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_safetensors=False,         \n",
    ")\n",
    "\n",
    "\n",
    "trainer3 = Trainer(\n",
    "    model = model, \n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,   \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer3.can_return_loss = True\n",
    "trainer3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr2e-5-cosine-bs32\\\\tokenizer_config.json',\n",
       " './results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr2e-5-cosine-bs32\\\\special_tokens_map.json',\n",
       " './results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr2e-5-cosine-bs32\\\\added_tokens.json')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\n",
    "    \"./results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr2e-5-cosine-bs32\",\n",
    "    safe_serialization=False\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained(\n",
    "    \"./results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr2e-5-cosine-bs32\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [7400/7400 03:23, Epoch 10/10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test metrics ===\n",
      "eval_loss: 0.4830\n",
      "eval_accuracy: 0.7944\n",
      "eval_precision: 0.8084\n",
      "eval_recall: 0.7732\n",
      "eval_f1: 0.7904\n",
      "eval_mcc: 0.5895\n",
      "eval_runtime: 0.9936\n",
      "eval_samples_per_second: 5958.2460\n",
      "eval_steps_per_second: 93.6010\n",
      "epoch: 10.0000\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer3.evaluate(eval_dataset=test_dataset)\n",
    "print(\"=== Test metrics ===\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 4\n",
    "epochs 10, lr 5e-4, lr scheduler linear, bs8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_25484\\3606503099.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer4 = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7400' max='7400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7400/7400 03:18, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.239000</td>\n",
       "      <td>0.515105</td>\n",
       "      <td>0.783446</td>\n",
       "      <td>0.804231</td>\n",
       "      <td>0.752937</td>\n",
       "      <td>0.777739</td>\n",
       "      <td>0.568223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.283300</td>\n",
       "      <td>0.471888</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.811957</td>\n",
       "      <td>0.784156</td>\n",
       "      <td>0.797814</td>\n",
       "      <td>0.600428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.251300</td>\n",
       "      <td>0.514518</td>\n",
       "      <td>0.785642</td>\n",
       "      <td>0.771947</td>\n",
       "      <td>0.814703</td>\n",
       "      <td>0.792749</td>\n",
       "      <td>0.572003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.214200</td>\n",
       "      <td>0.585591</td>\n",
       "      <td>0.786993</td>\n",
       "      <td>0.780719</td>\n",
       "      <td>0.801947</td>\n",
       "      <td>0.791191</td>\n",
       "      <td>0.574109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.168400</td>\n",
       "      <td>0.657293</td>\n",
       "      <td>0.790709</td>\n",
       "      <td>0.787129</td>\n",
       "      <td>0.800604</td>\n",
       "      <td>0.793809</td>\n",
       "      <td>0.581442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.128500</td>\n",
       "      <td>0.713919</td>\n",
       "      <td>0.788514</td>\n",
       "      <td>0.801186</td>\n",
       "      <td>0.771064</td>\n",
       "      <td>0.785836</td>\n",
       "      <td>0.577526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>0.865254</td>\n",
       "      <td>0.782264</td>\n",
       "      <td>0.778327</td>\n",
       "      <td>0.793219</td>\n",
       "      <td>0.785702</td>\n",
       "      <td>0.564560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>1.026394</td>\n",
       "      <td>0.784966</td>\n",
       "      <td>0.780223</td>\n",
       "      <td>0.797247</td>\n",
       "      <td>0.788644</td>\n",
       "      <td>0.569992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.031300</td>\n",
       "      <td>1.185943</td>\n",
       "      <td>0.783277</td>\n",
       "      <td>0.788239</td>\n",
       "      <td>0.778449</td>\n",
       "      <td>0.783314</td>\n",
       "      <td>0.566615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>1.285922</td>\n",
       "      <td>0.786318</td>\n",
       "      <td>0.789723</td>\n",
       "      <td>0.784156</td>\n",
       "      <td>0.786929</td>\n",
       "      <td>0.572651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7400, training_loss=0.14758749446353397, metrics={'train_runtime': 198.4493, 'train_samples_per_second': 2386.303, 'train_steps_per_second': 37.289, 'total_flos': 87614994954240.0, 'train_loss': 0.14758749446353397, 'epoch': 10.0})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args3 = TrainingArguments(\n",
    "    output_dir=\"./results/hyenadna-tiny-1k-seqlen-promoter4\",\n",
    "    label_names=[\"labels\"],\n",
    "    num_train_epochs=10,                \n",
    "    learning_rate=5e-4,                  \n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",         \n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_safetensors=False,         \n",
    ")\n",
    "\n",
    "\n",
    "trainer3 = Trainer(\n",
    "    model = model, \n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,   \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer3.can_return_loss = True\n",
    "trainer3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr5e-4-linear-bs8\\\\tokenizer_config.json',\n",
       " './results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr5e-4-linear-bs8\\\\special_tokens_map.json',\n",
       " './results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr5e-4-linear-bs8\\\\added_tokens.json')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\n",
    "    \"./results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr5e-4-linear-bs8\",\n",
    "    safe_serialization=False\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained(\n",
    "    \"./results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr5e-4-linear-bs8\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [7400/7400 03:19, Epoch 10/10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93/93 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test metrics ===\n",
      "eval_loss: 0.4613\n",
      "eval_accuracy: 0.8008\n",
      "eval_precision: 0.8153\n",
      "eval_recall: 0.7793\n",
      "eval_f1: 0.7969\n",
      "eval_mcc: 0.6023\n",
      "eval_runtime: 0.9632\n",
      "eval_samples_per_second: 6146.1090\n",
      "eval_steps_per_second: 96.5520\n",
      "epoch: 10.0000\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer3.evaluate(eval_dataset=test_dataset)\n",
    "print(\"=== Test metrics ===\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Some weights of HyenaDNAForSequenceClassification were not initialized from the model checkpoint at LongSafari/hyenadna-tiny-1k-seqlen-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HyenaDNAForSequenceClassification(\n",
       "  (hyena): HyenaDNAModel(\n",
       "    (backbone): HyenaLMBackbone(\n",
       "      (embeddings): HyenaEmbeddings(\n",
       "        (word_embeddings): Embedding(16, 128)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x HyenaBlock(\n",
       "          (mixer): HyenaOperator(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (in_proj): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (short_filter): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(2,), groups=384)\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (pos_emb): HyenaPositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "                (1): HyenaSin()\n",
       "                (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (3): HyenaSin()\n",
       "                (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (5): HyenaSin()\n",
       "                (6): Linear(in_features=64, out_features=128, bias=False)\n",
       "              )\n",
       "              (modulation): HyenaExponentialModulation()\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): HyenaMlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (score): Linear(in_features=128, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = HyenaDNAForSequenceClassification.from_pretrained(\n",
    "    \"LongSafari/hyenadna-tiny-1k-seqlen-hf\",\n",
    "    trust_remote_code=True,\n",
    "    num_labels=2,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"LongSafari/hyenadna-tiny-1k-seqlen-hf\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyena.backbone.embeddings.word_embeddings.weight: torch.Size([16, 128])\n",
      "hyena.backbone.layers.0.mixer.in_proj.weight: torch.Size([384, 128])\n",
      "hyena.backbone.layers.0.mixer.in_proj.bias: torch.Size([384])\n",
      "hyena.backbone.layers.0.mixer.out_proj.weight: torch.Size([128, 128])\n",
      "hyena.backbone.layers.0.mixer.out_proj.bias: torch.Size([128])\n",
      "hyena.backbone.layers.0.mixer.short_filter.weight: torch.Size([384, 1, 3])\n",
      "hyena.backbone.layers.0.mixer.short_filter.bias: torch.Size([384])\n",
      "hyena.backbone.layers.0.mixer.filter_fn.bias: torch.Size([128])\n",
      "hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.0.weight: torch.Size([64, 5])\n",
      "hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.0.bias: torch.Size([64])\n",
      "hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.1.freq: torch.Size([1, 64])\n",
      "hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.2.weight: torch.Size([64, 64])\n",
      "hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.2.bias: torch.Size([64])\n",
      "hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.4.weight: torch.Size([64, 64])\n",
      "hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.4.bias: torch.Size([64])\n",
      "hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.6.weight: torch.Size([128, 64])\n",
      "hyena.backbone.layers.0.norm1.weight: torch.Size([128])\n",
      "hyena.backbone.layers.0.norm1.bias: torch.Size([128])\n",
      "hyena.backbone.layers.0.mlp.fc1.weight: torch.Size([512, 128])\n",
      "hyena.backbone.layers.0.mlp.fc1.bias: torch.Size([512])\n",
      "hyena.backbone.layers.0.mlp.fc2.weight: torch.Size([128, 512])\n",
      "hyena.backbone.layers.0.mlp.fc2.bias: torch.Size([128])\n",
      "hyena.backbone.layers.0.norm2.weight: torch.Size([128])\n",
      "hyena.backbone.layers.0.norm2.bias: torch.Size([128])\n",
      "hyena.backbone.layers.1.mixer.in_proj.weight: torch.Size([384, 128])\n",
      "hyena.backbone.layers.1.mixer.in_proj.bias: torch.Size([384])\n",
      "hyena.backbone.layers.1.mixer.out_proj.weight: torch.Size([128, 128])\n",
      "hyena.backbone.layers.1.mixer.out_proj.bias: torch.Size([128])\n",
      "hyena.backbone.layers.1.mixer.short_filter.weight: torch.Size([384, 1, 3])\n",
      "hyena.backbone.layers.1.mixer.short_filter.bias: torch.Size([384])\n",
      "hyena.backbone.layers.1.mixer.filter_fn.bias: torch.Size([128])\n",
      "hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.0.weight: torch.Size([64, 5])\n",
      "hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.0.bias: torch.Size([64])\n",
      "hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.1.freq: torch.Size([1, 64])\n",
      "hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.2.weight: torch.Size([64, 64])\n",
      "hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.2.bias: torch.Size([64])\n",
      "hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.4.weight: torch.Size([64, 64])\n",
      "hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.4.bias: torch.Size([64])\n",
      "hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.6.weight: torch.Size([128, 64])\n",
      "hyena.backbone.layers.1.norm1.weight: torch.Size([128])\n",
      "hyena.backbone.layers.1.norm1.bias: torch.Size([128])\n",
      "hyena.backbone.layers.1.mlp.fc1.weight: torch.Size([512, 128])\n",
      "hyena.backbone.layers.1.mlp.fc1.bias: torch.Size([512])\n",
      "hyena.backbone.layers.1.mlp.fc2.weight: torch.Size([128, 512])\n",
      "hyena.backbone.layers.1.mlp.fc2.bias: torch.Size([128])\n",
      "hyena.backbone.layers.1.norm2.weight: torch.Size([128])\n",
      "hyena.backbone.layers.1.norm2.bias: torch.Size([128])\n",
      "hyena.backbone.ln_f.weight: torch.Size([128])\n",
      "hyena.backbone.ln_f.bias: torch.Size([128])\n",
      "score.weight: torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "for name, param in base_model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HyenaDNAForSequenceClassification(\n",
       "  (hyena): HyenaDNAModel(\n",
       "    (backbone): HyenaLMBackbone(\n",
       "      (embeddings): HyenaEmbeddings(\n",
       "        (word_embeddings): Embedding(16, 128)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x HyenaBlock(\n",
       "          (mixer): HyenaOperator(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (in_proj): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (short_filter): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(2,), groups=384)\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (pos_emb): HyenaPositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "                (1): HyenaSin()\n",
       "                (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (3): HyenaSin()\n",
       "                (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (5): HyenaSin()\n",
       "                (6): Linear(in_features=64, out_features=128, bias=False)\n",
       "              )\n",
       "              (modulation): HyenaExponentialModulation()\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): HyenaMlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (score): Linear(in_features=128, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_target_modules = [\n",
    "    # Mixer layers\n",
    "    \"hyena.backbone.layers.0.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.0.mixer.out_proj\",\n",
    "    \"hyena.backbone.layers.1.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.1.mixer.out_proj\",\n",
    "    \"hyena.backbone.layers.2.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.2.mixer.out_proj\",\n",
    "    \"hyena.backbone.layers.3.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.3.mixer.out_proj\",\n",
    "    \n",
    "    # MLP (FeedforwardNetwork) layers\n",
    "    \"hyena.backbone.layers.0.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.0.mlp.fc2\",\n",
    "    \"hyena.backbone.layers.1.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.1.mlp.fc2\",\n",
    "    \"hyena.backbone.layers.2.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.2.mlp.fc2\",\n",
    "    \"hyena.backbone.layers.3.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.3.mlp.fc2\",\n",
    "]\n",
    "mixer_only = [\n",
    "    \"hyena.backbone.layers.0.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.0.mixer.out_proj\",\n",
    "    \"hyena.backbone.layers.1.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.1.mixer.out_proj\",\n",
    "    \"hyena.backbone.layers.2.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.2.mixer.out_proj\",\n",
    "    \"hyena.backbone.layers.3.mixer.in_proj\",\n",
    "    \"hyena.backbone.layers.3.mixer.out_proj\",\n",
    "]\n",
    "ffn_only = [\n",
    "    \"hyena.backbone.layers.0.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.0.mlp.fc2\",\n",
    "    \"hyena.backbone.layers.1.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.1.mlp.fc2\",\n",
    "    \"hyena.backbone.layers.2.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.2.mlp.fc2\",\n",
    "    \"hyena.backbone.layers.3.mlp.fc1\",\n",
    "    \"hyena.backbone.layers.3.mlp.fc2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import TrainingArguments, Trainer\n",
    "base_model = base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: full modules, 10 epochs, BS=64, LR=5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,024 || all params: 469,376 || trainable%: 7.0357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: romanbokhyan (romanbokhyan-hse-university). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\1\\vscode-projects\\coursework\\HyenaDNA config\\wandb\\run-20250507_214204-5o3sti0k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/5o3sti0k' target=\"_blank\">./results/exp1_full_10ep_2e-5</a></strong> to <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface' target=\"_blank\">https://wandb.ai/romanbokhyan-hse-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/5o3sti0k' target=\"_blank\">https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/5o3sti0k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7400' max='7400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7400/7400 02:58, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.453200</td>\n",
       "      <td>0.430749</td>\n",
       "      <td>0.801182</td>\n",
       "      <td>0.831982</td>\n",
       "      <td>0.757972</td>\n",
       "      <td>0.793255</td>\n",
       "      <td>0.605004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.404300</td>\n",
       "      <td>0.423460</td>\n",
       "      <td>0.798649</td>\n",
       "      <td>0.840640</td>\n",
       "      <td>0.740181</td>\n",
       "      <td>0.787219</td>\n",
       "      <td>0.601954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.391600</td>\n",
       "      <td>0.416961</td>\n",
       "      <td>0.809797</td>\n",
       "      <td>0.786399</td>\n",
       "      <td>0.853978</td>\n",
       "      <td>0.818796</td>\n",
       "      <td>0.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.380700</td>\n",
       "      <td>0.426959</td>\n",
       "      <td>0.807601</td>\n",
       "      <td>0.804434</td>\n",
       "      <td>0.816046</td>\n",
       "      <td>0.810198</td>\n",
       "      <td>0.615216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.349300</td>\n",
       "      <td>0.426677</td>\n",
       "      <td>0.809459</td>\n",
       "      <td>0.802946</td>\n",
       "      <td>0.823431</td>\n",
       "      <td>0.813059</td>\n",
       "      <td>0.619045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.406700</td>\n",
       "      <td>0.423571</td>\n",
       "      <td>0.809966</td>\n",
       "      <td>0.805135</td>\n",
       "      <td>0.821081</td>\n",
       "      <td>0.813030</td>\n",
       "      <td>0.619991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.342000</td>\n",
       "      <td>0.430819</td>\n",
       "      <td>0.814696</td>\n",
       "      <td>0.822702</td>\n",
       "      <td>0.805304</td>\n",
       "      <td>0.813910</td>\n",
       "      <td>0.629570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.307400</td>\n",
       "      <td>0.442434</td>\n",
       "      <td>0.808108</td>\n",
       "      <td>0.804830</td>\n",
       "      <td>0.816717</td>\n",
       "      <td>0.810730</td>\n",
       "      <td>0.616232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.341500</td>\n",
       "      <td>0.446895</td>\n",
       "      <td>0.805405</td>\n",
       "      <td>0.807679</td>\n",
       "      <td>0.804968</td>\n",
       "      <td>0.806321</td>\n",
       "      <td>0.610807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.296700</td>\n",
       "      <td>0.462842</td>\n",
       "      <td>0.806250</td>\n",
       "      <td>0.811353</td>\n",
       "      <td>0.801276</td>\n",
       "      <td>0.806283</td>\n",
       "      <td>0.612563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_config1 = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, inference_mode=False,\n",
    "    r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=full_target_modules,\n",
    ")\n",
    "model1 = get_peft_model(base_model, peft_config1)\n",
    "model1.print_trainable_parameters()\n",
    "\n",
    "training_args1 = TrainingArguments(\n",
    "    output_dir=\"./results/exp1_full_10ep_5e-4\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs/exp1_full_10ep_5e-4\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "trainer1 = Trainer(\n",
    "    model=model1,\n",
    "    args=training_args1,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer1.train()\n",
    "model1 = model1.unload()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [7400/7400 02:58, Epoch 10/10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: mixer only, 10 epochs, BS=64, LR=5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,544 || all params: 448,896 || trainable%: 2.7944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: romanbokhyan (romanbokhyan-hse-university). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\1\\vscode-projects\\coursework\\HyenaDNA config\\wandb\\run-20250507_214646-900ib1zw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/900ib1zw' target=\"_blank\">./results/exp3_mixer_only_10ep_5e-4</a></strong> to <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface' target=\"_blank\">https://wandb.ai/romanbokhyan-hse-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/900ib1zw' target=\"_blank\">https://wandb.ai/romanbokhyan-hse-university/huggingface/runs/900ib1zw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7400' max='7400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7400/7400 02:37, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.459400</td>\n",
       "      <td>0.434438</td>\n",
       "      <td>0.802703</td>\n",
       "      <td>0.824436</td>\n",
       "      <td>0.772407</td>\n",
       "      <td>0.797574</td>\n",
       "      <td>0.606774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.412000</td>\n",
       "      <td>0.422418</td>\n",
       "      <td>0.806081</td>\n",
       "      <td>0.812991</td>\n",
       "      <td>0.798254</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.612293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.395500</td>\n",
       "      <td>0.421325</td>\n",
       "      <td>0.805912</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.839543</td>\n",
       "      <td>0.813201</td>\n",
       "      <td>0.612951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.410100</td>\n",
       "      <td>0.420770</td>\n",
       "      <td>0.808953</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.801276</td>\n",
       "      <td>0.808467</td>\n",
       "      <td>0.618033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.360900</td>\n",
       "      <td>0.423496</td>\n",
       "      <td>0.808108</td>\n",
       "      <td>0.796207</td>\n",
       "      <td>0.831487</td>\n",
       "      <td>0.813465</td>\n",
       "      <td>0.616704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.423800</td>\n",
       "      <td>0.417598</td>\n",
       "      <td>0.814696</td>\n",
       "      <td>0.814085</td>\n",
       "      <td>0.818731</td>\n",
       "      <td>0.816402</td>\n",
       "      <td>0.629373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.364500</td>\n",
       "      <td>0.424635</td>\n",
       "      <td>0.813851</td>\n",
       "      <td>0.816740</td>\n",
       "      <td>0.812353</td>\n",
       "      <td>0.814541</td>\n",
       "      <td>0.627709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.365300</td>\n",
       "      <td>0.424576</td>\n",
       "      <td>0.813007</td>\n",
       "      <td>0.809319</td>\n",
       "      <td>0.822088</td>\n",
       "      <td>0.815654</td>\n",
       "      <td>0.626039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.369100</td>\n",
       "      <td>0.426708</td>\n",
       "      <td>0.813851</td>\n",
       "      <td>0.819980</td>\n",
       "      <td>0.807318</td>\n",
       "      <td>0.813599</td>\n",
       "      <td>0.627800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.338800</td>\n",
       "      <td>0.428368</td>\n",
       "      <td>0.812838</td>\n",
       "      <td>0.819174</td>\n",
       "      <td>0.805975</td>\n",
       "      <td>0.812521</td>\n",
       "      <td>0.625781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_config2 = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, inference_mode=False,\n",
    "    r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=mixer_only,\n",
    ")\n",
    "model2 = get_peft_model(base_model, peft_config2)\n",
    "model2.print_trainable_parameters()\n",
    "\n",
    "training_args2 = TrainingArguments(\n",
    "    output_dir=\"./results/exp3_mixer_only_10ep_5e-4\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs/exp3_mixer_only_10ep_5e-4\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "trainer2 = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args2,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer2.train()\n",
    "model2 = model2.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [7400/7400 02:37, Epoch 10/10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: ffn only, 10 epochs, BS=64, LR=5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,736 || all params: 457,088 || trainable%: 4.5365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7400' max='7400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7400/7400 02:28, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.458300</td>\n",
       "      <td>0.447005</td>\n",
       "      <td>0.790541</td>\n",
       "      <td>0.816989</td>\n",
       "      <td>0.752266</td>\n",
       "      <td>0.783293</td>\n",
       "      <td>0.583133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.410700</td>\n",
       "      <td>0.434286</td>\n",
       "      <td>0.797804</td>\n",
       "      <td>0.808946</td>\n",
       "      <td>0.783149</td>\n",
       "      <td>0.795838</td>\n",
       "      <td>0.595982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.409600</td>\n",
       "      <td>0.433834</td>\n",
       "      <td>0.801858</td>\n",
       "      <td>0.779567</td>\n",
       "      <td>0.845250</td>\n",
       "      <td>0.811081</td>\n",
       "      <td>0.605668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.408700</td>\n",
       "      <td>0.433757</td>\n",
       "      <td>0.806250</td>\n",
       "      <td>0.813699</td>\n",
       "      <td>0.797583</td>\n",
       "      <td>0.805560</td>\n",
       "      <td>0.612655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.368900</td>\n",
       "      <td>0.435935</td>\n",
       "      <td>0.808446</td>\n",
       "      <td>0.803754</td>\n",
       "      <td>0.819402</td>\n",
       "      <td>0.811503</td>\n",
       "      <td>0.616946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.438900</td>\n",
       "      <td>0.432795</td>\n",
       "      <td>0.808108</td>\n",
       "      <td>0.805031</td>\n",
       "      <td>0.816381</td>\n",
       "      <td>0.810667</td>\n",
       "      <td>0.616227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>0.433374</td>\n",
       "      <td>0.810135</td>\n",
       "      <td>0.810512</td>\n",
       "      <td>0.812689</td>\n",
       "      <td>0.811599</td>\n",
       "      <td>0.620250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.436383</td>\n",
       "      <td>0.806081</td>\n",
       "      <td>0.800657</td>\n",
       "      <td>0.818395</td>\n",
       "      <td>0.809429</td>\n",
       "      <td>0.612243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.395200</td>\n",
       "      <td>0.434857</td>\n",
       "      <td>0.809966</td>\n",
       "      <td>0.814450</td>\n",
       "      <td>0.805975</td>\n",
       "      <td>0.810191</td>\n",
       "      <td>0.619976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.348200</td>\n",
       "      <td>0.437095</td>\n",
       "      <td>0.810135</td>\n",
       "      <td>0.815584</td>\n",
       "      <td>0.804632</td>\n",
       "      <td>0.810071</td>\n",
       "      <td>0.620344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_config3 = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, inference_mode=False,\n",
    "    r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=ffn_only,\n",
    ")\n",
    "model3 = get_peft_model(base_model, peft_config3)\n",
    "model3.print_trainable_parameters()\n",
    "\n",
    "training_args3 = TrainingArguments(\n",
    "    output_dir=\"./results/exp4_ffn_only_5ep_6e-4\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs/exp4_ffn_only_5ep_6e-4\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_safetensors=False\n",
    "\n",
    ")\n",
    "trainer3 = Trainer(\n",
    "    model=model3,\n",
    "    args=training_args3,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer3.train()\n",
    "model3 = model3.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [7400/7400 02:28, Epoch 10/10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: short_filter only, 10 epochs, BS=64, LR=5e-4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 24,832 || all params: 461,184 || trainable%: 5.3844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7400' max='7400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7400/7400 02:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.455400</td>\n",
       "      <td>0.430216</td>\n",
       "      <td>0.798142</td>\n",
       "      <td>0.821094</td>\n",
       "      <td>0.765693</td>\n",
       "      <td>0.792427</td>\n",
       "      <td>0.597822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.412400</td>\n",
       "      <td>0.426351</td>\n",
       "      <td>0.807601</td>\n",
       "      <td>0.806462</td>\n",
       "      <td>0.812689</td>\n",
       "      <td>0.809564</td>\n",
       "      <td>0.615186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.414400</td>\n",
       "      <td>0.444446</td>\n",
       "      <td>0.792568</td>\n",
       "      <td>0.743668</td>\n",
       "      <td>0.896945</td>\n",
       "      <td>0.813147</td>\n",
       "      <td>0.597599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.409500</td>\n",
       "      <td>0.435201</td>\n",
       "      <td>0.795777</td>\n",
       "      <td>0.795789</td>\n",
       "      <td>0.799261</td>\n",
       "      <td>0.797521</td>\n",
       "      <td>0.591531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.363300</td>\n",
       "      <td>0.417335</td>\n",
       "      <td>0.812162</td>\n",
       "      <td>0.804568</td>\n",
       "      <td>0.827795</td>\n",
       "      <td>0.816016</td>\n",
       "      <td>0.624503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.417800</td>\n",
       "      <td>0.410098</td>\n",
       "      <td>0.817061</td>\n",
       "      <td>0.804432</td>\n",
       "      <td>0.840886</td>\n",
       "      <td>0.822255</td>\n",
       "      <td>0.634660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.353600</td>\n",
       "      <td>0.410344</td>\n",
       "      <td>0.816385</td>\n",
       "      <td>0.824640</td>\n",
       "      <td>0.806647</td>\n",
       "      <td>0.815544</td>\n",
       "      <td>0.632959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.357800</td>\n",
       "      <td>0.412064</td>\n",
       "      <td>0.817905</td>\n",
       "      <td>0.817574</td>\n",
       "      <td>0.821417</td>\n",
       "      <td>0.819491</td>\n",
       "      <td>0.635792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.354200</td>\n",
       "      <td>0.415035</td>\n",
       "      <td>0.819426</td>\n",
       "      <td>0.801262</td>\n",
       "      <td>0.852635</td>\n",
       "      <td>0.826151</td>\n",
       "      <td>0.640022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.320900</td>\n",
       "      <td>0.414623</td>\n",
       "      <td>0.818581</td>\n",
       "      <td>0.817394</td>\n",
       "      <td>0.823431</td>\n",
       "      <td>0.820401</td>\n",
       "      <td>0.637148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "short_filter_only = [f\"hyena.backbone.layers.{i}.mixer.short_filter\" for i in range(4)]\n",
    "peft_config4 = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, inference_mode=False,\n",
    "    r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=short_filter_only,\n",
    ")\n",
    "model4 = get_peft_model(base_model, peft_config4)\n",
    "model4.print_trainable_parameters()\n",
    "\n",
    "training_args4 = TrainingArguments(\n",
    "    output_dir=\"./results/exp5_short_filter_only_5ep_6e-4\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs/exp5_short_filter_only_5ep_6e-4\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_safetensors=False\n",
    ")\n",
    "\n",
    "trainer4 = Trainer(\n",
    "    model=model4,\n",
    "    args=training_args4,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer4.train()\n",
    "model4 = model4.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [7400/7400 02:36, Epoch 10/10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5: FFN modules, 10 epochs, BS=32, LR=5e-4, r=16, r-alpha = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 45,568 || all params: 477,568 || trainable%: 9.5417\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14800' max='14800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14800/14800 05:28, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.486800</td>\n",
       "      <td>0.435273</td>\n",
       "      <td>0.805068</td>\n",
       "      <td>0.822780</td>\n",
       "      <td>0.780799</td>\n",
       "      <td>0.801240</td>\n",
       "      <td>0.611053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.435600</td>\n",
       "      <td>0.430312</td>\n",
       "      <td>0.805068</td>\n",
       "      <td>0.798691</td>\n",
       "      <td>0.819067</td>\n",
       "      <td>0.808750</td>\n",
       "      <td>0.610256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.402900</td>\n",
       "      <td>0.437333</td>\n",
       "      <td>0.797297</td>\n",
       "      <td>0.759405</td>\n",
       "      <td>0.874119</td>\n",
       "      <td>0.812734</td>\n",
       "      <td>0.601184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.401500</td>\n",
       "      <td>0.431223</td>\n",
       "      <td>0.802196</td>\n",
       "      <td>0.790862</td>\n",
       "      <td>0.825109</td>\n",
       "      <td>0.807623</td>\n",
       "      <td>0.604840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.370200</td>\n",
       "      <td>0.432860</td>\n",
       "      <td>0.800845</td>\n",
       "      <td>0.786442</td>\n",
       "      <td>0.829473</td>\n",
       "      <td>0.807384</td>\n",
       "      <td>0.602447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.395800</td>\n",
       "      <td>0.424653</td>\n",
       "      <td>0.808446</td>\n",
       "      <td>0.810920</td>\n",
       "      <td>0.807654</td>\n",
       "      <td>0.809284</td>\n",
       "      <td>0.616891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.348200</td>\n",
       "      <td>0.427731</td>\n",
       "      <td>0.809122</td>\n",
       "      <td>0.821342</td>\n",
       "      <td>0.793219</td>\n",
       "      <td>0.807036</td>\n",
       "      <td>0.618679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.354500</td>\n",
       "      <td>0.427939</td>\n",
       "      <td>0.808446</td>\n",
       "      <td>0.802360</td>\n",
       "      <td>0.821752</td>\n",
       "      <td>0.811940</td>\n",
       "      <td>0.616999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.378100</td>\n",
       "      <td>0.428002</td>\n",
       "      <td>0.809966</td>\n",
       "      <td>0.809413</td>\n",
       "      <td>0.814032</td>\n",
       "      <td>0.811715</td>\n",
       "      <td>0.619913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.331300</td>\n",
       "      <td>0.433793</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.814552</td>\n",
       "      <td>0.807989</td>\n",
       "      <td>0.811257</td>\n",
       "      <td>0.621646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_config5 = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, inference_mode=False,\n",
    "    r=16, lora_alpha=16, lora_dropout=0.1,\n",
    "    target_modules=ffn_only, bias='all'\n",
    ")\n",
    "model5 = get_peft_model(base_model, peft_config5)\n",
    "model5.print_trainable_parameters()\n",
    "\n",
    "training_args5 = TrainingArguments(\n",
    "    output_dir=\"./results/exp4_ffn_only_5ep_6e-4\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs/exp4_ffn_only_5ep_6e-4\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_safetensors=False,\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "trainer5 = Trainer(\n",
    "    model=model5,\n",
    "    args=training_args5,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer5.train()\n",
    "model5 = model5.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [14800/14800 05:28, Epoch 10/10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 6: embeddings only, 10 epochs, BS=64, LR=5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,560 || all params: 438,912 || trainable%: 0.5833\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7400' max='7400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7400/7400 02:25, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.470600</td>\n",
       "      <td>0.465025</td>\n",
       "      <td>0.786149</td>\n",
       "      <td>0.811115</td>\n",
       "      <td>0.749580</td>\n",
       "      <td>0.779135</td>\n",
       "      <td>0.574164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.447000</td>\n",
       "      <td>0.450250</td>\n",
       "      <td>0.788514</td>\n",
       "      <td>0.785078</td>\n",
       "      <td>0.798254</td>\n",
       "      <td>0.791611</td>\n",
       "      <td>0.577046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.446567</td>\n",
       "      <td>0.792399</td>\n",
       "      <td>0.786509</td>\n",
       "      <td>0.806311</td>\n",
       "      <td>0.796287</td>\n",
       "      <td>0.584901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.462000</td>\n",
       "      <td>0.444129</td>\n",
       "      <td>0.797973</td>\n",
       "      <td>0.798061</td>\n",
       "      <td>0.801276</td>\n",
       "      <td>0.799665</td>\n",
       "      <td>0.595924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.405300</td>\n",
       "      <td>0.443595</td>\n",
       "      <td>0.795439</td>\n",
       "      <td>0.804828</td>\n",
       "      <td>0.783484</td>\n",
       "      <td>0.794013</td>\n",
       "      <td>0.591142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.468500</td>\n",
       "      <td>0.442061</td>\n",
       "      <td>0.795439</td>\n",
       "      <td>0.798246</td>\n",
       "      <td>0.794226</td>\n",
       "      <td>0.796231</td>\n",
       "      <td>0.590882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.398500</td>\n",
       "      <td>0.440228</td>\n",
       "      <td>0.801182</td>\n",
       "      <td>0.805838</td>\n",
       "      <td>0.796912</td>\n",
       "      <td>0.801350</td>\n",
       "      <td>0.602414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.412600</td>\n",
       "      <td>0.441922</td>\n",
       "      <td>0.802534</td>\n",
       "      <td>0.798483</td>\n",
       "      <td>0.812689</td>\n",
       "      <td>0.805523</td>\n",
       "      <td>0.605103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.417100</td>\n",
       "      <td>0.440292</td>\n",
       "      <td>0.800169</td>\n",
       "      <td>0.806067</td>\n",
       "      <td>0.793891</td>\n",
       "      <td>0.799932</td>\n",
       "      <td>0.600430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.419900</td>\n",
       "      <td>0.438195</td>\n",
       "      <td>0.800338</td>\n",
       "      <td>0.809934</td>\n",
       "      <td>0.788184</td>\n",
       "      <td>0.798911</td>\n",
       "      <td>0.600948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_only = [\"hyena.backbone.embeddings.word_embeddings\"]\n",
    "peft_config6 = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, inference_mode=False,\n",
    "    r=16, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=embeddings_only,\n",
    ")\n",
    "model6 = get_peft_model(base_model, peft_config6)\n",
    "model6.print_trainable_parameters()\n",
    "\n",
    "training_args6 = TrainingArguments(\n",
    "    output_dir=\"./results/exp7_embeddings_only_5ep_6e-4\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs/exp7_embeddings_only_5ep_6e-4\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_safetensors=False,\n",
    "    label_names=[\"labels\"],\n",
    "\n",
    ")\n",
    "trainer6 = Trainer(\n",
    "    model=model6,\n",
    "    args=training_args6,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")\n",
    "trainer6.train()\n",
    "model6 = model6.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [7400/7400 02:25, Epoch 10/10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7: mixer only, 12 epochs, BS=8, LR=3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 29,184 || all params: 461,184 || trainable%: 6.3281\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71040' max='71040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71040/71040 23:49, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.428900</td>\n",
       "      <td>0.428381</td>\n",
       "      <td>0.805912</td>\n",
       "      <td>0.817708</td>\n",
       "      <td>0.790534</td>\n",
       "      <td>0.803891</td>\n",
       "      <td>0.612234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.345900</td>\n",
       "      <td>0.430634</td>\n",
       "      <td>0.807264</td>\n",
       "      <td>0.793798</td>\n",
       "      <td>0.833501</td>\n",
       "      <td>0.813165</td>\n",
       "      <td>0.615167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.446900</td>\n",
       "      <td>0.450373</td>\n",
       "      <td>0.807095</td>\n",
       "      <td>0.785514</td>\n",
       "      <td>0.848271</td>\n",
       "      <td>0.815688</td>\n",
       "      <td>0.615971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.481000</td>\n",
       "      <td>0.441327</td>\n",
       "      <td>0.808446</td>\n",
       "      <td>0.808425</td>\n",
       "      <td>0.811682</td>\n",
       "      <td>0.810050</td>\n",
       "      <td>0.616871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.413100</td>\n",
       "      <td>0.451517</td>\n",
       "      <td>0.810135</td>\n",
       "      <td>0.802216</td>\n",
       "      <td>0.826452</td>\n",
       "      <td>0.814153</td>\n",
       "      <td>0.620468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.287600</td>\n",
       "      <td>0.446828</td>\n",
       "      <td>0.809291</td>\n",
       "      <td>0.810820</td>\n",
       "      <td>0.810003</td>\n",
       "      <td>0.810411</td>\n",
       "      <td>0.618568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.291200</td>\n",
       "      <td>0.487171</td>\n",
       "      <td>0.808953</td>\n",
       "      <td>0.836980</td>\n",
       "      <td>0.770393</td>\n",
       "      <td>0.802307</td>\n",
       "      <td>0.620075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.444000</td>\n",
       "      <td>0.482913</td>\n",
       "      <td>0.805405</td>\n",
       "      <td>0.809346</td>\n",
       "      <td>0.802283</td>\n",
       "      <td>0.805799</td>\n",
       "      <td>0.610840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.430100</td>\n",
       "      <td>0.492555</td>\n",
       "      <td>0.807770</td>\n",
       "      <td>0.808375</td>\n",
       "      <td>0.810003</td>\n",
       "      <td>0.809188</td>\n",
       "      <td>0.615521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.423900</td>\n",
       "      <td>0.505958</td>\n",
       "      <td>0.804561</td>\n",
       "      <td>0.801656</td>\n",
       "      <td>0.812689</td>\n",
       "      <td>0.807135</td>\n",
       "      <td>0.609129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.321500</td>\n",
       "      <td>0.531117</td>\n",
       "      <td>0.802027</td>\n",
       "      <td>0.803493</td>\n",
       "      <td>0.802954</td>\n",
       "      <td>0.803224</td>\n",
       "      <td>0.604040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.330900</td>\n",
       "      <td>0.540588</td>\n",
       "      <td>0.801182</td>\n",
       "      <td>0.795023</td>\n",
       "      <td>0.815039</td>\n",
       "      <td>0.804906</td>\n",
       "      <td>0.602477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_config7 = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS, inference_mode=False,\n",
    "    r=16, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=mixer_only, bias='all'\n",
    ")\n",
    "model7 = get_peft_model(base_model, peft_config7)\n",
    "model7.print_trainable_parameters()\n",
    "\n",
    "training_args7 = TrainingArguments(\n",
    "    output_dir=\"./results/exp7_embeddings_only_5ep_6e-4\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=12,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs/exp7_embeddings_only_5ep_6e-4\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_safetensors=False,\n",
    "    label_names=[\"labels\"],\n",
    "\n",
    ")\n",
    "trainer7 = Trainer(\n",
    "    model=model7,\n",
    "    args=training_args7,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")\n",
    "trainer7.train()\n",
    "model7 = model7.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [71040/71040 23:49, Epoch 12/12]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    }
   ],
   "source": [
    "dir_best = \"./results/hyenadna-tiny-1k-seqlen-promoter-10ep-lr6e-4-linear-bs64\"\n",
    "config = AutoConfig.from_pretrained(dir_best, trust_remote_code=True)\n",
    "config.num_labels = 2\n",
    "best_model = HyenaDNAForSequenceClassification.from_pretrained(\n",
    "    dir_best,\n",
    "    config=config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "best_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    dir_best,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if best_tokenizer.pad_token is None:\n",
    "    best_tokenizer.add_special_tokens({\"pad_token\": best_tokenizer.sep_token})\n",
    "    best_model.resize_token_embeddings(len(best_tokenizer))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HyenaDNAForSequenceClassification(\n",
       "  (hyena): HyenaDNAModel(\n",
       "    (backbone): HyenaLMBackbone(\n",
       "      (embeddings): HyenaEmbeddings(\n",
       "        (word_embeddings): Embedding(16, 128)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x HyenaBlock(\n",
       "          (mixer): HyenaOperator(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (in_proj): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (short_filter): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(2,), groups=384)\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (pos_emb): HyenaPositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "                (1): HyenaSin()\n",
       "                (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (3): HyenaSin()\n",
       "                (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (5): HyenaSin()\n",
       "                (6): Linear(in_features=64, out_features=128, bias=False)\n",
       "              )\n",
       "              (modulation): HyenaExponentialModulation()\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): HyenaMlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (score): Linear(in_features=128, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def new_build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "    cls = [self.cls_token_id]\n",
    "    sep = [self.sep_token_id]\n",
    "    result = cls + token_ids_0 + sep\n",
    "    if token_ids_1 is not None:\n",
    "        result += token_ids_1 + sep\n",
    "    return result\n",
    "best_tokenizer.build_inputs_with_special_tokens = new_build_inputs_with_special_tokens.__get__(best_tokenizer, PreTrainedTokenizer)\n",
    "best_model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(self, model: transformers.modeling_utils.PreTrainedModel, tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, attribution_type: str = 'lig', custom_labels: Optional[List[str]] = None)\n"
     ]
    }
   ],
   "source": [
    "print(inspect.signature(SequenceClassificationExplainer.__init__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2512 true positives.\n",
      "       [CLS]: +0.0000\n",
      "           G: +0.0401\n",
      "           C: -0.0049\n",
      "           A: -0.0336\n",
      "           T: +0.0184\n",
      "       [SEP]: +0.0000\n"
     ]
    }
   ],
   "source": [
    "explainer = SequenceClassificationExplainer(best_model, best_tokenizer)\n",
    "\n",
    "true_positives = []\n",
    "for example in dataset[\"dev\"]:\n",
    "    seq = example[\"sequence\"]\n",
    "    inputs = best_tokenizer(\n",
    "        seq,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = best_model(**inputs).logits\n",
    "    pred = logits.argmax(dim=-1).item()\n",
    "    true = int(example[\"label\"])\n",
    "    if true == 1 and pred == 1:\n",
    "        true_positives.append(seq)\n",
    "\n",
    "print(f\"Found {len(true_positives)} true positives.\")\n",
    "\n",
    "token_scores = defaultdict(list)\n",
    "for seq in true_positives:\n",
    "    attributions = explainer(seq, n_steps=50)\n",
    "    for token, score in attributions:\n",
    "        token_scores[token].append(score)\n",
    "\n",
    "for token, scores in token_scores.items():\n",
    "    avg_score = sum(scores) / len(scores)\n",
    "    print(f\"{token:}: {avg_score:}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2m 9.7s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-mer, k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2512 true positives.\n",
      "GGCTG: +0.0652\n",
      "GCTGG: +0.0635\n",
      "GCTGC: +0.0617\n",
      "TTTCC: +0.0614\n",
      "CTGCT: +0.0610\n",
      "CTTCC: +0.0584\n",
      "TTCCG: +0.0581\n",
      "TTCCT: +0.0574\n",
      "TGCTG: +0.0566\n",
      "GGGAG: +0.0555\n",
      "TTTCT: +0.0551\n",
      "AGCTG: +0.0544\n",
      "TGGCT: +0.0542\n",
      "TTCCC: +0.0529\n",
      "CTGGC: +0.0529\n",
      "CTTTT: +0.0523\n",
      "CGGAG: +0.0514\n",
      "GGGCT: +0.0508\n",
      "TGCTT: +0.0501\n",
      "TTGCT: +0.0499\n",
      "CTGGG: +0.0497\n",
      "GGAGG: +0.0495\n",
      "CGCTG: +0.0480\n",
      "GCGGA: +0.0477\n",
      "CTGCG: +0.0474\n",
      "GGAGC: +0.0468\n",
      "CTTTC: +0.0467\n",
      "CTGCC: +0.0460\n",
      "CAGAG: +0.0459\n",
      "GGTGG: +0.0456\n",
      "GGCTT: +0.0456\n",
      "TGGGC: +0.0454\n",
      "GCTGA: +0.0451\n",
      "GGCGG: +0.0450\n",
      "TTTTT: +0.0449\n",
      "TGGAG: +0.0448\n",
      "TGGGA: +0.0447\n",
      "TGGGG: +0.0437\n",
      "CAGTT: +0.0436\n",
      "GCCGG: +0.0434\n",
      "GGCAG: +0.0432\n",
      "CTGAG: +0.0426\n",
      "TTTTC: +0.0423\n",
      "GCCTG: +0.0421\n",
      "GAGAG: +0.0421\n",
      "AGTGG: +0.0418\n",
      "CGGGA: +0.0415\n",
      "TGGCG: +0.0414\n",
      "GGGGA: +0.0414\n",
      "GAGCT: +0.0412\n",
      "AGAGA: +0.0409\n",
      "CTGGA: +0.0409\n",
      "TGTGG: +0.0409\n",
      "TGCGG: +0.0408\n",
      "TTTTG: +0.0405\n",
      "GGGGG: +0.0404\n",
      "GTGGG: +0.0404\n",
      "CTGCA: +0.0403\n",
      "TTGGC: +0.0402\n",
      "GAGGG: +0.0401\n",
      "GTGGC: +0.0400\n",
      "GCTTG: +0.0400\n",
      "CAGCT: +0.0399\n",
      "CTTGG: +0.0399\n",
      "TTCTT: +0.0397\n",
      "GCTTT: +0.0396\n",
      "CCTTT: +0.0395\n",
      "TGCCT: +0.0394\n",
      "TGGCA: +0.0392\n",
      "TGGTG: +0.0388\n",
      "CGGCT: +0.0385\n",
      "CTTCT: +0.0385\n",
      "TTGGG: +0.0384\n",
      "GCGGG: +0.0382\n",
      "GGAGA: +0.0381\n",
      "TGCCG: +0.0374\n",
      "AGCGG: +0.0372\n",
      "GGGTG: +0.0369\n",
      "GCAGC: +0.0366\n",
      "CTATA: +0.0365\n",
      "CTGGT: +0.0365\n",
      "GCTTC: +0.0364\n",
      "AGCAG: +0.0361\n",
      "GAGGC: +0.0361\n",
      "AGGCT: +0.0360\n",
      "CGGGG: +0.0359\n",
      "GCGCT: +0.0358\n",
      "GCAGG: +0.0357\n",
      "CCAGA: +0.0357\n",
      "GGGGC: +0.0356\n",
      "AGGGA: +0.0356\n",
      "GAGCG: +0.0355\n",
      "CCTGC: +0.0355\n",
      "TGCGC: +0.0352\n",
      "GCTGT: +0.0352\n",
      "CAGTG: +0.0351\n",
      "TTTGG: +0.0350\n",
      "TGCAG: +0.0349\n",
      "CTGTG: +0.0346\n",
      "TTCTG: +0.0346\n",
      "CCTGG: +0.0345\n",
      "GAGGA: +0.0342\n",
      "CCGGG: +0.0342\n",
      "TCTTT: +0.0340\n",
      "GTTGG: +0.0339\n",
      "AGAGC: +0.0336\n",
      "AGGTG: +0.0336\n",
      "CCAGT: +0.0333\n",
      "CCCAG: +0.0331\n",
      "TTTGC: +0.0329\n",
      "TTTGT: +0.0329\n",
      "AGAGG: +0.0328\n",
      "CCGGA: +0.0328\n",
      "CTCTT: +0.0326\n",
      "GAGTG: +0.0325\n",
      "GTGGA: +0.0323\n",
      "GGCCT: +0.0323\n",
      "CTTGC: +0.0321\n",
      "AGGAG: +0.0321\n",
      "TATAA: +0.0320\n",
      "ATTTT: +0.0319\n",
      "GCAGA: +0.0312\n",
      "GGGCG: +0.0312\n",
      "AGGCG: +0.0310\n",
      "CTCTG: +0.0310\n",
      "TTGTT: +0.0310\n",
      "AGTTT: +0.0307\n",
      "GCAGT: +0.0304\n",
      "CCGCT: +0.0303\n",
      "CTCAG: +0.0303\n",
      "AGCCG: +0.0303\n",
      "TCCGG: +0.0302\n",
      "GAGCC: +0.0302\n",
      "GGTTG: +0.0302\n",
      "GGTGC: +0.0301\n",
      "TGGCC: +0.0301\n",
      "TCCTT: +0.0299\n",
      "CCCTT: +0.0298\n",
      "GGCCG: +0.0297\n",
      "TGTTT: +0.0297\n",
      "GCCTT: +0.0294\n",
      "TGAGG: +0.0290\n",
      "CAGCA: +0.0287\n",
      "CTCCT: +0.0286\n",
      "GGGAA: +0.0286\n",
      "TGGGT: +0.0285\n",
      "GGAGT: +0.0283\n",
      "CGCTT: +0.0282\n",
      "CGGTG: +0.0282\n",
      "CGCAG: +0.0279\n",
      "GTTGC: +0.0278\n",
      "GGAAG: +0.0276\n",
      "AGCCT: +0.0275\n",
      "TCCCT: +0.0274\n",
      "GGGGT: +0.0272\n",
      "TGTTG: +0.0270\n",
      "CTTTG: +0.0267\n",
      "CGGGC: +0.0264\n",
      "TGGTT: +0.0263\n",
      "GCTCC: +0.0263\n",
      "AGGGG: +0.0262\n",
      "CATTT: +0.0261\n",
      "GAGCA: +0.0261\n",
      "CTCCG: +0.0261\n",
      "CCGGC: +0.0260\n",
      "CAGGC: +0.0258\n",
      "GAGAA: +0.0258\n",
      "TTGCG: +0.0258\n",
      "TTGGA: +0.0258\n",
      "AGTTG: +0.0258\n",
      "CAGCC: +0.0257\n",
      "AGCTT: +0.0257\n",
      "GTGCT: +0.0257\n",
      "CTCCC: +0.0257\n",
      "GCGGT: +0.0256\n",
      "TAGTT: +0.0255\n",
      "TCCTG: +0.0255\n",
      "GGGTT: +0.0254\n",
      "GCGGC: +0.0254\n",
      "CCTTC: +0.0254\n",
      "CAGGG: +0.0254\n",
      "GGGCA: +0.0253\n",
      "CGGCA: +0.0250\n",
      "GGTGA: +0.0249\n",
      "TGCTC: +0.0247\n",
      "GCCAG: +0.0247\n",
      "CCCTG: +0.0246\n",
      "TTGGT: +0.0246\n",
      "TGAGA: +0.0246\n",
      "GAGGT: +0.0244\n",
      "CAGTC: +0.0243\n",
      "AGAAG: +0.0243\n",
      "CCAGC: +0.0242\n",
      "GAAGA: +0.0241\n",
      "CACTT: +0.0241\n",
      "TTCCA: +0.0238\n",
      "CCCGG: +0.0237\n",
      "CAGCG: +0.0236\n",
      "TGAGC: +0.0235\n",
      "GGCGC: +0.0233\n",
      "GGCTC: +0.0229\n",
      "GCCCT: +0.0229\n",
      "GAAGC: +0.0227\n",
      "GTGGT: +0.0221\n",
      "TGGAA: +0.0220\n",
      "CGCCT: +0.0219\n",
      "TTGCC: +0.0218\n",
      "TCCCG: +0.0218\n",
      "CCTTG: +0.0217\n",
      "AGTGC: +0.0217\n",
      "CTGTT: +0.0216\n",
      "GCTCT: +0.0216\n",
      "TGCCC: +0.0215\n",
      "GTGCG: +0.0210\n",
      "GCGAG: +0.0210\n",
      "ATTTC: +0.0209\n",
      "GGGCC: +0.0206\n",
      "AGGAA: +0.0204\n",
      "CGGGT: +0.0203\n",
      "AGCGC: +0.0203\n",
      "TTTCA: +0.0198\n",
      "AGGGC: +0.0197\n",
      "CGGTT: +0.0196\n",
      "AGGCA: +0.0195\n",
      "TTATA: +0.0192\n",
      "CAGAC: +0.0192\n",
      "TCAGA: +0.0192\n",
      "GATGG: +0.0191\n",
      "CCGGT: +0.0191\n",
      "GCCGC: +0.0189\n",
      "AGAGT: +0.0188\n",
      "TCAGT: +0.0187\n",
      "CGCGG: +0.0186\n",
      "ACTGC: +0.0186\n",
      "CGTGG: +0.0184\n",
      "TCTGG: +0.0184\n",
      "GCCCA: +0.0183\n",
      "CCGAG: +0.0180\n",
      "GCCCG: +0.0180\n",
      "TTGTG: +0.0179\n",
      "GTGAG: +0.0176\n",
      "AGACT: +0.0173\n",
      "TCTGC: +0.0173\n",
      "TCCGC: +0.0172\n",
      "TTGCA: +0.0172\n",
      "GGAAA: +0.0171\n",
      "GTTTT: +0.0171\n",
      "CCTCT: +0.0168\n",
      "TTCAG: +0.0167\n",
      "CGGCG: +0.0166\n",
      "CCAGG: +0.0165\n",
      "AGCCC: +0.0165\n",
      "GAGTT: +0.0161\n",
      "GTTCC: +0.0161\n",
      "GCGTG: +0.0160\n",
      "TTTTA: +0.0157\n",
      "CCGCA: +0.0155\n",
      "AGTCC: +0.0155\n",
      "AGGGT: +0.0153\n",
      "GACTT: +0.0151\n",
      "TTCTC: +0.0151\n",
      "TTAGT: +0.0151\n",
      "GTTTG: +0.0151\n",
      "CCTCC: +0.0150\n",
      "TCTCT: +0.0149\n",
      "GACTG: +0.0148\n",
      "CAGGA: +0.0147\n",
      "GGATG: +0.0147\n",
      "CACTG: +0.0147\n",
      "TTGAG: +0.0147\n",
      "TCCAG: +0.0145\n",
      "CAGAA: +0.0144\n",
      "GCGCA: +0.0143\n",
      "CGAGA: +0.0143\n",
      "GCCTC: +0.0139\n",
      "GTGTG: +0.0137\n",
      "GTTTC: +0.0137\n",
      "AGGCC: +0.0134\n",
      "AAGCT: +0.0133\n",
      "TGTGT: +0.0133\n",
      "GTTGT: +0.0132\n",
      "CCCGC: +0.0131\n",
      "GGTTT: +0.0130\n",
      "CCTGT: +0.0129\n",
      "TCCCC: +0.0127\n",
      "GTGCA: +0.0126\n",
      "AGTCT: +0.0126\n",
      "CACAG: +0.0124\n",
      "CGAGC: +0.0123\n",
      "GCTCA: +0.0122\n",
      "GCCGA: +0.0121\n",
      "ACTGG: +0.0120\n",
      "TCCCA: +0.0119\n",
      "GAAGT: +0.0118\n",
      "TTCGC: +0.0117\n",
      "GGTGT: +0.0116\n",
      "CGGAA: +0.0116\n",
      "CTTGT: +0.0112\n",
      "TGCCA: +0.0112\n",
      "CAGGT: +0.0112\n",
      "CCCCT: +0.0109\n",
      "CGAGG: +0.0109\n",
      "GAGAC: +0.0107\n",
      "GTGCC: +0.0106\n",
      "AGTGT: +0.0105\n",
      "CGTGC: +0.0103\n",
      "GCGCG: +0.0101\n",
      "AGCAA: +0.0099\n",
      "TGTCT: +0.0097\n",
      "AGCCA: +0.0090\n",
      "TGACT: +0.0088\n",
      "AGTGA: +0.0088\n",
      "CCTGA: +0.0087\n",
      "GGGAC: +0.0087\n",
      "AGATT: +0.0087\n",
      "CTCGG: +0.0085\n",
      "GAAGG: +0.0083\n",
      "CGGCC: +0.0083\n",
      "CGCGC: +0.0082\n",
      "TGCGT: +0.0081\n",
      "AGAAA: +0.0081\n",
      "GCTCG: +0.0080\n",
      "GGCCC: +0.0079\n",
      "GGCGT: +0.0078\n",
      "TGCGA: +0.0077\n",
      "GTCCC: +0.0077\n",
      "CGCCG: +0.0076\n",
      "CCACT: +0.0075\n",
      "TGTGC: +0.0071\n",
      "GGCGA: +0.0071\n",
      "GTGTT: +0.0071\n",
      "CTGAC: +0.0070\n",
      "ACTTG: +0.0068\n",
      "TGGAC: +0.0068\n",
      "GCACT: +0.0067\n",
      "GGTCC: +0.0067\n",
      "TCGGG: +0.0066\n",
      "TGAGT: +0.0064\n",
      "ATTCC: +0.0063\n",
      "GCGCC: +0.0061\n",
      "TCTTC: +0.0060\n",
      "GCCGT: +0.0060\n",
      "AGCTC: +0.0058\n",
      "CCGTT: +0.0058\n",
      "CTCCA: +0.0058\n",
      "ACAGA: +0.0058\n",
      "AAGCA: +0.0057\n",
      "GGACG: +0.0054\n",
      "ACTGA: +0.0054\n",
      "CCGCC: +0.0053\n",
      "CGTTG: +0.0052\n",
      "ACTTC: +0.0051\n",
      "CTGTC: +0.0048\n",
      "CTCTC: +0.0047\n",
      "GCAAG: +0.0047\n",
      "AGACA: +0.0045\n",
      "CTTCA: +0.0045\n",
      "CCCCA: +0.0044\n",
      "CCCTC: +0.0044\n",
      "AGCGA: +0.0043\n",
      "CTTGA: +0.0042\n",
      "CCTCA: +0.0042\n",
      "CGCTC: +0.0042\n",
      "CGGAC: +0.0042\n",
      "AGATG: +0.0039\n",
      "CGTTT: +0.0039\n",
      "GTCTG: +0.0038\n",
      "CTTTA: +0.0037\n",
      "CCGCG: +0.0035\n",
      "GACGG: +0.0035\n",
      "ATGGA: +0.0035\n",
      "GGACT: +0.0034\n",
      "GAAAG: +0.0033\n",
      "ACTTT: +0.0030\n",
      "CGCCC: +0.0030\n",
      "GACGC: +0.0026\n",
      "AAGGA: +0.0026\n",
      "ACAGC: +0.0025\n",
      "AAGAG: +0.0022\n",
      "GACAG: +0.0021\n",
      "TATTT: +0.0009\n",
      "TCTTG: +0.0008\n",
      "TCGCT: +0.0008\n",
      "CCCAC: +0.0006\n",
      "TTTAT: +0.0005\n",
      "GGCAA: +0.0005\n",
      "AGTTC: +0.0005\n",
      "GGCCA: +0.0003\n",
      "CAGAT: +0.0002\n",
      "AAGAA: +0.0002\n",
      "CCGTG: +0.0001\n",
      "GCCCC: -0.0001\n",
      "TTTAG: -0.0001\n",
      "CTCGC: -0.0001\n",
      "GGACC: -0.0002\n",
      "TGCAC: -0.0005\n",
      "TCCTC: -0.0007\n",
      "GCATT: -0.0007\n",
      "GTTGA: -0.0007\n",
      "CTGAA: -0.0009\n",
      "GGGAT: -0.0009\n",
      "TCTGT: -0.0010\n",
      "CCCCG: -0.0011\n",
      "GTCCT: -0.0011\n",
      "TTTGA: -0.0014\n",
      "GAGAT: -0.0017\n",
      "GCTAG: -0.0018\n",
      "ACTCT: -0.0020\n",
      "TTGTC: -0.0020\n",
      "GATTT: -0.0022\n",
      "AAGTG: -0.0022\n",
      "GGGTC: -0.0022\n",
      "ACTCC: -0.0023\n",
      "TTACT: -0.0024\n",
      "GGATT: -0.0025\n",
      "TGACG: -0.0025\n",
      "TTCGG: -0.0028\n",
      "ATTCT: -0.0032\n",
      "ACCGG: -0.0032\n",
      "TGGTC: -0.0033\n",
      "CACTC: -0.0034\n",
      "GAAAA: -0.0036\n",
      "TCAGC: -0.0037\n",
      "AAGCC: -0.0040\n",
      "AGACC: -0.0041\n",
      "ATGGC: -0.0043\n",
      "CTTCG: -0.0044\n",
      "TGGAT: -0.0044\n",
      "GCACA: -0.0046\n",
      "CGAGT: -0.0047\n",
      "AAGCG: -0.0047\n",
      "AGGTT: -0.0047\n",
      "GAGTC: -0.0049\n",
      "CACCT: -0.0049\n",
      "TCACT: -0.0050\n",
      "CTAGT: -0.0052\n",
      "TATAT: -0.0052\n",
      "TCTCC: -0.0055\n",
      "AGGAC: -0.0055\n",
      "GACCT: -0.0055\n",
      "ACCCA: -0.0056\n",
      "TGCAA: -0.0057\n",
      "ACCTG: -0.0057\n",
      "GGTTC: -0.0058\n",
      "CACGC: -0.0059\n",
      "GCGTT: -0.0060\n",
      "GACCG: -0.0060\n",
      "ACGCG: -0.0061\n",
      "AGCGT: -0.0062\n",
      "CTGAT: -0.0062\n",
      "GGCAC: -0.0062\n",
      "GTGAC: -0.0064\n",
      "GTCAG: -0.0065\n",
      "TAGCT: -0.0068\n",
      "TCTGA: -0.0071\n",
      "GATTG: -0.0072\n",
      "TGCAT: -0.0072\n",
      "GTCCG: -0.0074\n",
      "ACTGT: -0.0075\n",
      "ATGGG: -0.0075\n",
      "GTCGG: -0.0076\n",
      "TGTGA: -0.0076\n",
      "ACGCT: -0.0077\n",
      "GCATG: -0.0077\n",
      "CCCGA: -0.0078\n",
      "ACCAG: -0.0080\n",
      "CGCAC: -0.0081\n",
      "GCACG: -0.0083\n",
      "CTAGA: -0.0084\n",
      "GACCC: -0.0084\n",
      "AGCAC: -0.0085\n",
      "AGTAG: -0.0088\n",
      "CCATT: -0.0090\n",
      "GGACA: -0.0091\n",
      "GTTCT: -0.0093\n",
      "GGAAC: -0.0094\n",
      "AGTCG: -0.0094\n",
      "TGCTA: -0.0096\n",
      "AGACG: -0.0096\n",
      "ACAGT: -0.0098\n",
      "CACCC: -0.0099\n",
      "GTCTT: -0.0100\n",
      "AGCTA: -0.0100\n",
      "TGTCC: -0.0100\n",
      "CGCGA: -0.0101\n",
      "GGAAT: -0.0101\n",
      "GAATG: -0.0103\n",
      "GCACC: -0.0103\n",
      "TGAAG: -0.0104\n",
      "AGGAT: -0.0104\n",
      "AAGGT: -0.0105\n",
      "CTAGC: -0.0105\n",
      "GGTCT: -0.0107\n",
      "CTCAC: -0.0110\n",
      "AAGTT: -0.0110\n",
      "CAGTA: -0.0111\n",
      "CAAGA: -0.0112\n",
      "TCGGA: -0.0115\n",
      "CGGAT: -0.0118\n",
      "GACGT: -0.0118\n",
      "TCAGG: -0.0120\n",
      "TAGTG: -0.0121\n",
      "ACGTG: -0.0121\n",
      "GAATT: -0.0122\n",
      "GAAAT: -0.0124\n",
      "ACCTT: -0.0124\n",
      "GTAGT: -0.0128\n",
      "AACTT: -0.0129\n",
      "TCGGC: -0.0131\n",
      "AAGGC: -0.0131\n",
      "ACAGG: -0.0133\n",
      "CATTC: -0.0133\n",
      "ACGCA: -0.0133\n",
      "AACTG: -0.0135\n",
      "AAGAT: -0.0135\n",
      "ATTGG: -0.0137\n",
      "CATGG: -0.0137\n",
      "ATAAA: -0.0144\n",
      "CAATT: -0.0144\n",
      "ACCCT: -0.0145\n",
      "AAAGC: -0.0145\n",
      "ACGGA: -0.0145\n",
      "TGACA: -0.0146\n",
      "GTGTC: -0.0147\n",
      "AATTT: -0.0147\n",
      "TACTG: -0.0149\n",
      "GACTC: -0.0149\n",
      "AAGGG: -0.0149\n",
      "CAAGG: -0.0152\n",
      "TGACC: -0.0153\n",
      "CCTCG: -0.0154\n",
      "CGCCA: -0.0154\n",
      "GAACT: -0.0155\n",
      "CGGTC: -0.0155\n",
      "CAAGC: -0.0156\n",
      "ACTCA: -0.0157\n",
      "CCTAG: -0.0158\n",
      "CTAGG: -0.0160\n",
      "TTATT: -0.0162\n",
      "AGTCA: -0.0166\n",
      "TCATT: -0.0167\n",
      "AGGTC: -0.0169\n",
      "TCCGT: -0.0170\n",
      "GGTAG: -0.0172\n",
      "TTGAC: -0.0172\n",
      "GCTTA: -0.0172\n",
      "GATGC: -0.0176\n",
      "AAAGA: -0.0177\n",
      "TTCAC: -0.0178\n",
      "ACGGG: -0.0179\n",
      "CCACA: -0.0186\n",
      "CCACC: -0.0187\n",
      "CGTGT: -0.0187\n",
      "TGTCA: -0.0188\n",
      "TGAAA: -0.0188\n",
      "AAACT: -0.0189\n",
      "CGCGT: -0.0193\n",
      "ACCGC: -0.0194\n",
      "CAAGT: -0.0195\n",
      "CGCAT: -0.0195\n",
      "GCCAC: -0.0196\n",
      "GAGTA: -0.0197\n",
      "GAACA: -0.0197\n",
      "AAAAG: -0.0198\n",
      "ACCCG: -0.0199\n",
      "TAGAG: -0.0200\n",
      "CCCCC: -0.0200\n",
      "CACCA: -0.0200\n",
      "GGTCA: -0.0200\n",
      "CAAAG: -0.0201\n",
      "TAGGG: -0.0201\n",
      "CACGG: -0.0201\n",
      "AGTTA: -0.0202\n",
      "GATGA: -0.0203\n",
      "ACGCC: -0.0203\n",
      "GCAAA: -0.0204\n",
      "GGCAT: -0.0205\n",
      "TTTCG: -0.0206\n",
      "TCGCC: -0.0207\n",
      "GTCAC: -0.0209\n",
      "CCCGT: -0.0210\n",
      "TTGAA: -0.0210\n",
      "ATTGT: -0.0211\n",
      "GGTCG: -0.0212\n",
      "CGTGA: -0.0212\n",
      "AAAGT: -0.0213\n",
      "TCTCA: -0.0215\n",
      "CAACT: -0.0216\n",
      "TAAAA: -0.0220\n",
      "ACACT: -0.0222\n",
      "GCCTA: -0.0225\n",
      "GGCTA: -0.0226\n",
      "CACGT: -0.0226\n",
      "TGATT: -0.0228\n",
      "TAGGA: -0.0228\n",
      "GAACC: -0.0229\n",
      "ACAAG: -0.0230\n",
      "TCCAC: -0.0232\n",
      "TGTTC: -0.0232\n",
      "CCAAG: -0.0234\n",
      "GTGAA: -0.0235\n",
      "ATGCT: -0.0235\n",
      "GATTC: -0.0242\n",
      "ACGGC: -0.0242\n",
      "AAGAC: -0.0243\n",
      "TCGCA: -0.0244\n",
      "GACCA: -0.0244\n",
      "TTTAA: -0.0244\n",
      "TAGGC: -0.0245\n",
      "CATTG: -0.0247\n",
      "TGTCG: -0.0250\n",
      "TCCGA: -0.0251\n",
      "GATGT: -0.0251\n",
      "ATGTG: -0.0251\n",
      "GTCCA: -0.0253\n",
      "GTCGC: -0.0253\n",
      "GTAGG: -0.0253\n",
      "TGATG: -0.0256\n",
      "TCACA: -0.0257\n",
      "GAAAC: -0.0259\n",
      "CCAAA: -0.0261\n",
      "CACCG: -0.0261\n",
      "TCGGT: -0.0266\n",
      "TAGCC: -0.0266\n",
      "AACAG: -0.0269\n",
      "AAAGG: -0.0269\n",
      "CCCAA: -0.0269\n",
      "TACAG: -0.0270\n",
      "GGGTA: -0.0271\n",
      "GTTTA: -0.0272\n",
      "GTCTC: -0.0273\n",
      "AGAAC: -0.0273\n",
      "CGTTC: -0.0274\n",
      "CTACT: -0.0274\n",
      "ACCCC: -0.0275\n",
      "CTTAG: -0.0277\n",
      "GTGAT: -0.0281\n",
      "TGTAG: -0.0281\n",
      "AGCAT: -0.0285\n",
      "GCGAC: -0.0285\n",
      "CTGTA: -0.0286\n",
      "AGAAT: -0.0289\n",
      "CGAAG: -0.0291\n",
      "ACACA: -0.0294\n",
      "TCGTT: -0.0295\n",
      "ACCTC: -0.0296\n",
      "GCCAA: -0.0298\n",
      "CACAA: -0.0301\n",
      "ATTTG: -0.0302\n",
      "ACCAA: -0.0304\n",
      "AAACA: -0.0309\n",
      "ATGGT: -0.0312\n",
      "GTATT: -0.0314\n",
      "GCGAA: -0.0315\n",
      "CGTCC: -0.0315\n",
      "CCGTC: -0.0317\n",
      "CGACT: -0.0317\n",
      "CATGT: -0.0318\n",
      "GCAAC: -0.0319\n",
      "CTCAT: -0.0322\n",
      "CATGC: -0.0322\n",
      "AATGG: -0.0322\n",
      "AAGTC: -0.0322\n",
      "TCGCG: -0.0325\n",
      "ACCGA: -0.0325\n",
      "ATATA: -0.0328\n",
      "AACAA: -0.0329\n",
      "TTTAC: -0.0333\n",
      "GTAGC: -0.0337\n",
      "TAGCA: -0.0337\n",
      "ATGCA: -0.0338\n",
      "GCTAC: -0.0340\n",
      "CACAC: -0.0341\n",
      "TTCAT: -0.0341\n",
      "GCGTC: -0.0343\n",
      "TAGTC: -0.0344\n",
      "AACCG: -0.0344\n",
      "ATGAG: -0.0347\n",
      "AACCT: -0.0351\n",
      "GTTCA: -0.0355\n",
      "AATTC: -0.0357\n",
      "AACCA: -0.0363\n",
      "CCCAT: -0.0364\n",
      "ATTCA: -0.0365\n",
      "TTCTA: -0.0365\n",
      "GGATC: -0.0365\n",
      "GATCC: -0.0368\n",
      "CGCAA: -0.0371\n",
      "TCACG: -0.0374\n",
      "TGAAT: -0.0375\n",
      "GTTCG: -0.0378\n",
      "CAAAA: -0.0378\n",
      "TAGGT: -0.0378\n",
      "AACCC: -0.0380\n",
      "GCCAT: -0.0383\n",
      "GACAA: -0.0383\n",
      "CCTAT: -0.0383\n",
      "TGATA: -0.0385\n",
      "ATGCG: -0.0387\n",
      "GACAC: -0.0389\n",
      "CGACG: -0.0390\n",
      "TACTC: -0.0391\n",
      "CCGAC: -0.0394\n",
      "TTAGA: -0.0396\n",
      "AGGTA: -0.0398\n",
      "CCACG: -0.0398\n",
      "TTGAT: -0.0402\n",
      "AAGTA: -0.0406\n",
      "TTAGG: -0.0410\n",
      "CTCAA: -0.0410\n",
      "ATTGC: -0.0411\n",
      "GTAGA: -0.0411\n",
      "CTCGT: -0.0413\n",
      "ATCTG: -0.0414\n",
      "CTCTA: -0.0414\n",
      "CTATT: -0.0414\n",
      "TCACC: -0.0415\n",
      "AAACC: -0.0421\n",
      "TTGTA: -0.0422\n",
      "CGTCT: -0.0422\n",
      "CGTCA: -0.0424\n",
      "CCGAA: -0.0425\n",
      "ACGGT: -0.0426\n",
      "TGAAC: -0.0427\n",
      "CCCTA: -0.0429\n",
      "ATCCT: -0.0430\n",
      "TTACA: -0.0430\n",
      "TGGTA: -0.0432\n",
      "AATTG: -0.0432\n",
      "ATTGA: -0.0433\n",
      "ACTCG: -0.0437\n",
      "TCCAA: -0.0439\n",
      "CCATG: -0.0439\n",
      "ATCCC: -0.0440\n",
      "AGATC: -0.0441\n",
      "TACTT: -0.0444\n",
      "AAAAC: -0.0444\n",
      "TTAAA: -0.0444\n",
      "GCTAT: -0.0446\n",
      "TCTAG: -0.0446\n",
      "CTTAC: -0.0447\n",
      "GCTAA: -0.0448\n",
      "ACGTT: -0.0450\n",
      "ACTTA: -0.0452\n",
      "CAACC: -0.0453\n",
      "AATGT: -0.0455\n",
      "GATCG: -0.0458\n",
      "AACGC: -0.0460\n",
      "ATGCC: -0.0460\n",
      "AATGA: -0.0467\n",
      "GTTAG: -0.0468\n",
      "GAACG: -0.0469\n",
      "ATGAA: -0.0469\n",
      "ACCAC: -0.0470\n",
      "CAACA: -0.0477\n",
      "ACATT: -0.0484\n",
      "TTCAA: -0.0484\n",
      "CATCT: -0.0485\n",
      "TACCT: -0.0487\n",
      "ATCTT: -0.0489\n",
      "CTTAA: -0.0489\n",
      "CACAT: -0.0489\n",
      "AGTAA: -0.0489\n",
      "TAGAA: -0.0491\n",
      "GTACT: -0.0494\n",
      "ACATG: -0.0495\n",
      "TTAGC: -0.0496\n",
      "TATTG: -0.0497\n",
      "ACTAG: -0.0500\n",
      "AAATG: -0.0501\n",
      "GATCT: -0.0502\n",
      "ATCAG: -0.0503\n",
      "ATGTC: -0.0505\n",
      "CCAAC: -0.0509\n",
      "AATGC: -0.0512\n",
      "CGGTA: -0.0512\n",
      "TAAAG: -0.0512\n",
      "ACACC: -0.0519\n",
      "TCCTA: -0.0524\n",
      "ACGTC: -0.0525\n",
      "GTCAT: -0.0527\n",
      "GGTTA: -0.0527\n",
      "CCTTA: -0.0528\n",
      "AATCC: -0.0532\n",
      "AATCT: -0.0533\n",
      "CGACC: -0.0537\n",
      "GAATC: -0.0537\n",
      "TCTCG: -0.0538\n",
      "ATTTA: -0.0539\n",
      "CGTCG: -0.0539\n",
      "CTACA: -0.0540\n",
      "CTAAG: -0.0542\n",
      "GCGAT: -0.0546\n",
      "CAAAC: -0.0547\n",
      "TAGCG: -0.0547\n",
      "AACTC: -0.0552\n",
      "CATCC: -0.0559\n",
      "AACAC: -0.0559\n",
      "TAACT: -0.0560\n",
      "AAAAA: -0.0560\n",
      "CATGA: -0.0561\n",
      "TCGTG: -0.0564\n",
      "GTGTA: -0.0567\n",
      "TAATT: -0.0567\n",
      "GACTA: -0.0571\n",
      "GCAAT: -0.0571\n",
      "TCAAA: -0.0574\n",
      "GACAT: -0.0575\n",
      "TCTTA: -0.0575\n",
      "ACAAT: -0.0577\n",
      "CTTAT: -0.0577\n",
      "TAAGC: -0.0578\n",
      "GTTAT: -0.0579\n",
      "GATTA: -0.0581\n",
      "AAATT: -0.0582\n",
      "TTACC: -0.0587\n",
      "GACGA: -0.0591\n",
      "ATGTT: -0.0595\n",
      "TAAAT: -0.0598\n",
      "TTAAG: -0.0599\n",
      "ACGAG: -0.0600\n",
      "GATCA: -0.0600\n",
      "ACAAC: -0.0602\n",
      "TGTTA: -0.0603\n",
      "ACCGT: -0.0603\n",
      "AGTAT: -0.0607\n",
      "TATTC: -0.0607\n",
      "TTAAT: -0.0609\n",
      "TAGAC: -0.0610\n",
      "GATAA: -0.0610\n",
      "ATCCA: -0.0612\n",
      "TACGC: -0.0613\n",
      "CGCTA: -0.0617\n",
      "AATCA: -0.0617\n",
      "ACAAA: -0.0619\n",
      "TAAGG: -0.0623\n",
      "ATAGT: -0.0624\n",
      "GCATC: -0.0626\n",
      "TCCAT: -0.0627\n",
      "TGATC: -0.0628\n",
      "CACTA: -0.0630\n",
      "AAAAT: -0.0630\n",
      "TCAAG: -0.0636\n",
      "GTCGT: -0.0645\n",
      "CAATG: -0.0645\n",
      "CGAAA: -0.0646\n",
      "TCGAG: -0.0650\n",
      "ATCCG: -0.0651\n",
      "GGATA: -0.0665\n",
      "CGTAG: -0.0666\n",
      "CAAAT: -0.0668\n",
      "CTACC: -0.0669\n",
      "ATAAG: -0.0670\n",
      "AAATA: -0.0674\n",
      "AGATA: -0.0674\n",
      "GTACA: -0.0675\n",
      "GTTAC: -0.0677\n",
      "GGTAA: -0.0680\n",
      "GTCGA: -0.0683\n",
      "AGTAC: -0.0684\n",
      "TAAGA: -0.0688\n",
      "AACGG: -0.0690\n",
      "TAAGT: -0.0690\n",
      "GGTAT: -0.0691\n",
      "CGACA: -0.0693\n",
      "TCATG: -0.0699\n",
      "ATGAC: -0.0699\n",
      "TCTAA: -0.0701\n",
      "TGTAT: -0.0709\n",
      "CGATG: -0.0710\n",
      "AATTA: -0.0710\n",
      "CTAAT: -0.0713\n",
      "ATTCG: -0.0714\n",
      "GCATA: -0.0716\n",
      "ATGAT: -0.0716\n",
      "TATAG: -0.0716\n",
      "GTATA: -0.0718\n",
      "CATCA: -0.0722\n",
      "CTAAA: -0.0724\n",
      "CCATC: -0.0728\n",
      "GTCAA: -0.0729\n",
      "CCTAA: -0.0733\n",
      "GGTAC: -0.0738\n",
      "GATAG: -0.0740\n",
      "CTCGA: -0.0742\n",
      "ACACG: -0.0744\n",
      "TTAAC: -0.0746\n",
      "AACAT: -0.0746\n",
      "ATAGC: -0.0746\n",
      "ATACT: -0.0747\n",
      "ATTAC: -0.0755\n",
      "GAATA: -0.0760\n",
      "CAACG: -0.0767\n",
      "ATATT: -0.0768\n",
      "TTATG: -0.0773\n",
      "TTCGT: -0.0773\n",
      "ATCGC: -0.0777\n",
      "AACTA: -0.0780\n",
      "TACCA: -0.0781\n",
      "ACCAT: -0.0784\n",
      "CATAA: -0.0788\n",
      "TACCG: -0.0791\n",
      "TAACA: -0.0792\n",
      "TTCGA: -0.0795\n",
      "CCGTA: -0.0797\n",
      "CCAAT: -0.0798\n",
      "GCGTA: -0.0805\n",
      "TAAAC: -0.0812\n",
      "AAATC: -0.0814\n",
      "CTACG: -0.0820\n",
      "CCGAT: -0.0822\n",
      "ATCTC: -0.0826\n",
      "AATAT: -0.0827\n",
      "CCTAC: -0.0829\n",
      "ACTAC: -0.0834\n",
      "TGTAC: -0.0836\n",
      "ATAGG: -0.0837\n",
      "TACAA: -0.0840\n",
      "TAGTA: -0.0848\n",
      "ATCAC: -0.0851\n",
      "CGAAC: -0.0852\n",
      "TAACC: -0.0852\n",
      "CTAAC: -0.0853\n",
      "GTATG: -0.0857\n",
      "AATAA: -0.0860\n",
      "GTTAA: -0.0862\n",
      "AACGT: -0.0869\n",
      "TATGG: -0.0869\n",
      "TCATC: -0.0870\n",
      "GTAAG: -0.0875\n",
      "CATTA: -0.0876\n",
      "CACGA: -0.0878\n",
      "TCGAC: -0.0878\n",
      "AATAG: -0.0880\n",
      "GATAT: -0.0880\n",
      "ACTAA: -0.0887\n",
      "TACAC: -0.0891\n",
      "AAACG: -0.0898\n",
      "TACCC: -0.0898\n",
      "AACGA: -0.0902\n",
      "CCATA: -0.0905\n",
      "ATAAC: -0.0905\n",
      "TGTAA: -0.0908\n",
      "CATAG: -0.0912\n",
      "ATTAA: -0.0918\n",
      "ATACA: -0.0918\n",
      "ATCGG: -0.0923\n",
      "ATCAA: -0.0926\n",
      "GTACC: -0.0927\n",
      "TATGT: -0.0930\n",
      "TATAC: -0.0934\n",
      "TAGAT: -0.0935\n",
      "ATTAG: -0.0935\n",
      "ACGAA: -0.0939\n",
      "ACCTA: -0.0944\n",
      "TCTAT: -0.0946\n",
      "TAATC: -0.0962\n",
      "CAATA: -0.0964\n",
      "CAATC: -0.0966\n",
      "GTAAT: -0.0969\n",
      "TAATG: -0.0992\n",
      "ATGTA: -0.0996\n",
      "TCAAC: -0.0996\n",
      "TCTAC: -0.1006\n",
      "CGAAT: -0.1008\n",
      "TCGTC: -0.1012\n",
      "TACGT: -0.1020\n",
      "CATAT: -0.1020\n",
      "GTAAA: -0.1024\n",
      "CGTTA: -0.1028\n",
      "TATGC: -0.1029\n",
      "ACTAT: -0.1031\n",
      "TACTA: -0.1036\n",
      "GTAAC: -0.1038\n",
      "TCAAT: -0.1039\n",
      "ATTAT: -0.1050\n",
      "CGATC: -0.1056\n",
      "CATAC: -0.1061\n",
      "TATGA: -0.1074\n",
      "TATCT: -0.1077\n",
      "GATAC: -0.1083\n",
      "CTATG: -0.1089\n",
      "TTATC: -0.1095\n",
      "ATCAT: -0.1098\n",
      "CGATT: -0.1104\n",
      "TCATA: -0.1109\n",
      "ACATC: -0.1115\n",
      "GTCTA: -0.1120\n",
      "ATATG: -0.1126\n",
      "TTACG: -0.1130\n",
      "TAACG: -0.1132\n",
      "ACGAC: -0.1136\n",
      "ATAAT: -0.1138\n",
      "ATAGA: -0.1139\n",
      "TATTA: -0.1158\n",
      "TACGG: -0.1187\n",
      "ATACC: -0.1188\n",
      "CATCG: -0.1190\n",
      "CTATC: -0.1191\n",
      "CGTAC: -0.1208\n",
      "TACAT: -0.1216\n",
      "AATCG: -0.1222\n",
      "TCGAA: -0.1236\n",
      "TATCA: -0.1248\n",
      "ATCGT: -0.1256\n",
      "ACGAT: -0.1257\n",
      "ACGTA: -0.1263\n",
      "ATCGA: -0.1267\n",
      "ACATA: -0.1279\n",
      "TATCC: -0.1283\n",
      "CGTAA: -0.1322\n",
      "TAATA: -0.1340\n",
      "GTACG: -0.1363\n",
      "AATAC: -0.1373\n",
      "ATCTA: -0.1406\n",
      "TACGA: -0.1431\n",
      "TATCG: -0.1461\n",
      "CGTAT: -0.1470\n",
      "GTATC: -0.1522\n",
      "ATACG: -0.1565\n",
      "TCGTA: -0.1618\n",
      "CGATA: -0.1635\n",
      "ATATC: -0.1697\n",
      "TCGAT: -0.1701\n"
     ]
    }
   ],
   "source": [
    "explainer = SequenceClassificationExplainer(best_model, best_tokenizer)\n",
    "\n",
    "true_positives = []\n",
    "for example in dataset[\"dev\"]:\n",
    "    seq = example[\"sequence\"]\n",
    "    inputs = best_tokenizer(\n",
    "        seq,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = best_model(**inputs).logits\n",
    "    pred = logits.argmax(dim=-1).item()\n",
    "    true = int(example[\"label\"])\n",
    "    if true == 1 and pred == 1:\n",
    "        true_positives.append(seq)\n",
    "\n",
    "print(f\"Found {len(true_positives)} true positives.\")\n",
    "\n",
    "k = 5\n",
    "kmer_scores = defaultdict(list)\n",
    "\n",
    "for seq in true_positives:\n",
    "    attributions = explainer(seq, n_steps=50)\n",
    "    tokens, scores = zip(*attributions)\n",
    "    specials = set(best_tokenizer.all_special_tokens)\n",
    "    filtered = [(t, s) for t, s in zip(tokens, scores) if t not in specials]\n",
    "    tokens, scores = zip(*filtered)\n",
    "\n",
    "    for i in range(len(tokens) - k + 1):\n",
    "        kmer = \"\".join(tokens[i: i + k])\n",
    "        kmer_score = sum(scores[i: i + k]) / k\n",
    "        kmer_scores[kmer].append(kmer_score)\n",
    "\n",
    "sorted_kmers = sorted(\n",
    "    ((kmer, sum(scores) / len(scores)) for kmer, scores in kmer_scores.items()),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for kmer, avg_score in sorted_kmers:\n",
    "    print(f\"{kmer:>5}: {avg_score:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2m 9.7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to sorted_kmers.csv\n"
     ]
    }
   ],
   "source": [
    "df_result = pd.DataFrame(sorted_kmers, columns=[\"kmer\", \"avg_score\"])\n",
    "df_result.to_csv(\"5mers_interpretation_IGseqclass.csv\", index=False)\n",
    "print(\"Results saved to sorted_kmers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothed Integrated Gradients (IG with noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-mer, k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2512 true positives.\n",
      "GCTGG: +0.0912\n",
      "CTGGG: +0.0875\n",
      "GGTGG: +0.0852\n",
      "CGCGG: +0.0825\n",
      "TGGGG: +0.0825\n",
      "GCGGG: +0.0808\n",
      "GTTGG: +0.0807\n",
      "TGGGA: +0.0805\n",
      "GCGCG: +0.0804\n",
      "GGAAG: +0.0801\n",
      "GCCGG: +0.0798\n",
      "CGCGC: +0.0788\n",
      "GGGTG: +0.0786\n",
      "GTGGG: +0.0784\n",
      "CCGGG: +0.0781\n",
      "GGCTG: +0.0780\n",
      "CGGGG: +0.0769\n",
      "GGGAG: +0.0766\n",
      "GGTTG: +0.0765\n",
      "TGGTG: +0.0756\n",
      "GGCGG: +0.0755\n",
      "CCCGG: +0.0754\n",
      "TGGAG: +0.0752\n",
      "TTTTT: +0.0743\n",
      "GGCGC: +0.0742\n",
      "TGCTG: +0.0739\n",
      "GCGGA: +0.0739\n",
      "CGCCG: +0.0737\n",
      "GGGGG: +0.0725\n",
      "TTGGG: +0.0723\n",
      "CCGCG: +0.0722\n",
      "CTGGA: +0.0721\n",
      "GGAGG: +0.0716\n",
      "GGGGA: +0.0715\n",
      "TGGCG: +0.0715\n",
      "GGGCG: +0.0712\n",
      "GCTGC: +0.0711\n",
      "CCTGG: +0.0709\n",
      "TGCGG: +0.0692\n",
      "CGCTG: +0.0691\n",
      "GCGCC: +0.0691\n",
      "CCGGA: +0.0689\n",
      "GTTTG: +0.0683\n",
      "TGTGG: +0.0683\n",
      "TGTTG: +0.0681\n",
      "CGGAG: +0.0678\n",
      "CTGGT: +0.0674\n",
      "GGAGC: +0.0671\n",
      "GTTGC: +0.0671\n",
      "GGGAA: +0.0669\n",
      "CGGCG: +0.0667\n",
      "CGGGA: +0.0667\n",
      "GTGGA: +0.0664\n",
      "GGCCG: +0.0663\n",
      "GCCGC: +0.0661\n",
      "TGGTT: +0.0660\n",
      "CGGTG: +0.0659\n",
      "GGGGT: +0.0658\n",
      "GCTTG: +0.0655\n",
      "CTGCG: +0.0655\n",
      "TTGGA: +0.0654\n",
      "TTTGG: +0.0653\n",
      "TGGAA: +0.0653\n",
      "CGGGC: +0.0652\n",
      "GCGGC: +0.0651\n",
      "TCCGG: +0.0649\n",
      "GGTGC: +0.0647\n",
      "GCGGT: +0.0642\n",
      "CGTGG: +0.0640\n",
      "CTTGG: +0.0640\n",
      "GCGCT: +0.0638\n",
      "GCGTG: +0.0635\n",
      "GCCTG: +0.0633\n",
      "GGGGC: +0.0631\n",
      "CCGGC: +0.0631\n",
      "TGGGT: +0.0628\n",
      "GGGTT: +0.0628\n",
      "CGGAA: +0.0625\n",
      "GTGTG: +0.0623\n",
      "CTGGC: +0.0609\n",
      "GTGGC: +0.0608\n",
      "TGCGC: +0.0598\n",
      "GGTGT: +0.0597\n",
      "GCCCG: +0.0597\n",
      "TCTGG: +0.0596\n",
      "CCCGC: +0.0594\n",
      "TGGGC: +0.0589\n",
      "GGAGT: +0.0589\n",
      "TTTTG: +0.0586\n",
      "CGCCC: +0.0582\n",
      "GGACG: +0.0581\n",
      "CGGGT: +0.0580\n",
      "GTGGT: +0.0580\n",
      "GTGCG: +0.0576\n",
      "TTCCG: +0.0575\n",
      "AGCTG: +0.0573\n",
      "CCGGT: +0.0563\n",
      "CCGCC: +0.0557\n",
      "CCCCG: +0.0556\n",
      "CTGTG: +0.0556\n",
      "GCTGT: +0.0551\n",
      "GGGCT: +0.0550\n",
      "GGTTT: +0.0550\n",
      "GTTGT: +0.0548\n",
      "CGGCC: +0.0545\n",
      "GCAGG: +0.0545\n",
      "TTGCT: +0.0539\n",
      "TTGTT: +0.0536\n",
      "GGGCC: +0.0533\n",
      "AGTTG: +0.0533\n",
      "TTGGT: +0.0532\n",
      "TTGCG: +0.0529\n",
      "TCGGG: +0.0525\n",
      "TTCTG: +0.0520\n",
      "GAGCG: +0.0516\n",
      "GGATG: +0.0508\n",
      "GGGAC: +0.0506\n",
      "CGGCT: +0.0506\n",
      "GAGGG: +0.0505\n",
      "TGCCG: +0.0495\n",
      "GGCAG: +0.0493\n",
      "GCGAG: +0.0493\n",
      "CGGTT: +0.0492\n",
      "TTGTG: +0.0490\n",
      "GGACC: +0.0490\n",
      "TTTGT: +0.0489\n",
      "GAGGA: +0.0484\n",
      "ACGCG: +0.0482\n",
      "CTGCC: +0.0481\n",
      "GGAGA: +0.0479\n",
      "GGAAC: +0.0478\n",
      "CTATA: +0.0477\n",
      "GGCGT: +0.0476\n",
      "CTGCT: +0.0476\n",
      "GACGG: +0.0476\n",
      "GTGTT: +0.0476\n",
      "ACTGG: +0.0474\n",
      "CCTGC: +0.0473\n",
      "GACTG: +0.0473\n",
      "GAAGC: +0.0472\n",
      "GTGCT: +0.0471\n",
      "TTTGC: +0.0467\n",
      "GAGTG: +0.0465\n",
      "AGTGG: +0.0465\n",
      "GGCCC: +0.0465\n",
      "CGGAC: +0.0465\n",
      "CTCGG: +0.0463\n",
      "TGCAG: +0.0462\n",
      "AGCGG: +0.0461\n",
      "TCCCG: +0.0461\n",
      "TCGCG: +0.0460\n",
      "GGAAA: +0.0458\n",
      "TGGCT: +0.0455\n",
      "GAAGT: +0.0455\n",
      "TGTTT: +0.0454\n",
      "TTGGC: +0.0454\n",
      "GACGC: +0.0452\n",
      "AGGTG: +0.0451\n",
      "CGCGT: +0.0450\n",
      "CTGTT: +0.0449\n",
      "GCCCC: +0.0448\n",
      "GCTTC: +0.0443\n",
      "CGTTG: +0.0440\n",
      "CTTTG: +0.0439\n",
      "TGGCC: +0.0437\n",
      "TCCTG: +0.0436\n",
      "CCCTG: +0.0434\n",
      "TGGAC: +0.0427\n",
      "GCTCG: +0.0426\n",
      "CGCCT: +0.0423\n",
      "GGTTC: +0.0423\n",
      "TATAA: +0.0419\n",
      "GAAGG: +0.0414\n",
      "CTCTG: +0.0413\n",
      "GCGCA: +0.0413\n",
      "AGGAA: +0.0412\n",
      "GGCTT: +0.0411\n",
      "GAGGT: +0.0410\n",
      "GAGCT: +0.0410\n",
      "CGCGA: +0.0406\n",
      "GCAGC: +0.0406\n",
      "TCCGC: +0.0405\n",
      "TGCTT: +0.0405\n",
      "GTCGG: +0.0405\n",
      "GAGGC: +0.0403\n",
      "CTGAG: +0.0402\n",
      "CCGCT: +0.0402\n",
      "CCGTG: +0.0401\n",
      "TGAGG: +0.0400\n",
      "GTTTC: +0.0399\n",
      "GAGAG: +0.0398\n",
      "TGTGT: +0.0397\n",
      "GATGG: +0.0396\n",
      "CGCTT: +0.0395\n",
      "CTCCG: +0.0392\n",
      "CAGGG: +0.0391\n",
      "GCCTC: +0.0390\n",
      "GCCGT: +0.0389\n",
      "CTTCC: +0.0389\n",
      "CCCCC: +0.0387\n",
      "GTGAG: +0.0387\n",
      "GGTGA: +0.0386\n",
      "GGCGA: +0.0386\n",
      "GTTTT: +0.0385\n",
      "AGCGC: +0.0385\n",
      "AGCCG: +0.0379\n",
      "ACCGG: +0.0378\n",
      "GCGTC: +0.0377\n",
      "GCAGT: +0.0377\n",
      "CTTGC: +0.0375\n",
      "GGGTC: +0.0373\n",
      "GGCTC: +0.0373\n",
      "GCGTT: +0.0373\n",
      "GAAGA: +0.0370\n",
      "GAGTT: +0.0369\n",
      "GCTGA: +0.0369\n",
      "GAGCC: +0.0368\n",
      "GTCCG: +0.0367\n",
      "GGGCA: +0.0367\n",
      "GGCCT: +0.0357\n",
      "CAGTG: +0.0357\n",
      "CTCGC: +0.0357\n",
      "CGAGG: +0.0356\n",
      "TTGAG: +0.0354\n",
      "GGTCC: +0.0354\n",
      "TGGCA: +0.0353\n",
      "GGACT: +0.0353\n",
      "CGTGC: +0.0351\n",
      "CGCAG: +0.0351\n",
      "CGTCG: +0.0351\n",
      "CCTTG: +0.0348\n",
      "TGCCT: +0.0347\n",
      "CCGAG: +0.0346\n",
      "AGGCG: +0.0344\n",
      "GCTCC: +0.0341\n",
      "GTGCC: +0.0337\n",
      "GTCGC: +0.0336\n",
      "CGGTC: +0.0333\n",
      "TTTCC: +0.0332\n",
      "TGTGC: +0.0331\n",
      "GGTCG: +0.0330\n",
      "GCCGA: +0.0329\n",
      "GTCTG: +0.0327\n",
      "CCAGG: +0.0327\n",
      "CGCTC: +0.0326\n",
      "GCCAG: +0.0325\n",
      "AGGAG: +0.0325\n",
      "CAGTT: +0.0324\n",
      "AGGGA: +0.0323\n",
      "CTTTT: +0.0321\n",
      "CTGCA: +0.0320\n",
      "GTTCC: +0.0319\n",
      "AGCAG: +0.0318\n",
      "GCACG: +0.0317\n",
      "CCTCG: +0.0315\n",
      "TTCGG: +0.0314\n",
      "AGTGT: +0.0307\n",
      "TTTCT: +0.0303\n",
      "TCGGC: +0.0302\n",
      "TTGCA: +0.0302\n",
      "AGGGG: +0.0302\n",
      "GCCCT: +0.0301\n",
      "CTTGT: +0.0300\n",
      "TTTTC: +0.0296\n",
      "TCGCC: +0.0295\n",
      "GTTGA: +0.0295\n",
      "CCCAG: +0.0295\n",
      "TGCGT: +0.0295\n",
      "TTCCT: +0.0293\n",
      "CGAGC: +0.0290\n",
      "TTGCC: +0.0288\n",
      "CAGCG: +0.0287\n",
      "TTCTT: +0.0283\n",
      "GCTTT: +0.0279\n",
      "CTCCC: +0.0278\n",
      "ACGCC: +0.0277\n",
      "TCCCC: +0.0276\n",
      "GACCG: +0.0274\n",
      "GCGAC: +0.0273\n",
      "AGAAG: +0.0273\n",
      "TCTGC: +0.0272\n",
      "TTCCC: +0.0272\n",
      "CGTTT: +0.0270\n",
      "TCTTT: +0.0269\n",
      "CGGCA: +0.0265\n",
      "CCTCC: +0.0264\n",
      "CCTGT: +0.0262\n",
      "TCTTG: +0.0262\n",
      "CGCAC: +0.0261\n",
      "AGTTT: +0.0261\n",
      "GCAGA: +0.0256\n",
      "GTGCA: +0.0253\n",
      "GTCCC: +0.0249\n",
      "TTCGC: +0.0246\n",
      "TCGGA: +0.0246\n",
      "AGGGC: +0.0245\n",
      "AGGGT: +0.0244\n",
      "CTTCT: +0.0243\n",
      "CCAGC: +0.0239\n",
      "GAGCA: +0.0235\n",
      "CAGGA: +0.0232\n",
      "CCGTT: +0.0231\n",
      "GTGTC: +0.0230\n",
      "CGACG: +0.0228\n",
      "CACTG: +0.0226\n",
      "AGTGC: +0.0224\n",
      "GGTAG: +0.0224\n",
      "ACCCG: +0.0224\n",
      "TCTGT: +0.0224\n",
      "AGGTT: +0.0224\n",
      "TGAGC: +0.0219\n",
      "TGCCC: +0.0219\n",
      "CCCTC: +0.0219\n",
      "GCTCT: +0.0216\n",
      "GTAGT: +0.0215\n",
      "GATTG: +0.0210\n",
      "TATAT: +0.0210\n",
      "CACGC: +0.0209\n",
      "CAGAG: +0.0207\n",
      "ACGGG: +0.0207\n",
      "AGAGG: +0.0205\n",
      "GAGAA: +0.0204\n",
      "ACTGC: +0.0202\n",
      "CAGGC: +0.0201\n",
      "CACGG: +0.0201\n",
      "TGGTC: +0.0200\n",
      "CAGGT: +0.0199\n",
      "ACTTG: +0.0199\n",
      "GGGAT: +0.0196\n",
      "GCAAG: +0.0194\n",
      "TGAGA: +0.0194\n",
      "CCGCA: +0.0193\n",
      "GACTT: +0.0191\n",
      "ACGGC: +0.0188\n",
      "CTCTT: +0.0188\n",
      "CTTTC: +0.0186\n",
      "CCCCT: +0.0185\n",
      "CAGCT: +0.0182\n",
      "TGAGT: +0.0182\n",
      "CGCCA: +0.0180\n",
      "GACCC: +0.0180\n",
      "GCCCA: +0.0180\n",
      "GACGT: +0.0178\n",
      "TGCTC: +0.0176\n",
      "TTCAG: +0.0174\n",
      "CGTGT: +0.0174\n",
      "TCGCT: +0.0172\n",
      "AGAGC: +0.0170\n",
      "AGCTT: +0.0168\n",
      "CGGAT: +0.0167\n",
      "TGAAG: +0.0166\n",
      "GGTCT: +0.0166\n",
      "TGGAT: +0.0163\n",
      "GCCTT: +0.0162\n",
      "AACTG: +0.0162\n",
      "TCCAG: +0.0162\n",
      "CCGTC: +0.0160\n",
      "CAGAA: +0.0158\n",
      "ACTGT: +0.0155\n",
      "AAGTG: +0.0154\n",
      "TTATA: +0.0154\n",
      "CCTCT: +0.0153\n",
      "CCAGA: +0.0153\n",
      "ACCTG: +0.0153\n",
      "GGCAC: +0.0150\n",
      "CCCGA: +0.0149\n",
      "CAGCC: +0.0149\n",
      "GAACT: +0.0145\n",
      "GGATT: +0.0143\n",
      "TGTGA: +0.0141\n",
      "CCCTT: +0.0140\n",
      "CCCGT: +0.0140\n",
      "CCTTC: +0.0138\n",
      "CCAGT: +0.0136\n",
      "CCTTT: +0.0136\n",
      "GTTCT: +0.0136\n",
      "AGGCT: +0.0136\n",
      "GAGTC: +0.0135\n",
      "CTGTC: +0.0135\n",
      "TCTCT: +0.0135\n",
      "AGCCC: +0.0134\n",
      "GTTCG: +0.0133\n",
      "GAATG: +0.0133\n",
      "TTTGA: +0.0132\n",
      "TCCTT: +0.0131\n",
      "ATTTT: +0.0130\n",
      "GGCCA: +0.0129\n",
      "ACGGA: +0.0126\n",
      "TGCGA: +0.0125\n",
      "CTGAC: +0.0120\n",
      "AGGAC: +0.0117\n",
      "CGTCC: +0.0117\n",
      "TCGGT: +0.0117\n",
      "GAAAG: +0.0116\n",
      "CTTCG: +0.0114\n",
      "ACGTG: +0.0114\n",
      "GTGAC: +0.0113\n",
      "CTGAA: +0.0112\n",
      "CTCAG: +0.0111\n",
      "GAGAC: +0.0109\n",
      "CCCCA: +0.0107\n",
      "CTCTC: +0.0107\n",
      "CAGAC: +0.0106\n",
      "AAGCG: +0.0105\n",
      "AGGCC: +0.0103\n",
      "GGGTA: +0.0100\n",
      "CTCCT: +0.0099\n",
      "TCCCT: +0.0099\n",
      "TGTTC: +0.0098\n",
      "GAACC: +0.0094\n",
      "CCCAC: +0.0093\n",
      "TTCTC: +0.0093\n",
      "TCTTC: +0.0092\n",
      "GCACT: +0.0088\n",
      "TGTCT: +0.0087\n",
      "ATATA: +0.0087\n",
      "TGACG: +0.0087\n",
      "GCACC: +0.0084\n",
      "CTTGA: +0.0083\n",
      "AGAGT: +0.0081\n",
      "GCGAA: +0.0078\n",
      "TGCAC: +0.0078\n",
      "AGAGA: +0.0077\n",
      "GACAG: +0.0076\n",
      "GTCCT: +0.0074\n",
      "CGAAG: +0.0074\n",
      "AGCCT: +0.0073\n",
      "GGAAT: +0.0072\n",
      "GCCAC: +0.0072\n",
      "GACCT: +0.0072\n",
      "ACCGC: +0.0072\n",
      "CCTGA: +0.0067\n",
      "TCCTC: +0.0066\n",
      "AAGCT: +0.0066\n",
      "AGTTC: +0.0066\n",
      "GAAAC: +0.0065\n",
      "TGTAG: +0.0064\n",
      "CAGTC: +0.0062\n",
      "GTCAG: +0.0060\n",
      "GGACA: +0.0059\n",
      "ACGCT: +0.0057\n",
      "AGCTC: +0.0056\n",
      "TGACT: +0.0054\n",
      "TTGTC: +0.0053\n",
      "TCAGT: +0.0052\n",
      "TTTCA: +0.0051\n",
      "TGTCG: +0.0047\n",
      "AGACG: +0.0047\n",
      "TCAGA: +0.0046\n",
      "TCCGT: +0.0044\n",
      "GTGAA: +0.0042\n",
      "ATGGG: +0.0039\n",
      "CCACG: +0.0038\n",
      "GTCGT: +0.0038\n",
      "CAGCA: +0.0035\n",
      "TAGTT: +0.0035\n",
      "AGTCC: +0.0035\n",
      "AAGCA: +0.0034\n",
      "AAGGA: +0.0034\n",
      "ATTGG: +0.0034\n",
      "TCGTT: +0.0033\n",
      "ATGGC: +0.0031\n",
      "GTCTT: +0.0031\n",
      "AACGC: +0.0031\n",
      "ACTGA: +0.0031\n",
      "AGTGA: +0.0029\n",
      "AGCGT: +0.0028\n",
      "ATAAA: +0.0026\n",
      "GATGT: +0.0025\n",
      "TGCCA: +0.0024\n",
      "TCTCC: +0.0023\n",
      "GTAGC: +0.0023\n",
      "ATGTG: +0.0022\n",
      "ACTTC: +0.0019\n",
      "CACCG: +0.0017\n",
      "GGCAA: +0.0016\n",
      "AGCGA: +0.0011\n",
      "TCTGA: +0.0009\n",
      "CGAGT: +0.0007\n",
      "ATGGA: +0.0006\n",
      "GTAGG: +0.0005\n",
      "AGTCT: +0.0005\n",
      "TTGAC: +0.0003\n",
      "GCATG: +0.0001\n",
      "TTTAG: +0.0001\n",
      "CCGAC: -0.0002\n",
      "TCAGG: -0.0003\n",
      "TGTCC: -0.0004\n",
      "CACTC: -0.0005\n",
      "GAACA: -0.0005\n",
      "AAGCC: -0.0005\n",
      "AGACT: -0.0007\n",
      "AGGCA: -0.0009\n",
      "CGTGA: -0.0009\n",
      "CACCC: -0.0012\n",
      "CGAGA: -0.0014\n",
      "CACTT: -0.0017\n",
      "TGACC: -0.0019\n",
      "GGATC: -0.0019\n",
      "GATGC: -0.0019\n",
      "TCGTG: -0.0021\n",
      "AGTCG: -0.0022\n",
      "ACAGT: -0.0023\n",
      "TAGTG: -0.0027\n",
      "ACTCT: -0.0028\n",
      "GTCTC: -0.0029\n",
      "CCACC: -0.0029\n",
      "ACTTT: -0.0031\n",
      "ACCCC: -0.0031\n",
      "CATGG: -0.0032\n",
      "AAGTT: -0.0033\n",
      "AGAAC: -0.0033\n",
      "AACCG: -0.0035\n",
      "GCTAG: -0.0036\n",
      "ACCAG: -0.0038\n",
      "AAGGT: -0.0042\n",
      "GCAAC: -0.0043\n",
      "TGCAA: -0.0043\n",
      "ACGGT: -0.0044\n",
      "ACTCC: -0.0047\n",
      "CCACT: -0.0048\n",
      "AGATG: -0.0049\n",
      "TTAGT: -0.0053\n",
      "GCTCA: -0.0053\n",
      "ACAGG: -0.0055\n",
      "CTCAC: -0.0056\n",
      "TTCCA: -0.0057\n",
      "TGCAT: -0.0057\n",
      "TGAAC: -0.0058\n",
      "GAACG: -0.0061\n",
      "TTGAA: -0.0062\n",
      "AGTAG: -0.0063\n",
      "TTTTA: -0.0064\n",
      "TTTAC: -0.0066\n",
      "AGCAA: -0.0067\n",
      "ACAGC: -0.0067\n",
      "AGGTC: -0.0067\n",
      "AAGAA: -0.0068\n",
      "AGACC: -0.0071\n",
      "CAAGC: -0.0074\n",
      "TCAGC: -0.0076\n",
      "TGGTA: -0.0076\n",
      "GACTC: -0.0077\n",
      "TACTG: -0.0078\n",
      "GATTT: -0.0079\n",
      "AAGAC: -0.0080\n",
      "TTACT: -0.0083\n",
      "CACAG: -0.0087\n",
      "AGCAC: -0.0091\n",
      "TAGGA: -0.0091\n",
      "TGATG: -0.0095\n",
      "GTCAC: -0.0097\n",
      "GTAGA: -0.0098\n",
      "CCTCA: -0.0099\n",
      "CGTTC: -0.0100\n",
      "ACCTC: -0.0100\n",
      "AGCCA: -0.0101\n",
      "CTCCA: -0.0102\n",
      "TCCCA: -0.0103\n",
      "TTGTA: -0.0107\n",
      "AACAG: -0.0108\n",
      "AAGGG: -0.0110\n",
      "AAGAG: -0.0110\n",
      "TAAAA: -0.0111\n",
      "CGTCT: -0.0113\n",
      "ATTTG: -0.0116\n",
      "TTTCG: -0.0119\n",
      "CGACC: -0.0119\n",
      "GGTAT: -0.0122\n",
      "CCGAA: -0.0123\n",
      "CTGTA: -0.0123\n",
      "ACCTT: -0.0124\n",
      "CGTCA: -0.0129\n",
      "CGCAA: -0.0134\n",
      "CTTCA: -0.0135\n",
      "AGGAT: -0.0138\n",
      "CACGT: -0.0139\n",
      "AACTT: -0.0142\n",
      "CTAGG: -0.0145\n",
      "AAAGC: -0.0147\n",
      "GGTCA: -0.0148\n",
      "GCACA: -0.0150\n",
      "CGCAT: -0.0151\n",
      "ACAGA: -0.0151\n",
      "TCACT: -0.0155\n",
      "GCATT: -0.0156\n",
      "CAAGG: -0.0156\n",
      "CATTT: -0.0160\n",
      "CACAC: -0.0163\n",
      "CAAGT: -0.0167\n",
      "GAGAT: -0.0168\n",
      "ATTTC: -0.0168\n",
      "ACGTT: -0.0168\n",
      "CCAAG: -0.0168\n",
      "CTTTA: -0.0168\n",
      "AGATT: -0.0170\n",
      "AGGTA: -0.0171\n",
      "GACCA: -0.0173\n",
      "GAGTA: -0.0174\n",
      "TCTCA: -0.0175\n",
      "AAGGC: -0.0176\n",
      "CACCT: -0.0177\n",
      "ACCCT: -0.0177\n",
      "CAAAG: -0.0180\n",
      "AGAAA: -0.0181\n",
      "GTGAT: -0.0183\n",
      "ACGTC: -0.0186\n",
      "TACAG: -0.0187\n",
      "ACAAG: -0.0187\n",
      "TCACG: -0.0188\n",
      "GAATT: -0.0188\n",
      "TCCGA: -0.0188\n",
      "GTTCA: -0.0188\n",
      "GTATA: -0.0188\n",
      "TAGCG: -0.0190\n",
      "TCTCG: -0.0191\n",
      "GGCAT: -0.0191\n",
      "CAGAT: -0.0191\n",
      "AGACA: -0.0192\n",
      "TCCAC: -0.0193\n",
      "GTTTA: -0.0193\n",
      "TCGCA: -0.0196\n",
      "GTCCA: -0.0204\n",
      "TTTAT: -0.0207\n",
      "TAGGT: -0.0209\n",
      "GACAC: -0.0216\n",
      "AAGTC: -0.0216\n",
      "ACTCA: -0.0217\n",
      "GTGTA: -0.0217\n",
      "AACCC: -0.0219\n",
      "GCAAA: -0.0219\n",
      "GGCTA: -0.0219\n",
      "AGAAT: -0.0220\n",
      "GACGA: -0.0221\n",
      "GATCG: -0.0222\n",
      "AGTTA: -0.0226\n",
      "TGAAT: -0.0229\n",
      "TGTCA: -0.0230\n",
      "CGTAG: -0.0232\n",
      "ACGCA: -0.0232\n",
      "ACACT: -0.0233\n",
      "GATGA: -0.0233\n",
      "TTCAC: -0.0234\n",
      "CTGAT: -0.0241\n",
      "ATTCT: -0.0243\n",
      "CCTAT: -0.0243\n",
      "GTTAG: -0.0246\n",
      "TCATT: -0.0247\n",
      "ATCTG: -0.0249\n",
      "CTAGC: -0.0254\n",
      "CGCTA: -0.0254\n",
      "GTATT: -0.0257\n",
      "TTATT: -0.0257\n",
      "TTAGG: -0.0258\n",
      "GTTAC: -0.0259\n",
      "GCATC: -0.0259\n",
      "GAAAA: -0.0259\n",
      "ATTCC: -0.0259\n",
      "AGTCA: -0.0260\n",
      "GCTAC: -0.0261\n",
      "TATTT: -0.0262\n",
      "GATTC: -0.0268\n",
      "ATGGT: -0.0270\n",
      "TGAAA: -0.0270\n",
      "TGCTA: -0.0272\n",
      "AAACC: -0.0272\n",
      "CGACT: -0.0274\n",
      "CATGT: -0.0275\n",
      "TTCAT: -0.0280\n",
      "GCGAT: -0.0280\n",
      "TTGAT: -0.0280\n",
      "AACGG: -0.0281\n",
      "ACTCG: -0.0282\n",
      "ATTGT: -0.0283\n",
      "CAACT: -0.0283\n",
      "ACACA: -0.0284\n",
      "CTCGT: -0.0287\n",
      "AACCT: -0.0287\n",
      "CGAAC: -0.0288\n",
      "GCCAA: -0.0288\n",
      "GCTTA: -0.0290\n",
      "ACCCA: -0.0291\n",
      "GGTAA: -0.0291\n",
      "TAGCT: -0.0292\n",
      "GGTTA: -0.0295\n",
      "ACCGA: -0.0296\n",
      "CATTG: -0.0298\n",
      "TGATT: -0.0299\n",
      "AATTT: -0.0300\n",
      "CCACA: -0.0304\n",
      "TAGAA: -0.0305\n",
      "CCTAG: -0.0308\n",
      "ATGTT: -0.0308\n",
      "TAGGG: -0.0309\n",
      "ACCGT: -0.0310\n",
      "AGCAT: -0.0311\n",
      "GATCC: -0.0312\n",
      "CAAGA: -0.0315\n",
      "GACAA: -0.0315\n",
      "TGACA: -0.0316\n",
      "TCACA: -0.0317\n",
      "CTCAT: -0.0317\n",
      "AGTAT: -0.0318\n",
      "CATTC: -0.0318\n",
      "CGGTA: -0.0319\n",
      "TAGAG: -0.0320\n",
      "CAACC: -0.0321\n",
      "TAGGC: -0.0323\n",
      "AATGG: -0.0325\n",
      "ACGAG: -0.0325\n",
      "ATGCA: -0.0325\n",
      "TCACC: -0.0330\n",
      "AAAGT: -0.0333\n",
      "ATCGC: -0.0333\n",
      "ATTGC: -0.0336\n",
      "TTAGA: -0.0338\n",
      "ATGCT: -0.0341\n",
      "TATAG: -0.0342\n",
      "CCATG: -0.0342\n",
      "GCCTA: -0.0343\n",
      "GCTAT: -0.0344\n",
      "AAAAG: -0.0345\n",
      "ACAAC: -0.0346\n",
      "AAAGA: -0.0346\n",
      "ATGAG: -0.0349\n",
      "CAGTA: -0.0349\n",
      "CCAAC: -0.0351\n",
      "GAATC: -0.0354\n",
      "AATGT: -0.0356\n",
      "GCCAT: -0.0356\n",
      "TGTAT: -0.0356\n",
      "GGTAC: -0.0363\n",
      "AAAAC: -0.0364\n",
      "AAAGG: -0.0368\n",
      "GTATG: -0.0369\n",
      "AAGTA: -0.0370\n",
      "GAAAT: -0.0370\n",
      "AGATC: -0.0376\n",
      "AACCA: -0.0377\n",
      "GTACT: -0.0378\n",
      "TCGTC: -0.0378\n",
      "ACCAA: -0.0380\n",
      "CTACT: -0.0380\n",
      "ATCCT: -0.0383\n",
      "CCCAA: -0.0388\n",
      "TAGCA: -0.0388\n",
      "CATCC: -0.0393\n",
      "CGTTA: -0.0396\n",
      "CTAGT: -0.0397\n",
      "TTCTA: -0.0401\n",
      "TAGCC: -0.0404\n",
      "CCCAT: -0.0404\n",
      "AAACT: -0.0405\n",
      "CAACA: -0.0405\n",
      "ATGCG: -0.0406\n",
      "TACGG: -0.0406\n",
      "CGATG: -0.0407\n",
      "CACCA: -0.0407\n",
      "ACATG: -0.0408\n",
      "CACAA: -0.0409\n",
      "AACAC: -0.0412\n",
      "AGCTA: -0.0415\n",
      "TTACC: -0.0417\n",
      "CTTAT: -0.0418\n",
      "CATGC: -0.0419\n",
      "CCATT: -0.0419\n",
      "ATGTC: -0.0420\n",
      "ATCCG: -0.0423\n",
      "TCGAG: -0.0427\n",
      "TGTTA: -0.0427\n",
      "GTAAG: -0.0427\n",
      "TACCT: -0.0430\n",
      "ACACC: -0.0431\n",
      "TATTG: -0.0433\n",
      "TACGC: -0.0433\n",
      "ACCAC: -0.0434\n",
      "CTTAG: -0.0436\n",
      "AATTC: -0.0436\n",
      "CTCTA: -0.0437\n",
      "TTCGT: -0.0438\n",
      "CCAAA: -0.0439\n",
      "TACTC: -0.0441\n",
      "AAACA: -0.0441\n",
      "AAATA: -0.0444\n",
      "CAAAC: -0.0445\n",
      "AACTC: -0.0446\n",
      "TATGG: -0.0447\n",
      "TACTT: -0.0448\n",
      "CACAT: -0.0450\n",
      "CAAAA: -0.0453\n",
      "TGATA: -0.0453\n",
      "ATCCC: -0.0457\n",
      "ATCAG: -0.0458\n",
      "AATTG: -0.0459\n",
      "GTCGA: -0.0462\n",
      "GTTAT: -0.0464\n",
      "CATAA: -0.0464\n",
      "CTAGA: -0.0464\n",
      "GATAT: -0.0465\n",
      "CTTAC: -0.0468\n",
      "GCGTA: -0.0472\n",
      "CATCT: -0.0478\n",
      "ATTCG: -0.0478\n",
      "ACATT: -0.0480\n",
      "ATGCC: -0.0481\n",
      "TAGTC: -0.0481\n",
      "TGTAA: -0.0483\n",
      "TATTC: -0.0487\n",
      "CCGAT: -0.0490\n",
      "GATCA: -0.0497\n",
      "AAGAT: -0.0497\n",
      "CCATC: -0.0498\n",
      "AAACG: -0.0500\n",
      "GATCT: -0.0502\n",
      "TCAAG: -0.0502\n",
      "ATTCA: -0.0508\n",
      "TAAGC: -0.0511\n",
      "AATCT: -0.0513\n",
      "GCATA: -0.0515\n",
      "CTACC: -0.0515\n",
      "ATCGG: -0.0517\n",
      "ATAGT: -0.0517\n",
      "GTACC: -0.0518\n",
      "TCAAA: -0.0519\n",
      "TTCAA: -0.0519\n",
      "TCCAA: -0.0522\n",
      "AAAAA: -0.0524\n",
      "ATCTT: -0.0524\n",
      "GTCAT: -0.0526\n",
      "TTACA: -0.0527\n",
      "TTAGC: -0.0532\n",
      "AAATG: -0.0532\n",
      "ACACG: -0.0532\n",
      "TCGAC: -0.0533\n",
      "TTATG: -0.0534\n",
      "CTACG: -0.0535\n",
      "AACAA: -0.0535\n",
      "ATCTC: -0.0539\n",
      "GGATA: -0.0539\n",
      "AATGC: -0.0542\n",
      "TCTTA: -0.0542\n",
      "GACAT: -0.0543\n",
      "CTATT: -0.0543\n",
      "CATCA: -0.0544\n",
      "ATTTA: -0.0545\n",
      "TGTAC: -0.0547\n",
      "TCATG: -0.0553\n",
      "CCCTA: -0.0556\n",
      "CAACG: -0.0558\n",
      "AATGA: -0.0559\n",
      "ATAAG: -0.0559\n",
      "CTCAA: -0.0562\n",
      "CATGA: -0.0562\n",
      "TCTAG: -0.0565\n",
      "CGACA: -0.0568\n",
      "CAATT: -0.0572\n",
      "ATATG: -0.0572\n",
      "CCTAC: -0.0573\n",
      "CCGTA: -0.0575\n",
      "TAAAT: -0.0576\n",
      "TATGA: -0.0577\n",
      "TACCG: -0.0578\n",
      "TGATC: -0.0580\n",
      "AATCC: -0.0580\n",
      "TATGT: -0.0587\n",
      "ACTTA: -0.0588\n",
      "TAAGG: -0.0589\n",
      "AGTAA: -0.0595\n",
      "CTACA: -0.0596\n",
      "TCTAT: -0.0597\n",
      "TCAAC: -0.0603\n",
      "ATTAC: -0.0606\n",
      "TCATA: -0.0611\n",
      "ATTGA: -0.0611\n",
      "TATGC: -0.0612\n",
      "TCCAT: -0.0617\n",
      "TCCTA: -0.0617\n",
      "TTTAA: -0.0621\n",
      "TATAC: -0.0621\n",
      "ACAAA: -0.0624\n",
      "ATGAA: -0.0629\n",
      "TAAGT: -0.0631\n",
      "TCATC: -0.0635\n",
      "CTATG: -0.0635\n",
      "GTCAA: -0.0637\n",
      "GTACA: -0.0637\n",
      "GTAAC: -0.0638\n",
      "TAAAG: -0.0643\n",
      "ATGAC: -0.0645\n",
      "TACAA: -0.0647\n",
      "CCTTA: -0.0649\n",
      "TTAAG: -0.0656\n",
      "GTAAA: -0.0660\n",
      "ATACT: -0.0663\n",
      "ATCCA: -0.0663\n",
      "TAGAC: -0.0664\n",
      "TAATT: -0.0669\n",
      "TCTAA: -0.0669\n",
      "GATAA: -0.0670\n",
      "CTAAG: -0.0676\n",
      "GCAAT: -0.0677\n",
      "AACGT: -0.0677\n",
      "ATACA: -0.0678\n",
      "AGTAC: -0.0679\n",
      "GCTAA: -0.0680\n",
      "GATTA: -0.0681\n",
      "ATAAC: -0.0686\n",
      "TACCC: -0.0686\n",
      "ATTAG: -0.0688\n",
      "TAACA: -0.0690\n",
      "TTAAT: -0.0693\n",
      "CATAG: -0.0695\n",
      "TATCA: -0.0695\n",
      "TCTAC: -0.0696\n",
      "ATCAC: -0.0702\n",
      "ACTAG: -0.0704\n",
      "TTACG: -0.0705\n",
      "TACCA: -0.0708\n",
      "CTTAA: -0.0710\n",
      "TAAAC: -0.0710\n",
      "ACGAA: -0.0712\n",
      "AATAT: -0.0713\n",
      "CGAAA: -0.0715\n",
      "AACAT: -0.0720\n",
      "ATTAT: -0.0722\n",
      "CATTA: -0.0723\n",
      "TTAAC: -0.0724\n",
      "TAACT: -0.0729\n",
      "ATATT: -0.0734\n",
      "TTATC: -0.0739\n",
      "ATAGG: -0.0740\n",
      "TAACC: -0.0744\n",
      "ATTAA: -0.0751\n",
      "GACTA: -0.0754\n",
      "GAATA: -0.0754\n",
      "CTCGA: -0.0759\n",
      "TAAGA: -0.0761\n",
      "AATTA: -0.0765\n",
      "CGATT: -0.0765\n",
      "AATCA: -0.0766\n",
      "TATTA: -0.0767\n",
      "TTCGA: -0.0773\n",
      "TACAC: -0.0778\n",
      "CGATC: -0.0780\n",
      "ATAGC: -0.0781\n",
      "ACATC: -0.0783\n",
      "ACTAT: -0.0783\n",
      "CTAAC: -0.0785\n",
      "ACAAT: -0.0785\n",
      "GATAG: -0.0793\n",
      "AAATT: -0.0795\n",
      "TATCT: -0.0799\n",
      "CCTAA: -0.0801\n",
      "TAGAT: -0.0803\n",
      "ATCAT: -0.0805\n",
      "ATGTA: -0.0809\n",
      "AAAAT: -0.0813\n",
      "CATAT: -0.0817\n",
      "AGATA: -0.0818\n",
      "GTACG: -0.0818\n",
      "CACGA: -0.0820\n",
      "CGTAC: -0.0825\n",
      "CATCG: -0.0826\n",
      "TAATG: -0.0830\n",
      "ATCAA: -0.0831\n",
      "TAATC: -0.0837\n",
      "CAATG: -0.0838\n",
      "CTAAT: -0.0846\n",
      "AATAA: -0.0850\n",
      "AACGA: -0.0851\n",
      "ACCTA: -0.0854\n",
      "GTAAT: -0.0855\n",
      "TTAAA: -0.0858\n",
      "ACGAC: -0.0864\n",
      "GTTAA: -0.0867\n",
      "TATCC: -0.0872\n",
      "AATAG: -0.0878\n",
      "ATAGA: -0.0883\n",
      "CTATC: -0.0883\n",
      "ATAAT: -0.0887\n",
      "TAGTA: -0.0890\n",
      "ATGAT: -0.0891\n",
      "AATAC: -0.0892\n",
      "ACATA: -0.0894\n",
      "CATAC: -0.0899\n",
      "ACTAC: -0.0901\n",
      "GTATC: -0.0903\n",
      "TACTA: -0.0911\n",
      "CACTA: -0.0915\n",
      "AACTA: -0.0917\n",
      "GATAC: -0.0923\n",
      "CCATA: -0.0937\n",
      "CAATC: -0.0953\n",
      "CGTAA: -0.0970\n",
      "CTAAA: -0.0972\n",
      "ACCAT: -0.0987\n",
      "ATCGA: -0.0994\n",
      "CAAAT: -0.0995\n",
      "TATCG: -0.1004\n",
      "GTCTA: -0.1018\n",
      "ATACC: -0.1019\n",
      "TAATA: -0.1036\n",
      "AAATC: -0.1048\n",
      "TAACG: -0.1054\n",
      "ACGTA: -0.1066\n",
      "TCGTA: -0.1068\n",
      "ACTAA: -0.1083\n",
      "TCAAT: -0.1093\n",
      "ATATC: -0.1094\n",
      "TACGT: -0.1125\n",
      "ATACG: -0.1156\n",
      "CGAAT: -0.1157\n",
      "CCAAT: -0.1160\n",
      "TACAT: -0.1162\n",
      "AATCG: -0.1183\n",
      "ATCTA: -0.1201\n",
      "ACGAT: -0.1287\n",
      "ATCGT: -0.1305\n",
      "CGTAT: -0.1347\n",
      "CAATA: -0.1368\n",
      "TCGAT: -0.1403\n",
      "TCGAA: -0.1404\n",
      "CGATA: -0.1587\n",
      "TACGA: -0.1925\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model.to(device).eval()\n",
    "\n",
    "embedding_layer = best_model.get_input_embeddings()\n",
    "\n",
    "def forward_embeddings(inputs_embeds, attention_mask):\n",
    "    outputs = best_model(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "    return outputs.logits\n",
    "\n",
    "ig = IntegratedGradients(forward_embeddings)\n",
    "nt = NoiseTunnel(ig)\n",
    "\n",
    "true_positives = []\n",
    "for ex in dataset[\"dev\"]:\n",
    "    seq = ex[\"sequence\"]\n",
    "    enc = best_tokenizer(seq, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = best_model(**enc).logits.argmax(dim=-1).item()\n",
    "    if int(ex[\"label\"]) == 1 and pred == 1:\n",
    "        true_positives.append(seq)\n",
    "\n",
    "print(f\"Found {len(true_positives)} true positives.\")\n",
    "\n",
    "k = 5\n",
    "kmer_scores = defaultdict(list)\n",
    "\n",
    "for seq in true_positives:\n",
    "    enc = best_tokenizer(seq, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    mask = enc.get(\"attention_mask\", (input_ids != best_tokenizer.pad_token_id).long())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = best_model(**enc).logits.argmax(dim=-1).item()\n",
    "\n",
    "    embeds = embedding_layer(input_ids)  # [1, seq_len, emb_dim]\n",
    "    baseline = torch.zeros_like(embeds)  \n",
    "\n",
    "    attributions = nt.attribute(\n",
    "        inputs=embeds,\n",
    "        baselines=baseline,\n",
    "        target=pred,\n",
    "        nt_type=\"smoothgrad\",\n",
    "        nt_samples=50,\n",
    "        stdevs=0.02,\n",
    "        additional_forward_args=(mask,),\n",
    "    )\n",
    "\n",
    "    token_attr = attributions.sum(dim=-1).squeeze(0).cpu().tolist()\n",
    "    tokens = best_tokenizer.convert_ids_to_tokens(input_ids.squeeze(0))\n",
    "\n",
    "    tokens = tokens[1:-1]\n",
    "    token_attr = token_attr[1:-1]\n",
    "\n",
    "    for i in range(len(tokens) - k + 1):\n",
    "        kmer = \"\".join(tokens[i: i + k])\n",
    "        kmer_score = sum(token_attr[i: i + k]) / k\n",
    "        kmer_scores[kmer].append(kmer_score)\n",
    "\n",
    "sorted_kmers = sorted(\n",
    "    ((kmer, sum(v) / len(v)) for kmer, v in kmer_scores.items()),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for kmer, avg in sorted_kmers:\n",
    "    print(f\"{kmer:>5}: {avg:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9m 49.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to sorted_kmers.csv\n"
     ]
    }
   ],
   "source": [
    "df_result = pd.DataFrame(sorted_kmers, columns=[\"kmer\", \"avg\"])\n",
    "df_result.to_csv(\"5mers_interpretation_smoothgrad.csv\", index=False)\n",
    "print(\"Results saved to sorted_kmers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpets arrangment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved ranking2_IG_vs_SG_corrected.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_ranked_features(fw: pd.DataFrame, id_col: str = \"kmer\") -> pd.DataFrame:\n",
    "    num = fw.drop(columns=[id_col])\n",
    "    means = num.mean()\n",
    "    dev = num.sub(means).div(means.abs()) * 100\n",
    "    dev[id_col]    = fw[id_col]\n",
    "    dev[\"mean_dev\"] = dev.drop(columns=[id_col]).mean(axis=1)\n",
    "    return dev[[id_col, \"mean_dev\"]].sort_values(\"mean_dev\", ascending=False)\n",
    "\n",
    "df_ig = (\n",
    "    pd.read_csv(\"5mers_interpretation_IGseqclass.csv\")\n",
    "      .rename(columns={\"avg_score\": \"IG_imp\"})\n",
    ")\n",
    "df_sg = (\n",
    "    pd.read_csv(\"5mers_interpretation_smoothgrad.csv\")\n",
    "      .rename(columns={\"avg\":      \"SG_imp\"})\n",
    ")\n",
    "\n",
    "merged = df_ig.merge(df_sg, on=\"kmer\")\n",
    "\n",
    "merged = merged[(merged[\"IG_imp\"] > 0) & (merged[\"SG_imp\"] > 0)]\n",
    "\n",
    "ranked = get_ranked_features(merged)\n",
    "\n",
    "ranked.to_csv(\"ranking2_IG_vs_SG_corrected.csv\", index=False)\n",
    "print(\"saved ranking2_IG_vs_SG_corrected.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
